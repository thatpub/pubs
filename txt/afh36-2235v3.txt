BY ORDER OF THE                                                                AIR FORCE HANDBOOK 36-2235
SECRETARY OF THE AIR FORCE                                                                     VOLUME 3

                                                                                                     1 NOVEMBER 2002

                                                                                                                    Personnel

                            INFORMATION FOR DESIGNERS OF INSTRUCTIONAL SYSTEMS

                                                                                 APPLICATION TO ACQUISITION


RECORDS DISPOSITION: Ensure that all records created by this handbook are
maintained and disposed of IAW AFMAN 37-139, ‚ÄúRecords Disposition Schedule‚Äù

NOTICE:      This publication is available digitally on the AFDPO www site at:
http://afpubs.hq.af.mil.


OPR: HQ AETC/DOZ (Gary J. Twogood)                 Certified by: HQ USAF/DPDT
Supersedes: AFH 36-2235, Volume 3, 1 November 1993     (Col Patricia L. C. Priest)
Pages: 264/Distribution: F


This volume provides information and guidance to ensure that the Instructional
Systems Development (ISD) process is properly applied during defense acquisition.
This handbook is a guide for Air Force personnel who acquire defense systems and the
training to operate and support those systems.


Chapter 1 GENERAL INFORMATION ........................................................ 5
Figure 1 System Acquisition Life Cycle ......................................................................................6
Figure 2 Updated ISD Model ....................................................................................................10
Figure 3 System Functions.......................................................................................................11
Figure 4 Functions with Phases ...............................................................................................12
Figure 5 Updated AF ISD Model...............................................................................................14


Chapter 2 TOTAL TRAINING SYSTEM..................................................... 15
    Section A Acquisition Concept Definition...........................................................................17
    Section B Analysis.............................................................................................................28
    Section C Design...............................................................................................................30
    Section D Development.....................................................................................................32
    Section E Implementation..................................................................................................34
    Section F Evaluation .........................................................................................................35
    Section G Quality Improvement........................................................................................ 37
Figure 6 Chart It, Check It, Change It .......................................................................................42
                                                                 AFH 36-2235 VOLUME 3                   1 NOVEMBER 2002


Figure 7 Shewhart Cycle ..........................................................................................................44
    Section H System Engineering Interaction ........................................................................45


Chapter 3 CONTRACTOR-DEVELOPED TRAINING ............................... 47
Figure 8 Updated ISD Model ....................................................................................................48
    Section A Acquisition Planning..........................................................................................50
Figure 9 System Acquisition Life Cycle ....................................................................................51
    Section B Request for Proposal Development ..................................................................55
    Section C Proposal Writing................................................................................................58
    Section D Source Selection...............................................................................................60


Chapter 4 PLANNING ................................................................................ 61
Figure 10 Updated ISD Model ..................................................................................................62
    Section A Assess Instructional Needs ...............................................................................64
    Section B Develop Overall Ou tline ...................................................................................66
    Section C Define Planning Requirements .........................................................................68
Figure 11 Instructional Systems Metrics Process .....................................................................70


Chapter 5 Analysis .................................................................................... 72
Figure 12 Analysis Phase.........................................................................................................73
    Section A Mission Analysis................................................................................................74
    Section B Task Analysis ....................................................................................................75
    Section C Training Requirements Analysis........................................................................77
    Section D Objectives Analysis ...........................................................................................80
    Section E Media Analysis ..................................................................................................82
    Section F Cost Analysis.....................................................................................................85
Figure 13 Cost Analysis............................................................................................................86
    Section G Training System Basis Analysis ........................................................................88
    Section H Preliminary Syllabus..........................................................................................91


Chapter 6 DESIGN ..................................................................................... 93
Figure 14 Design Phase ...........................................................................................................94
    Section A Start of Development ........................................................................................95
    Section B Guidance Conferences .....................................................................................96
    Section C System-Level Development Plans ....................................................................97
    Section D Courseware Planning Leading to System Readiness Reviews .......................101
    Section E Development Activities ....................................................................................104


Chapter 7 DEVELOPMENT...................................................................... 113
Figure 15 Development Phase ..............................................................................................114
    Section A Lesson Outlines/Flow Diagrams......................................................................115
    Section B Lesson Strategy/Lesson Plans........................................................................117
    Section C Storyboards.....................................................................................................119
    Section D Coding, Programming, Writing ........................................................................110
Figure 16 Example Diagram of Incremental Lesson Production and Evaluation ....................117
AFH 36-2235 VOLUME 3                     1 NOVEMBER 2002                                                                                 


      Section E Lesson Tests (Individual Tryouts)....................................................................121
      Section F Course-Level Integration Tests........................................................................123
      Section G Small-Group Tryouts.......................................................................................124
      Section H Interactive Remedy and Retest .......................................................................126


Chapter 8 IMPLEMENTATION................................................................. 128
Figure 17 Implementation Phase............................................................................................129
    Section A Site Training Readiness Review......................................................................130
    Section B Implementation of System Functions ..............................................................131
    Section C Full-Class Tryouts ...........................................................................................136
    Section D Mature System Performance Review ..............................................................137


Chapter 9 EVALUATION.......................................................................... 138
Figure 18 Evaluation ..............................................................................................................139
    Section A Formative Evaluation.......................................................................................140
    Section B Summative Evaluation.....................................................................................141
    Section C Operational Evaluation ...................................................................................141


Chapter 10 AIR FORCE-DEVELOPED MAINTENANCE TRAINING ..... 145
    Section A Major Functions in Applying ISD to Air Force-Developed Maintenance
    Training ...........................................................................................................................146
Figure 19 Example Gantt Chart..............................................................................................147
Figure 20 Example PERT Chart .............................................................................................148
Figure 21 Data Collection and Analysis Process ....................................................................149
    Section B ISD Process Applied to Maintenance Training Acquisition Environment .........152


Chapter 11 INTERACTIVE COURSEWARE (ICW)................................. 159

ATTACHMENT 1....................................................................................... 162

GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION .. 162
References .............................................................................................................................162
Abbreviations and Acronyms..................................................................................................167
Terms .....................................................................................................................................170


ATTACHMENT 2....................................................................................... 210
CROSS-WALK MAPPING OF MIL-HDBK 29612 TASKS TO
      AFH 36-2235, VOLUME 3 ................................................................... 210

ATTACHMENT 3 LESSONS LEARNED.................................................. 214
                                                                 AFH 36-2235 VOLUME 3                  1 NOVEMBER 2002



ATTACHMENT 4 374 TRAINING DEVELOPMENT SQUADRON 15
  STEP ISD PROCESS .......................................................................... 216
Figure E-1 Media/Method Matrix.............................................................................................239
Figure E-2 Functionality Fidelity .............................................................................................247
Figure E-3 Physical Fidelity ....................................................................................................249
Figure E-4 ICW Functional Fidelity .........................................................................................253
Figure E-5 ICW Physical Fidelity ............................................................................................255
AFH 36-2235    VOLUME 3      1 NOVEMBER 2002                                             5


                               Chapter 1
                          GENERAL INFORMATION
                                    Overview


Introduction      This handbook serves as a guide for applying the Instructional
                  System Development (ISD) process in defense systems
                  acquisition. It follows the principles of AFPD 36-22, AFMAN
                  36-2234, and other policy documents. It is intended as an easy
                  reading guide for the novice to ISD, as well as the veteran.
                  While it is designed as a "stand-alone" document, you must also
                  be familiar with the referenced policy documents.


Background        In the past, application of ISD to defense system acquisition has
                  not necessarily been an orderly or well-thought-out process.
                  Sometimes training was considered at concept exploration while
                  in extreme cases it may not have occurred until after the system
                  was fielded. As a result of these variances, the quality and depth
                  of training coverage varied widely from program to program.


Purpose           The purpose of this handbook is to incorporate the applicable
                  regulations and manuals into a handbook that covers the major
                  phases of the ISD process and addresses them to the various
                  phases of defense system acquisition. The ISD process has
                  application in all acquisition phases, but the major effort occurs
                  between the demonstration and validation phase, and the
                  production and deployment phase (Figure 1). The acquisition
                  cycle can last ten years or more, requiring frequent coordination
                  and evaluation, revisiting prior effort and redirecting as required.

                                                                 Continued on next page
                                                              AFH 36-2235 VOLUME 3                                                  1 NOVEMBER 2002




Purpose            Figure 1 System Acquisition Life Cycle
(Continued)


                                   Determination
                                        of
                                                                                     Acquisition Life Cycle
                                   Mission Need
                                                                                    Milestones and Phases


                                             Phase 0                      Phase I                     Phase II                Phase III          Phase IV

                                               Concept                  Demonstration              Engineering and             Production            Operations
                                            Exploration and                  and                   Manufacturing                  and                  and
                                              Definition                  Validation                Development                Deployment             Support




                              Milestone 0                 Milestone 1               Milestone II             Milestone III                Milestone IV

                                Concept                     Concept                                                                           Major
                                 Studies                  Demonstration              Development                 Production                 Modification
                                Approval                    Approval                  Approval                   Approval                    Approval




                                                                                                                                            As Required




Is this handbook   This handbook addresses the question, "How do you apply ISD
for you?           in defense system acquisition?" It applies whether the
                   instruction is contractor-developed or Air Force-developed. But,
                   is it for you?

                   Do You ‚Ä¶                                                                                                                                Yes    No
                   Ensure that contractor deliverables in the training
                   arena meet contract requirements?
                   Review instructional products throughout the phases
                   of ISD?
                   Monitor contractors extensively?
                   Develop training Requests for Proposals?
                   Train engineers, analysts or others to monitor ISD
                   during an acquisition?
                   Develop training during the acquisition or major
                   modification of defense systems?

                                                                                                                              Continued on next page
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                            7




Is this handbook      Are You ‚Ä¶                                                 Yes    No
for you?
(Continued)           A training analyst or psychologist with some
                      experience in ISD and an "expert" instructional
                      designer?
                      A program manager, system engineer, or analyst
                      with experience in hardware/software issues but
                      little experience with ISD?
                      A novice or entry-level engineer with little practical
                      experience in training?
                      A novice or entry-level acquisition manager with little
                      practical experience in training?

                      If you checked YES to any of these questions, this handbook will
                      help you do your job.


How to use this       This handbook is a guide. It incorporates various ISD regulations
handbook              to try to make your job easier. But you still must read the
                      applicable regulations and references. Use this handbook by
                      thinking about your specific assignment and use the examples to
                      develop your products. The following questionnaire will assist
                      you in identifying the sections you need to read.

                      Do You Have To ‚Ä¶                             Yes     No      Page
                      Participate in training planning teams?                       19
                      Develop system-training concepts?                             22
                      Develop system-training plans?                                24
                      Conduct acquisition strategy analysis?                        26
                      Develop quality plans?                                        36
                      Perform systems engineering duties?                           44
                      Assess instructional needs?                                    63
                      Develop overall training outlines?                             65
                      Define planning requirements?                                 67
                      Conduct task analysis?                                        73
                      Conduct training requirements analysis?                       75
                      Conduct objectives analysis?                                  78
                      Conduct media analysis?                                       80
                      Conduct cost analysis?                                        83

                                                                     Continued on next page
                                       AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




How to use this   Do You Have To ‚Ä¶                           Yes     No     Page
handbook
(Continued)       Conduct training system basis analysis?                     86
                  Develop syllabi?                                            89
                  Write system-level development plans?                       94
                  Conduct development activities leading                     102
                  to system design review?
                  Conduct incremental lesson production                      114
                  activities?
                  Conduct incremental tests?                                 118
                  Conduct iterative remedies and retests?                    123
                  Conduct on-site reviews?                                   127
                  Implement system functions?                                128
                  Conduct formative evaluations?                             137
                  Conduct summative evaluations?                             138
                  Conduct operational evaluations?                           140


What is ISD?      Instructional System Development is a deliberate and orderly,
                  but flexible process used for planning, developing, implementing,
                  and managing instructional systems. It ensures that personnel
                  are taught in a cost-efficient way the knowledge, skills, and
                  attitudes essential for successful job performance. ISD helps to
                  validate that:

                     There is a training need.
                     There is an effective and efficient solution to the need.
                     The solution can be implemented.
                     The solution can be assessed to determine whether it meets
                     the need.
                     During the development and operation of an instructional
                     system, there is a total continuing requirement for technical
                     and management improvement.


Basis for ISD     ISD is based on:

                     Basic research on how people learn
                     Basic research on how people communicate
                     The systems engineering process
                     The development of media and computer technologies
AFH 36-2235      VOLUME 3     1 NOVEMBER 2002                                          9




Why use ISD?        Instruction must be planned if it is to be effective. ISD
                    requires that:

                       Job tasks and mission requirements are analyzed.
                       The target population to perform the job is analyzed.
                       The difference between what the target population can do
                       now vs. what they must be able to do to perform the job is
                       identified as training requirements.
                       Training requirements are addressed by a hierarchy of
                       training objectives.
                       Training objectives are achieved through media and methods.
                       Media and methods are selected to optimize efficiency and
                       effectiveness of training.
                       Training courses are developed and validated to ensure that
                       training objectives are met.
                       There is a continuous feedback loop throughout for quality
                       improvement.


Goals of ISD        The goals of ISD are to produce students who can perform their
                    jobs after receiving instruction, and to reduce overall costs of
                    training by accurately identifying training requirements and
                    equipment.


How to use ISD      ISD is:

                       Flexible and systematic
                       A tool to develop the right training to solve the problem

                    ISD is NOT a step-by-step linear process.

                                  Don‚Äôt have a "checklist mentality"
                                       when you‚Äôre using ISD.

                    You need to think of ISD as being circular. You can start
                    anywhere at any time, as the need requires. During each phase
                    of the ISD process, you continually assess the quality of the
                    process and any product input. Evaluation is ongoing throughout
                    the life cycle of the training system. The updated Air Force ISD

                                                                  Continued on next page
                                       AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




How to use ISD     model is depicted in Figure 2. Although each part of this model
(Continued)        is explained in AFMAN 36-2234, further discussion tailored to
                   acquisition is included in this handbook.

                   Figure 2 Updated ISD Model




What is a total    A total training system is a systematically developed curriculum
training system?   including, but not necessarily limited to, courseware, classroom
                   aids, training simulators and devices, operational equipment,
                   embedded training capability and personnel to operate, maintain
                   or employ a system. The training system includes all necessary
                   elements of logistic support. This is covered in more detail in
                   Chapter 2.


Quality            Quality improvement, with an emphasis on evaluation throughout
improvement        the life cycle of the program, is the glue that holds the total
                   system together. Figure 2 shows the updated ISD model with
                   quality improvement and system functions.


What are system    Successful training systems must have basic top-level functions,
functions?         and these functions must be in place before a training system
                   can operate. The basic training system functions are discussed
                   in the following paragraphs and are depicted in Figure 3.

                                                                Continued on next page
AFH 36-2235       VOLUME 3     1 NOVEMBER 2002                                         11




What are system         Management. This is the function of directing or controlling
functions?              all aspects of the training system. These activities are an
(Continued)             integral part of conducting training.

                        Support. This provides for and maintains the system on a
                        day-to-day and long-term basis. This includes long-range
                        planning as well as day-to-day activities. Examples are
                        resources you need to keep equipment functioning, such as
                        an equipment maintenance contract.

                        Administration. This is the part of management that
                        performs the day-to-day tasks of operating an instructional
                        system. This includes functions such as documentation,
                        student assignments, and student records.

                        Delivery. This is the means of giving students the training.
                        Instructors, computers, and textbooks are examples of ways
                        to deliver instruction.

                        Evaluation. This function is the continuous process of
                        gathering feedback data through formative, summative and
                        operational evaluations to assess the system and, most
                        important, student performance.

                     Figure 3 System Functions
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




When do you        The system functions must be working before you start the
implement them?    training. Aspects of the training system functions are active
                   throughout all phases of ISD.


Relation to ISD    Using these essential functions to design the overall training
                   system architecture and then allocating them to the respective
                   system components, or people responsible, ensures that these
                   functions are operational when the total training system is
                   fielded. ISD products are integrated into the total training
                   system, and aspects of the system functions are active
                   throughout all phases of the ISD process.


System functions   Figure 4 shows the phases most often used in the systems
and ISD phases     approach, which are analysis, design, development, and
                   implementation, with the evaluation activities integrated into each
                   phase of the process. The phases are embedded within the
                   system functions. Evaluation is shown as the central feedback
                   "network" for the total system.

                   Figure 4 Functions with Phases




                                                                 Continued on next page
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                             13




System functions      The instructional development process, which the model
and ISD phases        summarizes, calls for instructional developers to:
(Continued)
                         Analyze and determine what instruction is needed.
                         Design instruction to meet the need.
                         Develop instructional materials to support system
                         requirements.
                         Implement the instructional system.

                      Evaluation is a central function that takes place at every phase.
                      Symbolically, Figure 4 shows that all phases of the model
                      depend on each of the other phases. The ISD process allows
                      the instructional developer or design team to enter or reenter the
                      various phases of the process as determined by the nature and
                      scope of the development or revision activity. The phases of the
                      updated model are described in more detail in separate chapters
                      of this handbook.


Evaluation            Evaluation is a continuous process beginning during the analysis
                      phase and continuing throughout the life cycle of the instructional
                      system. Evaluation consists of:

                         Formative Evaluation, consisting of process and product
                         evaluations conducted during the analysis and design
                         phases, and validation, which is conducted during the
                         development phase. Included are individual and small-group
                         tryouts.
                         Summative Evaluation, consisting of operational tryouts
                         conducted once the entire training system is fielded.
                         Operational Evaluation, consisting of periodic internal and
                         external evaluation of the operational system during the
                         implementation phase.

                      Each form of evaluation should be used during development,
                      update, and revision of instruction, if possible, and if the form of
                      evaluation is applicable.
                                     AFH 36-2235 VOLUME 3        1 NOVEMBER 2002




Updated AF ISD   Figure 5 depicts the updated Air Force ISD model. This
model            completed figure shows the system functions and ISD phases
                 embedded within the quality improvement (QI) process.

                 Figure 5 Updated AF ISD Model




                 The updated model graphically illustrates that:

                    Evaluation is the foundation of the ISD process.
                    ISD is a continuous process with the flexibility to enter and
                    reenter the various phases, as necessary, to develop, update,
                    or revise instruction.
                    All ISD activities take place within and are dependent on the
                    system functions.
                    Teamwork is required between personnel performing system
                    functions and those designing, developing, and implementing
                    instructional systems.
                    All ISD activities and system functions focus on continuous
                    quality improvements in the system.


Quality          Although Quality Improvement (QI) is covered in detail on page
improvement      36, you should remember that QI is the continuous, organized
                 creation of beneficial change to the system. The objective of QI
                 is to foster continuous improvements in the process and products
                 of ISD.
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                          15


                                  Chapter 2
                           TOTAL TRAINING SYSTEM
                                     Overview


Introduction       Fielding a new defense system with a total training system is a
                   project that requires considerable management, coordination and
                   integration. Lessons learned in fielding total training systems
                   have shown that organizations responsible for integration of the
                   training system have been left scrambling. Why? Because
                   important and sometimes even critical functions were overlooked
                   early in the overall design. The shortfalls range from "common
                   sense" such as failing to analyze student production
                   requirements, to "technical" such as improper integration of out-
                   the-cockpit visual system design with the design of the simulator.

                   Analysis of successful programs concluded that there are basic
                   top-level functions required for operation of a total training
                   system.


Purpose            The purpose of this chapter is to explain the total training system
                   concept in the context of instructional system design. These
                   concepts apply whether you are buying a total system or parts of
                   a system, a new defense system or a modification.


Where to read      The overview of total training system acquisition is explained in
about it           the following sections.

                    Section         Title                                    Page
                        A       Acquisition Concept Definition                 17
                        B       Analysis                                       27
                        C       Design                                         29
                        D       Development                                    31
                        E       Implementation                                 33
                        F       Evaluation                                     34
                        G       Quality Improvement                            36
                        H       System Engineering Interaction                 44
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




What is a total       As mentioned earlier, a total training system is a systematically
training system?      developed curriculum. The training system includes all
                      necessary elements of logistic support. In order to acquire a
                      total training system, follow the instructional system development
                      process interacting with the systems engineering process.


There is no perfect   No model is the "perfect" model. Variations of each model may
model                 be used to best meet the objectives for the training being
                      developed.
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                         17


                                Section A
                      Acquisition Concept Definition

Introduction       Acquisition is the obtaining of supplies or services, by and for the
                   use of the federal government, using appropriated funds. Before
                   acquiring these supplies and services, you must evaluate the
                   constraints and opportunities to make a sound decision. Once
                   the decision to acquire something (rather than obtaining it
                   through internal development) is made, an overall approach to
                   the acquisition should be developed.


Purpose            The acquisition concept serves as a starting point from which
                   other activities originate. The activities generated by a decision
                   to lease supplies or services can be very different from those
                   generated by a decision to purchase them. Other factors that
                   can change acquisition activities are sole sourcing, competitive
                   contracting, research and development, dual sourcing, and other
                   contracting methods.


Where to read      Prior to making a decision whether or not to obtain defense
about it           system training through acquisition, the process leading to the
                   decision has begun. Important aspects of this process are
                   described under the topics listed below.

                      Topic                                                   Page
                   Training System Requirements Studies Initiation             18
                   Training Planning Team                                       19
                   Training System Concept                                     22
                   System Training Plan                                        24
                   Training Acquisition Strategy Analysis                      26
                                           AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                    Training System Requirements Studies Initiation


Introduction           To ensure proper planning and development, Training System
                       Requirements (TSR) must be considered early in the cycle of
                       defense system acquisition. For this reason, preliminary training
                       consideration should begin in the pre-concept phase of system
                       acquisition. At this time, the defense system using command will
                       form and chair the Training Planning Team (TPT). Once the
                       defense system is defined in the demonstration and validation
                       phase of acquisition, TSR studies are initiated. A Training
                       System Requirements Analysis (TSRA) is a systematic approach
                       to front-end analysis of a training system based upon an
                       integrated instructional systems development/systems
                       engineering process that develops data items to document the
                       training and preliminary system requirements.


Purpose                The purposes of the TSR studies are to:

                          Determine training need.
                          Outline the "big picture."
                          Define training requirements.
                          Define thoroughness of data and analysis.
                          Involve defense system contractor in training issues early on.


What are training      Training requirements are determined by comparing the skills
requirements?          and knowledge requirements needed to do a job, to the current
                       abilities of the persons expected to do the job. The difference
                       between those current attitudes, abilities, knowledge, and the
                       skills necessary to perform specific tasks in order to operate,
                       maintain, and support a defense system are the training
                       requirements.
AFH 36-2235   VOLUME 3      1 NOVEMBER 2002                                        19




Further          There are several areas to consider when deciding on the TSR
explanation      study approach. For example, is a TSR study really needed? If
                 so, to what depth? Should the TSR study be done by an
                 independent contractor or the training system development
                 contractor (phased approach)? Have alternative acquisition
                 strategies been identified to allow for consideration of innovative
                 solutions to the training need? The TSR study lays the
                 foundation for the ISD process. (Further information is available
                 as each document is explained in this handbook.)
                                    AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                         Training Planning Team

Introduction   Coordination and communication are the keys to success in any
               program, and are most critical in the training business. The
               formation of a Training Planning Team (TPT) helps keep the
               many players working together to reach the training system
               objectives.


What it is     A Training Planning Team (TPT) is defined as an action group
               composed of representatives from all pertinent functional areas,
               disciplines, and interests involved in the life cycle of a specific
               defense training system. The TPT is formed at pre-concept and
               continues throughout the acquisition and day-to-day operation of
               the training system. The personnel on the TPT represent the
               using command, the system program office, and other concerned
               agencies. The TPT develops and uses the System Training Plan
               (STP) to ensure that training considerations, constraints and
               opportunities are adequately addressed in the defense system
               acquisition modification process.


Objectives     The primary objective of the training planning team is to get the
               right agencies communicating and coordinating from the very
               beginning as a team. Once a System Program Office (SPO) is
               formed, the TPT bridges between the SPO and the operating
               command. The goal is to develop the STP and keep it current
               throughout the life of the defense system.


Who is         The primary operating command will establish and chair TPTs
responsible?   throughout the life cycle of the defense system. The program
               office (normally AFMC) will support and assist the chair as
               required. The using command will also be responsible for
               developing a Manpower Estimate Report (MER) following
               acquisition guidelines.

               Note: The training wheels were put into motion prior to the
               formation of the SPO. Training was specified in the Program
               Management Directive (PMD), which is the primary document
               that directs the program office to begin a system acquisition.
               Using the PMD, the program office develops a Program

                                                             Continued on next page
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                           21




Who is                Management Plan (PMP) which outlines responsibilities and
responsible?          general management objectives. As part of PMP development,
(Continued)           all applicable MAJCOMs provide inputs. While the SPO and
                      primary operating command are considered the prime players,
                      Air Education and Training Command (AETC) has a key role.
                      This is true even if the training system will be contractor
                      procured, developed and operated.


Requirements and      Air Force Pamphlet 50-11 details specific requirements and
responsibilities      responsibilities for the training planning team.


Other                 Acquiring training is a complex process involving many agencies
responsibilities      and personnel. As stated earlier, while the operating command
                      and the program office are the two major players, many other key
                      offices are involved. For example, AETC plays a key role that
                      varies depending on the nature of the training involved. AETC
                      normally will:

                         Be a member of and support the TPT.
                         Designate an AETC Office of Primary Responsibility (OPR)
                         for the specific program.
                         Assist in determining whether all or parts of training will be
                         AETC- or contractor-developed. (If AETC or USAF
                         developed maintenance training, read "Air Force-Developed
                         Maintenance Training" [page 145].)
                         Designate AETC responsible agencies and define roles.
                         Designate AETC supporting agencies and define roles.

                      Note: As mentioned earlier, the TPT continues throughout the
                      life of the defense system. While the TPT may not meet every
                      day, every week, or even every quarter, they will meet frequently
                      enough to evaluate changes in the defense system for their effect
                      on the training system. The TPT will update the STP annually or
                      when changes occur that affect training in:

                         Tactics
                         Personnel
                            Structure
                            Demographics
                            Manning levels

                                                                    Continued on next page
                                        AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




Other                 Defense system
responsibilities         Hardware
(Continued)              Software
                         Subsystem

                      Training assets availability
                      Funding priorities/levels
                      Basing
                      Operating commands

                   The TPT develops and implements alternate training strategies
                   until the training system becomes current again with the defense
                   system.

                   Whenever possible, advance notice of changes should be
                   provided to the TPT to allow training of personnel prior to
                   implementation of defense system changes.
AFH 36-2235     VOLUME 3       1 NOVEMBER 2002                                            23


                             Training System Concept


Introduction        The development of an overall training system concept is the
                    beginning of a system-training plan. This concept serves as the
                    starting point for all other planning. It provides the framework to
                    develop system requirements, resource requirements, etc.


Objective           The objective of the training system concept is to define the
                    training philosophy and policy within which the training system
                    will be designed and operated. The training system concept
                    includes the characteristics to be exhibited by the overall training
                    system.


Who is              The operating command, as part of the TPT, is responsible for
responsible?        developing the Training System Concept (TSC). They will be
                    assisted by:

                       Subject Matter Experts (SME)
                       Contractors
                       Instructional designers
                       System Program Office (SPO) representative
                       Air Education and Training Command (AETC) representative
                       Other managers and specialists as required


What is in a TSC?   The TSC is purposely broad, but it sets the boundaries within
                    which training system decisions can be made. It can contain
                    items such as:

                       Training system life cycle master plan
                       Training philosophy
                       Type and amount of training that may be needed
                       Estimated funding requirements
                       Estimated training and support equipment needs
                       Type and size of facilities needed
                       Estimated time to develop and deliver the system
                       Projected impacts on personnel
                       Training constraints
                                     AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




Where to begin   The best way to begin developing a TSC is to review
                 predecessor systems. Look for similarities and differences.
                 What were the lessons learned from those predecessors? Now
                 is the time to identify the expense and time drivers. Many other
                 considerations must be taken to develop the TSC, which leads to
                 process analysis decisions and helps in developing the system-
                 training plan.
AFH 36-2235    VOLUME 3     1 NOVEMBER 2002                                           25


                             System Training Plan


Introduction      The Training System Concept (TSC) starts the System Training
                  Plan (STP). The training planning team is tasked in AFP 50-11
                  with initiating the STP. The primary operating command is
                  tasked with overall responsibility for the effort.


Objective         The objective of the STP is to support acquisition and
                  modification processes, requirements, documents, and milestone
                  decisions. The STP also ensures that proper training is
                  identified as the mission changes, the defense system changes,
                  or the world situation changes. The STP is a living document.


Description       The STP is a life cycle, iterative planning document that defines
                  the following functions of a training system:

                     Design
                     Development
                     Funding
                     Resources
                     Support
                     Modification
                     Operation
                     Management

                  A STP format is available in AFP 50-11.


Subset plans      The STP is composed of subset plans for each functional area
                  required, such as:

                     Operations
                     Logistics
                     Support


IMPACTS           The STP is included in the IMPACTS Program Plan (IPP) as the
                  training input and submitted to AF/XO for Air Staff coordination.
                                           AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




IPP                 The STP is submitted as part of the IPP to SAF/AQ prior to each
                    milestone decision point, or as required, starting with Milestone 1
                    (see Figure 1).


What does the STP   The STP will normally:
do?
                       Establish training system definition through acquisition and
                       modification documentation, which will support the review and
                       decision process.
                       Identify training needs, concepts, strategies, constraints,
                       risks, data, alternatives, resources, responsibilities, and other
                       areas, through an iterative process.
                       Document the results of early, front-end, and follow-on
                       training task analyses.
                       Provide information and identify resources for management
                       decisions within the planning, programming, and budgeting
                       process which support defense/training system acquisition
                       and modification processes.
                       Provide the basic concepts and strategy to attain and
                       maintain training system concurrency to support desired
                       training capability at the appropriate time.
                       Identify alternate training strategies, to include methodology
                       and media, if funding, concurrency, or other unknowns
                       negatively impact required training system capabilities.
                       Establish milestones and schedules to ensure timely
                       development, testing, and fielding of training capability and
                       training support.


STP development     Various development levels are required to support milestone
levels              decisions and reviews. See AFP 50-11 for an outline of STP
                    development levels.
AFH 36-2235      VOLUME 3       1 NOVEMBER 2002                                            27


                       Training Acquisition Strategy Analysis


Introduction         At this point, when the TPT is formed and the STP is being
                     written, a preliminary decision will be made on whether to
                     contract for all or parts of the training. Assuming the decision is
                     to have contractors develop at least a part of the training, the
                     command with program management responsibility will develop
                     an acquisition strategy. The acquisition strategy is finalized
                     before each contracted activity.


Things to consider   In developing an acquisition strategy, the following should be
                     considered by the SPO in coordination with the user:

                     Equipment:
                        Current federal acquisition regulations
                        Funding availability and constraints
                        Defense system schedules
                        Complexity of training system
                        Types of training being acquired (operator / maintenance /
                        other)
                        Sole vs. multiple sourcing
                        Lease vs. purchase

                     Personnel:
                        Trained personnel requirements
                           How many? and, When needed?
                        On-site training or schoolhouse?
                        One-time course vs. life cycle use
                        Total contractor training vs. turnkey (using command
                        operation)


Other                Getting the "big picture" is important in developing the
considerations       acquisition strategy. The total training system perspective is
                     needed to understand its full scope and how the integration will
                     take place in order to have a fully operational system. Though a
                     contracted activity may be treated as independent, the tie into
                     the "big picture" ensures a good fit. Always consider how the
                     training system fits into the overall defense system acquisition.
                     Choosing the wrong acquisition strategy not only affects the
                     training system, but can also cause delays in the defense system
                     testing, support, and initial operational capability.
                                      AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                  Section B
                                  Analysis

What you do      During analysis, you should:

                    Collect information on job performance requirements of Air
                    Force missions/duties/jobs/tasks.
                    Determine the necessary qualifications of the job performers.


Why do it?       Analysis must be conducted to make sure you get the right kind
                 of training for the stated need.


Where to read    A detailed discussion of analysis in acquisition is available in
about it         Chapter 5. Specific topics are listed below.

                  Section         Topic                                     Page
                      A        Mission Analysis                               73
                      B        Task Analysis                                  74
                      C        Training Requirements Analysis                 76
                      D        Objectives Analysis                            79
                      E        Media Analysis                                 81
                      F        Cost Analysis                                  84
                      G        Training System Basis Analysis                 87
                      H        Preliminary Syllabus                           90


When you do it   Do your ISD analysis when you need to get information that will
                 affect your design or when you need to assess trade-offs
                 between alternatives. Do ISD analysis:

                    Before beginning design development
                    When defense system changes require instructional system
                    changes
                    When a more efficient alternative applies
                    When a more effective approach is suggested
                    When new instructional technology is to be incorporated
AFH 36-2235    VOLUME 3       1 NOVEMBER 2002                                         29




What you get       If you have conducted the analysis correctly, you will get valid
                   task details that describe training requirements and identify
                   potential alternatives for training equipment.


What you need to   Systems process is an input-process-output activity.
do it              A variety of subject matter reference data documents determine
                   what type of analysis is needed by providing input.
                   These documents identify the sources from which data is
                   necessary:

                      Defense system data
                      Similar system data
                      Input from Subject Matter Experts (SME)
                      Specialty training standards (STS)
                      Technical Orders (TO)
                      Logistics Support Analysis (LSA) data
                      Engineering data
                      Career development courses (CDC)
                      Any other reference material that helps you identify duties,
                      tasks, activities, and behaviors for a given job
                                      AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                 Section C
                                  Design

What it is       Instructional design is similar to architectural design. You
                 determine what you want the training to look like and how you
                 want it to work. The analysis that you previously conducted will
                 help determine the basic structure; not only for the training
                 system, but also for the defense system it supports. A continuing
                 effort in the design phase is the assurance of quality in the
                 design process and products with an emphasis on
                 improvements, where possible.


Why do it?       Design is conducted to save money, increase the quality of the
                 product and get the training done on time. You don‚Äôt just go
                 out and start developing instruction, just as you don‚Äôt run right
                 out and start building a house without planning and designing it
                 first.


Where to read    A detailed discussion of ISD design is available in Chapter 6.
about it         Specific topics are listed below.

                    Section                                                Page
                 Start of Development                                       94
                 Guidance Conferences                                       95
                 System-Level Development Plans                             96
                 Courseware Planning Leading to System                      100
                 Readiness Review
                 Development Activities                                     104


When you do it   Design is conducted before beginning to develop the training.
AFH 36-2235    VOLUME 3       1 NOVEMBER 2002                                       31




What you get       Proper design will result in:

                      Preliminary syllabus
                      Courseware development plan
                      Key process definition
                      Personnel to do the job
                      Training media (i.e., devices/simulators)


What you need to   For ISD design you need all of the planning and analysis phase
do it              products that you previously developed.
                                      AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                Section D
                               Development

What it is       During ISD development you make the instruction and validate it.
                 ISD development acquisition verbs include:

                    Write (print)
                    Produce (video or A/V materials)
                    Program (ICW)
                    Install (management systems)
                    Build and validate (devices/simulators)
                    Validate (instruction)
                    Revise (instruction)


Why do it?       Development is conducted to have a valid quality product ready
                 for implementation.


Where to read    A detailed discussion of development activities is available in
about it         Chapter 7. Specific topics are listed below.

                  Section         Topic                                    Page
                      A       Lesson Outlines/Flow Diagrams                 114
                      B       Lesson Strategy/Lesson Plans                  116
                      C       Storyboards                                   118
                      D       Coding, Programming, Writing                  119
                      E       Lesson Tests (Individual Tryouts)             120
                      F       Course-Level Integration Tests                123
                     G        Small-Group Tryouts                           124
                      H       Iterative Remedy and Retest                   126


When you do it   Development proceeds after design and before implementation.
AFH 36-2235    VOLUME 3       1 NOVEMBER 2002                                            33




What you get       If done correctly, you will get an instructional product that meets
                   the design specifications for the training requirement. The
                   product will be validated using students from the target
                   population, revised as required, and produced in final form.


What you need to   For ISD development you need:
do it
                      Analysis and design documents and products
                      Students and equipment for validation
                      Appropriate tools
                      Skilled personnel/subject matter experts
                                        AFH 36-2235 VOLUME 3        1 NOVEMBER 2002


                                  Section E
                                Implementation

What it is         At the ISD implementation phase you begin using the
                   instructional program to train students.


Why do it?         You have an implementation phase for instruction to assure
                   yourself that you are meeting the need.


Where to read      A detailed discussion of implementation is available in Chapter 8.
about it           Specific topics are listed below.

                    Section         Topic                                      Page
                        A        Site Training Readiness Review                 130
                        B        Implementation of System Functions             131
                        C        Full-Class Tryouts                             136
                        D        Mature System Performance Review               137


When you do it     Implementation is conducted after validation, revision, and final
                   production of the instruction.


What you get       After implementing instruction, you will have the knowledge that
                   you‚Äôve helped someone learn a job and be a successful
                   contributor to the Air Force. You will also have satisfied the first
                   objective of ISD by satisfying a training need.


What you need to   For ISD implementation you need:
do it
                      All training system functions in place
                      Total training system components in place
                      Trained instructors
                      Trained training managers
AFH 36-2235      VOLUME 3      1 NOVEMBER 2002                                        35


                                    Section F
                                    Evaluation

What it is?         Evaluation is the way to measure the effectiveness of the
                    training. Evaluation answers the questions:

                       Have the students mastered the objectives?
                       How well are the course graduates performing in the field?
                       How can the training be improved?
                       How well is the process working?


Why do it?          You conduct evaluation after implementation to gain feedback
                    internally and from the field throughout the life cycle of the
                    training to make sure graduates can still perform the job to
                    standards. Evaluation is conducted throughout the acquisition to
                    provide feedback about the quality of the ISD process and the
                    resulting products.


Where to read       A detailed discussion of evaluation is available in Chapter 9.
about it            Evaluation is discussed throughout this handbook and
                    summarized in the sections listed below.

                     Section         Topic                                    Page
                         A       Formative Evaluation                           140
                         B       Summative Evaluation                           141
                         C       Operational Evaluation                         143


When you do it      Evaluation is going on throughout the ISD process and as long
                    as the training program is in place.


What you get        Evaluation will ensure an effective and efficient ISD process with
                    quality ISD products. Evaluation will also provide you with data
                    to ensure that graduates are performing to the expected level.
                    This is done by continuous improvement in the training.
                                         AFH 36-2235 VOLUME 3   1 NOVEMBER 2002




What you need to   To properly perform evaluation you need:
do it
                   Personnel:

                      Students
                      Graduates
                      Trained evaluators
                      Field visits
                      Interviews

                   Material:

                      Systems
                      Courseware
                      Equipment
                      Questionnaires
                      Other items as required
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                           37


                                    Section G
                               Quality Improvement

Introduction          ISD is a continuous, systematic process with continuous
                      evaluation. The ISD process is the Air Force tool to ensure that
                      quality systems are built to the customer‚Äôs satisfaction. It helps
                      managers and training developers build programs that teach
                      what Air Force people need to know, when they need to know it,
                      in the most effective and most efficient manner possible. The
                      ISD process implements all of the principles of the Quality Air
                      Force (QAF) program.

                      Quality is the vehicle to ensure that training systems are built
                      and delivered customer-centered.


What it is            Quality improvement (QI) is the continuous, organized creation
                      of beneficial change. It occurs throughout the ISD process.
                      Quality improvement results in raising student performance (due
                      to training) to an unprecedented level.


Objectives of QI      The objective of QI is to foster continuous improvement in the
                      ISD processes and products and to ensure on-time development
                      of high-quality courseware that enables students to reach the
                      desired performance levels in an effective and cost-efficient
                      manner. QI occurs throughout the ISD process.


Results of QI         High quality in training product development brings:

                         Increased student satisfaction
                         Products that are easy to maintain
                         Increased ability of students to perform a job immediately
                         after training

                      High quality in training design brings:

                         Fewer errors
                         Less rework (and waste)
                         More successful training
                         Less time spent in developing new products
                         Potentially lower life cycle costs
                                           AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Test and               Test and evaluation is the check to ensure that system
evaluation             requirements are met according to the specification. It is used as
                       part of the quality process.


Quality relationship   Customers:
                         Know your customers. The information gained in the
                         mission/job analysis process gives the instructional design
                         team information that defines the customer‚Äôs expectations.
                         Focus on customers. As mentioned earlier, the needs of
                         the work center drives the instructional needs. By continuing
                         to trace the relationship between the job requirements and all
                         aspects of the instructional program, you maintain a continual
                         focus on the actual field requirements. In addition, ISD also
                         requires that the capabilities, aptitudes and attitudes of the
                         student target population be considered during the design
                         process.

                       Team Players:
                          Foster teamwork. A training program cannot be designed
                          and developed in a vacuum. In order to develop effective
                          training, the design team must include representatives from
                          the work center and evaluation offices. This helps ensure
                          that the training matches the performance requirements of the
                          job.
                          Empower your people. ISD is a problem solving, decision-
                          making model. The flexibility of the process, combined with
                          the fact that there are any number of ways to solve a given
                          training problem, requires that design teams be allowed
                          freedom and authority to design, develop, and implement
                          training that meets job performance requirements.

                       Final Product:
                          Know your mission. ISD depends on mission and job
                          analysis for basic data. All instruction must be based directly
                          on mission or job requirements. The checks in the process
                          help eliminate instruction not related to the job.

                          Job analysis uses data from many sources, including mission
                          statements found in regulations or locally developed
                          statements. Analysts also make use of management

                                                                     Continued on next page
AFH 36-2235      VOLUME 3     1 NOVEMBER 2002                                      39


Quality relationship   engineering reports, occupational survey data, and direct
(Continued)            observation to determine the actual job requirements.

                       As part of the job analysis process, a training needs
                       assessment is conducted to arrive at the actual performance
                       problem. In some cases, a problem is not related to lack of
                       training, but to a problem with the job structure or
                       environment. The ISD process helps ensure that you don‚Äôt
                       build a training program for a non-training problem.
                       Set goals and standards. Goals and standards for an
                       instructional development effort come in many variations.
                       First, the job requirements and the impact of the performance
                       deficiency determine the timing required for the development
                       process and the conduct of the instructional program.
                       Second, the content of the training is determined by the job
                       performance requirements. The design team should directly
                       translate the cues, conditions, and performance standards of
                       the job directly into the instructional program.
                       Manage by fact. Each phase of the ISD process requires
                       constant evaluation against the job requirements identified
                       earlier in the process. In addition, a variety of tools have
                       been developed to help ensure that design and development
                       decisions are made with supporting data. For example, a
                       number of media selection tools are being used that provide
                       managers with information that matches training media with
                       the training requirements. These matches are based on
                       learning theories and development cost factors (money and
                       time). ISD is designed to guide managers and developers to
                       awareness of factors affecting their decisions.
                       Integrate quality in all phases. Evaluation is continuous
                       quality checking. This is true during each phase of the ISD
                       process, from analysis to implementation. Built-in checks in
                       each phase ensure the quality of the ISD process and
                       products. The emphasis is on satisfying the job performance
                       requirements and producing graduates who can do their jobs.
                       Evaluate quality constantly. The ISD process is a cyclical,
                       ongoing process of continuous improvements. As curriculum
                       developers progress through the different phases of ISD, the
                       process and products of each phase are constantly evaluated
                       against the job requirements and principles of learning. The
                       results of the evaluations determine which phase of ISD to
                       enter next. Constant evaluation identifies changes in job
                       requirements due to updates in equipment and personnel,
                       which results in new ISD efforts to provide the best possible
                       training to Air Force personnel.
                                       AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




Basis of process   The basis of process improvement is Quality Air Force. QAF is a
improvement        management philosophy and a methodology that work together
                   to produce continuous process improvements. It is based on the
                   following principles.

                      All work is a process.
                      Processes receive work from suppliers, add value, and
                      deliver output to customers.
                      Anyone from whom a process receives work is a supplier.
                      Anyone to whom a process delivers output is a customer.
                      Customers have needs and expectations.
                      Customers will define and measure quality in terms of those
                      needs and expectations.
                      Quality is meeting customer needs and expectations.
                      Improving process quality increases productivity.
                      Processes can be identified, understood, measured, and
                      improved.
                      The people who operate the processes know best how to
                      improve them.


Procedure for      In order to ensure process improvements, you will need to use a
process            systematic method to identify and correct the causes of the
improvement        problems. The six steps of process improvement are outlined in
                   the following table.

                    Step          Activity
                     1     Define the process and determine the main problem
                           areas.
                     2     Analyze the problems and identify the causes of each.
                     3     Identify and evaluate possible changes to the process.
                     4     Implement the changes and monitor the process.
                     5     Institutionalize the changes.
                     6     Repeat for continuous improvements.
AFH 36-2235    VOLUME 3       1 NOVEMBER 2002                                     41




Ways to implement   There are many different ways to implement the basic procedure
the procedure       mentioned above. Two of the ways are:

                       "Chart it, check it, change it"
                       Shewhart Cycle (plan-do-check-act)

                    Each of these techniques uses the six basic steps mentioned
                    above.
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                           Chart It, Check It, Change It


What it is      "Chart it, check it, change it" is a simple phrase that summarizes
                one of the ways to implement the procedure. It is a systematic
                approach to continuous improvement. This approach has three
                principal steps, as shown below and in Figure 6.

                Step            What You Do
                1. Chart          Describe the process.
                                  Gather data.
                2. Check          Analyze the data.
                                  Evaluate the process.
                                  Identify opportunities.
                3. Change         Improve the process.
                                  Institutionalize the change.


How to use it   Figure 6 Chart It, Check It, Change It


                                                CHART IT




                                                CHECK IT




                                               CHANGE IT




                Chart It

                   Using a process flowchart, describe the process to be
                   improved.
                   Gather data on the process and its products.

                                                                 Continued on next page
AFH 36-2235     VOLUME 3       1 NOVEMBER 2002                                      43




How to use it      Check It
(Continued)
                      Analyze the data to isolate the problems and opportunities.
                      Evaluate the process to identify alternative approaches.
                      Identify opportunities (i.e., useful changes) from the
                      alternatives.

                   Change It

                      Improve the process by implementing changes identified as
                      opportunities.
                      Institutionalize the changes through training, standardization,
                      and other means. Then, use another process (or use this
                      same one again) to make further improvements.
                                           AFH 36-2235 VOLUME 3    1 NOVEMBER 2002


                                   Shewhart Cycle

 What it is       The Shewhart Cycle is a systematic approach to achieving a
                  continuous improvement in quality. The cycle includes planning,
                  doing, checking, and acting.


 Graphic          Figure 7 Shewhart Cycle
 representation




                                              ACT        PLAN
                                             ON THE       AN
                                            RESULTS    APPROACH




                                             CHECK         DO
                                              THE          THE
                                            RESULTS      ACTIVITY




 How to use it    Because the approach involves repetition, it is represented
                  graphically as a circle in Figure 7.

                  To use the Shewhart Cycle, follow the steps listed below.
                     Plan an approach for quality improvement. Study the process
                     flow and any existing data. Formulate possible improvements,
                     experiments to be run, or additional data to be gathered.
                     Do the activity planned. Implement the improvement effort that
                     you planned. Train the people who are responsible for
                     implementation.
                     Check the results. Measure the results of the improvement
                     effort you implemented. Analyze the data you collected.
                     Act on the results. If the effort was truly an improvement,
                     standardize and document it. If it was not successful, determine
                     what could be done to improve it.
                     Repeat. Continue around the cycle again by planning and
                     carrying out further activity.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                           45


                                 Section H
                        System Engineering Interaction
Introduction          Nothing in the ISD process occurs in a vacuum or at least it
                      shouldn‚Äôt. It‚Äôs extremely important that ISD interface with other
                      defense system acquisition/life cycle support functions
                      continuously. One important way that the ISD process meshes
                      with the defense system is through interacting with system
                      engineering. An "interaction" is a two-way street: ISD and
                      system engineering communicate and support each other. But
                      why is it important and how does it happen?


What it is            A system is a composite of skilled people and equipment
                      (hardware and software) that provide an operational capability to
                      perform a stated mission.

                      ISD is the systematic process employed to design and develop
                      training for a defense system. It is used to identify training
                      requirements, to determine appropriate media for training, and to
                      design, develop, implement and evaluate training and training
                      materials for defense systems.

                      The system engineering process is a logical sequence of
                      activities and decisions transforming an operational need into a
                      description of system performance parameters and a preferred
                      system configuration.


Relationship of ISD   ISD and system engineering are two complementary processes
and system            that are used to design and develop training systems for defense
engineering           systems. The processes have many similarities and each
                      process accomplishes functions not accomplished by the other.


Importance of ISD     System engineering must consider personnel, the skills they
to system             require, and the training program to teach these skills as integral
engineering           parts of the defense system. Failure to integrate ISD into system
                      engineering can result in an inadequately supported system.

                      System engineering addresses those training system design
                      issues having to do with translation of training system functional

                                                                    Continued on next page
                                          AFH 36-2235 VOLUME 3    1 NOVEMBER 2002




Importance of ISD   requirements (stated by ISD) into hardware and software. It
to system           considers the defense system hardware, software, support
engineering         equipment, operations, and maintenance concept. System
(Continued)         engineering examines new technology, similar systems, and
                    existing systems to arrive at a functional description of the
                    system in terms of hardware and software requirements. The
                    system engineering process is used to produce the management
                    and design decisions and data upon which the training system is
                    based. ISD alone cannot fulfill all the needs of a total training
                    system.


Interaction         ISD and system engineering are mutually supporting facets of a
                    defense system acquisition and life cycle. All individuals
                    involved with acquisition must ensure that ISD is considered in
                    system engineering and vice versa. Many avenues exist for this
                    interaction. Among them are:

                    Personnel:

                       Training planning team
                       Technical interchange meetings

                    Plans:

                       System training plan
                       Logistic support plans
                       Test plans
                       Program development plans

                    Reports:

                       Integrated Manpower, Personnel And Comprehensive
                       Training & Safety (IMPACTS)
                       Requests for Proposal (RFP)

                    Other Considerations:

                       Logistic support analysis
                       Quality control
                       Design reviews
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                       47


                           Chapter 3
                 CONTRACTOR-DEVELOPED TRAINING
                                    Overview


Introduction       During the first two chapters, you were given a basic background
                   of ISD and were introduced to some of the concepts that apply.
                   It was stressed that ISD is a flexible process, always employing
                   the principles of continuous or quality improvement. You have
                   seen how training system requirements must be considered early
                   in the cycle of defense system acquisition. Teamwork,
                   communication, and coordination have been stressed frequently.
                   But at this point, a contract has not yet been awarded and could
                   still be a long time away. In fact, you learned in the previous
                   chapter that a TPT has been formed and that the TPT
                   determines the approach to take for acquiring training to support
                   the defense system. Sometimes it may be in the best interests of
                   the Air Force, for the Air Force to develop the training
                   themselves. If the decision is to have contractors develop all or
                   part of the training, another series of events, processes and
                   activities begins, leading to contract award. It is very important
                   that you know what occurs before contract award because
                   several key program office actions will occur during this time,
                   ultimately affecting the team and the ISD process.


Purpose            The purpose of this chapter is to explain the major tasks that
                   occur during pre-award of a training system contract and to focus
                   on the contractor and government responsibilities, along with the
                   ISD implications.


Where to read      This chapter contains four sections.
about it

                    Section         Title                                   Page
                        A       Acquisition Planning                          49
                        B       Request for Proposal Development              54
                       C        Proposal Writing                              57
                       D        Source Selection                              59
                                     AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




Special          As mentioned above, there will be occasions when it is best for
considerations   the Air Force to develop its own training. One area that needs
                 special consideration is Air Force developed maintenance
                 training. Another key consideration is interactive courseware
                 (ICW). There will be occasions where you will need to acquire
                 ICW. Because of their importance, these two areas are covered
                 in separate chapters as listed:

                    Chapter Title                                       Page
                 Air Force-Developed Maintenance Training                145
                 Interactive Courseware (ICW)                            159


ISD phase        At this stage, you are conducting planning, preceding the
                 analysis phase of ISD. The updated Air Force ISD model is
                 shown in Figure 8.

                 Figure 8 Updated ISD Model
AFH 36-2235   VOLUME 3      1 NOVEMBER 2002                                        49




Tasking          Throughout the rest of this handbook, various tasks and
                 responsibilities will be described. They will normally be listed as
                 either "Contractor Tasks" or "Air Force Tasks." Never assume
                 that task listings are complete. As you learned earlier, each
                 defense program varies depending on such things as funds,
                 scope and time. Also, note that while all USAF tasks are listed
                 as "Air Force Tasks," they could be performed by different
                 agencies of the USAF, again depending on the program. For
                 example, Air Education and Training Command (AETC) may
                 have various tasks and other responsibilities at different stages
                 in the ISD and acquisition phases. These requirements should
                 be specified and agreed to in a Memorandum of Agreement
                 (MOA) signed by the using command, program office and AETC.
                                        AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                 Section A
                            Acquisition Planning

Introduction       A training system concept has been published that describes the
                   desired overall training philosophy, principles and capabilities.
                   At this point, the Air Force has decided that it will pursue a
                   training contract and now must take steps to prepare and award
                   a contract. Acquisition planning is where considerable
                   communication, assessments, strategy sessions and preliminary
                   planning will take place.


Purpose            The purpose of acquisition planning is to develop preliminary
                   concepts, plans and strategies for the acquisition. By completing
                   this task, the Air Force will be assured that the acquisition
                   strategy is best suited to the purpose, meets user needs, meets
                   regulations and other constraints, and ideally "covers all the
                   bases."


Acquisition life   Before going any further, look again at the diagram of the
cycle              acquisition life cycle milestones and phases (Figure 9).
                   Acquisition planning is accomplished at each phase. The more
                   defined the defense system becomes, the closer ISD activities
                   can come to actual training system design. The ISD analysis
                   phase is typically part of acquisition planning for Phase I and is
                   repeated again for Phase II. Although training systems are
                   considerations in Phase 0, little ISD analysis is done before
                   Phase I.

                                                                  Continued on next page
AFH 36-2235        VOLUME 3               1 NOVEMBER 2002                                                                                                                           51




Acquisition life      Figure 9 System Acquisition Life Cycle
cycle
(Continued)
                                Determination
                                      of
                                                                                        Acquisition Life Cycle
                                Mission Need
                                                                                       Milestones and Phases


                                          Phase 0                            Phase I                         Phase II                   Phase III                Phase IV

                                            Concept                        Demonstration                  Engineering and                  Production                  Operations
                                         Exploration and                         and                       Manufacturing                       and                        and
                                           Definition                        Validation                    Development                    Deployment                    Support




                           Milestone 0                     Milestone 1                     Milestone II                 Milestone III                   Milestone IV

                              Concept                        Concept                                                                                        Major
                               Studies                     Demonstration                    Development                   Production                      Modification
                              Approval                       Approval                        Approval                     Approval                         Approval




                                                                                                                                                          As Required




Contractor tasks      During acquisition planning, the potential contractors are doing
                      everything possible to learn about the potential contract. This
                      includes, but is not limited to:

                      -   Marketing capabilities to using command(s) and program
                          office
                      -   Gathering intelligence data such as:
                           - Funding profile
                           - Users‚Äô perceived requirements
                           - Program office‚Äôs perceived requirements
                           - Political advantages and constraints
                           - Potential follow-on work
                      -   Identifying strengths and weaknesses
                      -   Evaluating teaming trade-offs
                      -   Evaluating profit potential
                      -   Evaluating cost/schedule/performance risks
                      -   Listing pros/cons of bid/no bid decision
                      -   Responding to Requests for Information (RFI) and Searches
                          for Information (SFI)
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Air Force tasks   At this point, the using command will ensure that the SME
                  support principles are agreed to by all levels. While the potential
                  contractors are gathering data and going through their various
                  actions, the program office is becoming very busy in getting this
                  potential program moving. Some actions that they are taking
                  include:

                  -   Engage the using command.
                      - Trigger update of training system concept.
                      - Get user to define system functional requirements
                      (management, administration, support, delivery of instruction,
                      ISD, QI).

                  -   Engage industry by issuing SFI.
                  -   Perform preliminary program risk assessment.
                  -   Write training system life cycle master plan (top-level system
                      management strategy).
                  -   Engage Program Element Monitor (PEM).
                      - Negotiate program-funding profile.
                      - Define test agency responsibilities.
                      - Identify major program constraints.
                      - Negotiate Program Management Document (PMD).

                  -   Assess development risk.
                  -   Define acquisition strategy.
                      - Identify alternative acquisition strategies.
                      - Examine feasibility of a phased approach.
                      - Define Training System Requirements Analysis (TSRA)
                      requirements.
                      - If any, which TSRA tasks?
                      - Should there be a separate TSRA contract?
                      - Lock in strategy at this point.
                      - Write acquisition plan.

                  -   Write baseline concept description supporting integrated
                      weapon system management.
                      - Establish SME support principles.

                                                                Continued on next page
AFH 36-2235       VOLUME 3      1 NOVEMBER 2002                                         53




Air Force tasks      Having been involved up-front with the TPT, AETC could (if
(Continued)          agreed to in Memorandum of Agreement):
                     - Develop and coordinate AETC participation plan.
                        - Specify methodology:
                            - Training development or quality assessment
                            - Training assessment during Operational Test and
                            Evaluation (OT&E) (AETC will assess whether the
                            maintenance program meets user needs)
                        - Specify personnel and other resources.
                        - List data requirements.

                     -   Review listing of data items to be considered when a new Air
                         Force training program or course is to be developed under
                         contract.
                         - Review MIL-STD-1388-2A and -2B for Logistics Support
                         Analysis (LSA) data requirements.


Military standards   You read that AETC is reviewing military standards (MIL-STD).
                     MIL-STDs are documents issued within the Department of
                     Defense (DoD) in accordance with the basic policy of the
                     Defense Standardization Program (MIL-STD-962). MIL-STDs
                     establish engineering and technical requirements for items,
                     equipment, processes, procedures, practices and methods that
                     have been adopted as standards. MIL-HDBK-29612 is for
                     military training programs.


MIL-HDBK-29612       Since MIL-HDBK-29612 is limited in scope, you need to refer to
                     Attachment C to understand the cross-walk for total training
                     systems acquisition (AFMC/ASC process) and MIL-HDBK-29612.
                     The ultimate goal of MIL-HDBK-29612 is to enable the
                     Government to identify more accurately the data or information
                     that the Government must have to fulfill a training requirement.
                     Because the standard has been prepared for joint service use,
                     understanding how to tailor the task descriptions and data
                     requirements cited in MIL-HDBK-29612 is critical. Failure to
                     tailor accurately and intelligently will result in performance of
                     tasks and purchase of data that either extend significantly
                     beyond the minimum scope of the original training requirement or
                     do not meet the needs of the end user, while unnecessarily
                     escalating

                                                                  Continued on next page
                                      AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




MIL-HDBK-29612   the cost. The contracting activity and training activity must work
(Continued)      as a team to tailor the tasks and Data Item Descriptions (DID)
                 cited in this standard to meet Service-specific needs.


Agreements       During this period, a MOA may be developed between the user
                 and the SPO defining subject matter expert support.


Data             The SPO should have completed the acquisition plan detailing
                 the parameters of the acquisition. Data plans may indicate a
                 need to use consolidated DIDs or use a one-time DID written
                 specifically for the program. You can contact your training
                 engineering support office for assistance in tailoring the RFP to
                 meet your acquisition strategy.
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                           55


                                   Section B
                       Request for Proposal Development

Introduction          Initial planning for contract development has been completed
                      and revisions will be ongoing based on training system concept
                      updates, training system life cycle master plan, risk and strategy
                      documents. The Request for Proposal (RFP) must now be
                      developed.


What it is            The RFP is an acquisition package soliciting design and
                      development proposals for new or updated systems from
                      contractors. The document generally consists of:

                         Executive summary
                         Instructions to offerers
                         Requirements documents
                         Statement of work
                         Contract Data Requirements List (CDRL)
                         Model contract
                         Other special items

                      The release of the RFP is the official start of the contracting
                      process.


Contractor tasks      During this stage, the potential contractors are continuing their
                      intelligence gathering operations and trying to "scope out" the
                      potential contract as much as possible. They have now begun to
                      talk with SPO personnel concerning:

                         Specific requirements
                         "Show stoppers"
                         Government prejudices or peculiarities

                      Note: Sometimes the Air Force will issue a draft RFP to potential
                      contractors to get them more in line with the acquisition strategy
                      and to give the contractors an opportunity to give feedback to the
                      Air Force. The draft RFP is not binding in any manner. A second
                      draft release before final RFP clarifies any misunderstandings
                      before going into source selection rules.

                                                                     Continued on next page
                                        AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Contractor tasks   If a draft RFP has been received, the potential contractors will:
(Continued)
                      Review and comment on the RFP.
                      Attempt to influence Government (not to give them the
                      contract, but to make various changes to the specifications
                      such as to improve performance, cost, and quality).
                      Negotiate potential teaming agreements.
                      Generate preliminary system design.


Air Force tasks    The program office is now busy writing various documents
                   specifying system functional requirements, reflecting the user‚Äôs
                   requirements and constraints. These include:

                      System Requirements Document (SRD)
                      Statement of work inputs
                      Instructions to offerers
                      Requirements correlation matrix

                   In addition, Acquisition Strategy Panel (ASP) decisions affecting
                   the Training System Requirements Analysis (TSRA) are
                   documented. These include:

                      Reasons for doing TSRA
                      How results precede and feed subsequent program phases
                      Rationale for separate TSRA contract or pursuit of phased
                      program

                   If tasked, AETC will also develop a "strawman" concept of
                   system life cycle training using their experience and familiarity
                   with similar systems. For example, the "strawman" concept of life
                   cycle training will include AETC‚Äôs estimate of:

                      Training requirements for the life cycle of the defense system
                      Recommended training methods and media
                      Anticipated levels of the training needed by the workforce
                      Specific Air Force specialties needing training
                      Career point at which the training will likely be required

                                                                  Continued on next page
AFH 36-2235       VOLUME 3      1 NOVEMBER 2002                                          57




Air Force tasks      This "strawman" training concept will be used as a baseline for
(Continued)          comparison of the contractor proposals. In many respects the
                     "strawman" concepts cover many of the same areas that would
                     be covered in the early development stages of a system-training
                     plan.

                     AETC could also review the draft RFP for concerns such as the
                     following:

                     Requirements:

                             Are overall data requirements adequate?
                             Is the contractor required to warrant data for accuracy and
                             completeness?
                             Have Government Furnished Property (GFP) and
                             Government Furnished Equipment (GFE) been clearly
                             identified?
                             Are requirements for government quality assurance
                             included?
                             If Computer-Based Instruction (CBI) is required, has it
                             been considered as an acquisition?

                     Other Considerations:

                             Are milestones related to the defense system events
                             rather than calendar dates?
                             Have availability and supportability of GFP/GFE been
                             planned?
                             Have technical data requirements been included?
                             Is digitized data or Computer-Aided Acquisition Logistics
                             Support (CALS) considered?
                             Have facilities and classrooms been considered?
                             (Facilities must be considered up front as part of the
                             acquisition.)
                                        AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                  Section C
                               Proposal Writing

Introduction       A lot of work has gone into acquisition planning and RFP
                   development to produce a draft RFP. Now the RFP will be
                   refined, rewritten in final form and distributed, and potential
                   contractors will write and submit proposals. The contractor will
                   go through many steps to ensure that the proposal is the best
                   document possible, reflects the contractor‚Äôs capabilities, and is at
                   a fair, competitive price. To write the proposal, the contractor
                   must ensure that several actions are done.

                   Note: Before the final RFP is delivered to the potential
                   contractors, the SPO and using commands will sponsor bidders‚Äô
                   conferences to review the draft RFP and conduct question and
                   answer sessions. Once this is completed and draft RFP
                   shortfalls are remedied, the formal RFP is delivered. A second
                   draft RFP ensures that remedies are satisfactory before final
                   RFP release.


Purpose            The purpose of proposal writing is for industry to prepare a
                   document for the Air Force. The document should convince the
                   AF contracting officer that the contractor‚Äôs company can produce
                   the best quality product that meets or exceeds the minimum
                   requirements, within specifications, at a fair price, and will be
                   delivered on time.


Contractor tasks   In the proposal, the contractor must show an understanding of
                   the requirements and the intent to comply. The contractor must
                   also include a top-level training system specification showing
                   allocated functional requirements. Source data requirements
                   and tasks must be identified to include:

                      Identification of associate contractor agreements
                      Identification of simulator data integrity standards

                                                                  Continued on next page
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                          59




Contractor tasks      The first major product that the potential contractor prepares is
(Continued)           the top-level System Engineering Management Plan (SEMP).
                      This includes plans for development of:

                         Hardware
                         Software
                         Courseware
                         Firmware

                      In addition, the contractor will prepare the System Engineering
                      Master Schedule (SEMS), write training system specifications,
                      and identify source data requirements.


Air Force tasks       The SPO will conduct bidders‚Äô conferences, discussing draft
                      proposals with the contractor. The SPO will write evaluation
                      standards and identify the source selection team. The using
                      commands and AETC will support the SPO at the bidders‚Äô
                      conferences as required.
                                       AFH 36-2235 VOLUME 3     1 NOVEMBER 2002


                                   Section D
                                Source Selection

Synopsis          At this stage, the Air Force will review all proposals using
                  guidance developed by the SPO. The using commands and
                  AETC should participate in this process. During this process, the
                  Air Force will try to determine the contractor‚Äôs understanding of
                  the total system functional integration. The using commands will
                  ensure that the training concepts are understood and represent
                  users‚Äô interests.


Air Force tasks   Source selection will follow a predetermined and documented
                  schedule of activities. The SPO, working with the using command
                  and AETC, will review many areas. Some questions they may
                  ask include the following.

                  Feasibility

                     Can the contractor really do what is proposed in a quality
                     fashion within schedule and budget?

                  Conformance

                     Does the proposal meet requirements of the RFP?

                  Coverage of User Requirements

                     Will this training satisfy the customer?
                     Is it what the customer "ordered?"

                  Consistency

                     Is planned training consistent with defense system operation
                     and maintenance concepts?

                  The SPO will also conduct a training system capability and
                  capacity review, if appropriate.


Contract award    After all the above actions are completed and preview steps
                  satisfied, a contract is awarded. Once the contract is awarded,
                  considerable work will begin using the ISD process.
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                          61


                                   Chapter 4
                                   PLANNING
                                    Overview


Introduction       While reading about the total training system earlier (page 15),
                   you learned that a key ingredient to success is planning.
                   Planning in defense system acquisition consists of many tasks
                   that occur early in the acquisition cycle and continue throughout
                   all phases.


Purpose            The purpose of this chapter is to explain the major tasks in
                   performing planning. While you will not become a planning
                   expert by reading this chapter, you will know the concepts and
                   processes necessary for success.


Where to read      This chapter contains three sections.
about it
                    Section         Title                                   Page
                        A       Assess Instructional Needs                    63
                        B       Develop Overall Outline                       65
                       C        Define Planning Requirements                  67
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Which ISD phase?   You have not yet entered an ISD phase but are doing preliminary
                   work. You are still in a planning stage, focusing on training while
                   in the management function. This function is highlighted in
                   Figure 10.

                   Figure 10 Updated ISD Model




Contractor tasks   Since the defense system contractor has been contracted to
                   produce and deliver a system, that contractor plays a large role
                   in planning for training. The contractor is responsible for
                   conducting all the steps in planning while coordinating with the
                   program office and primary operating commands. The
                   acquisition contract may be for courseware or devices, on-site
                   training, type 1 training, or a total training system. (For
                   information about type 1 training, see AFI 36-2201.)
AFH 36-2235       VOLUME 3      1 NOVEMBER 2002                                           63




Air Force tasks      Throughout the planning stage, the program office will review all
                     contractor products and design decisions, working with the
                     contractor informally to make required changes, while elevating
                     areas of concern.

                     The primary operating command supports the contractor with
                     SME advisors from staff, line and supporting positions; they
                     advise regarding training doctrine and provide the practical field
                     experience.

                     Air Education and Training Command provides support, as
                     requested, by reviewing contractor products and elevating
                     concerns. Specific areas of interest to AETC are discussed later
                     in this chapter.
                                           AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                   Section A
                           Assess Instructional Needs

Introduction          Instructional need has been defined many ways but is generally
                      considered to be the discrepancy or gap between desired
                      performance and current performance. In the acquisition
                      business, it is defined as the process of identifying the problem,
                      documenting any shortfall and developing a solution.
                                  An instructional need exists when an
                                employee lacks the necessary knowledge,
                                 skills or attitudes to perform a required


Purpose               The purpose of assessing instructional needs is to validate a
                      need for training in a defense system acquisition.


Stages of             The three basic stages of assessing instructional needs are:
assessing
instructional needs      Define the problem.
                         Document the deficiency.
                         Develop the solution.
AFH 36-2235      VOLUME 3     1 NOVEMBER 2002                                       65




Basic steps of      The three overall stages can be broken down into six or more
analysis            basic steps. You can modify these as necessary.

                       Step      Analysis Activity
                        1        Define the Objectives. Why are you doing the
                                 analysis? What is your goal?
                        2        Identify Data Requirements. What kind of data
                                 will you need to review to conduct the analysis?
                        3        Select Methods of Gathering Data. How will you
                                 obtain the data you need? Examples are:
                                 ‚Ä¢ Use questionnaires.
                                 ‚Ä¢ Make observations.
                                 ‚Ä¢ Conduct interviews.
                                 ‚Ä¢ Review records/reports.
                                 ‚Ä¢ Analyze work samples.
                                 ‚Ä¢ Conduct tests.
                        4        Gather Data. Obtain, organize and catalog data.
                        5        Conduct Analysis of Data. How does this data
                                 compare with skills required? Is your data
                                 accurate? Should you double check and verify?
                        6        Prepare Reports. What reports are required?
                                 Where do you submit them? Is any follow-up
                                 required? Did you meet your objectives? What
                                 kind of training is required?
                                         AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                  Section B
                            Develop Overall Outline

Introduction        At this point in the planning phase, you have assessed
                    instructional needs and determined that there will be a training
                    requirement for the product that you are acquiring. But you have
                    also found out that this "product" will create a lot of unknowns in
                    the training and support areas if you don‚Äôt get a handle on it now.
                    For these reasons, you need to develop an overall outline.


What it is          An overall outline is defined as a macro, rough profile of what‚Äôs
                    going on and what needs to be done. This outline gives the "big
                    picture" of the entire training system.


Structure           There is no required structure to the outline. It will vary
                    depending on the size and complexity of the system being
                    acquired and the training required.


How to develop an   The best way to start in developing the outline is to review the
outline             contract and the SOW. Developing the outline is similar to
                    developing a Work Breakdown Structure (WBS). A WBS is a
                    hierarchical ordering of work activities and products which is
                    used in detailed planning of work activity units and in costing for
                    proposals and contracts. Your outline need not be as detailed as
                    a WBS, but the areas to consider are similar. Begin with the
                    training system at the top. Next, list broad categories of things
                    that comprise the system. Then consider each of these things
                    and list the activities that go into them. Continue this "pyramid
                    building" until the desired level of detail is reached.


What to consider    Items to consider could include, but are not limited to:

                    Objective:

                       Training equipment
                       Courseware
                       Trained personnel requirements

                                                                   Continued on next page
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                         67




What to consider         Support equipment
(Continued)              Training sites
                         Facilities
                         Use of operational aircraft/equipment
                         Aircraft data/parts

                      Subjective:

                         Instructors
                         Defense system schedules
                         Test plans
                         Desired instructional approach


Other conditions      When developing the outline, consider how the components of
                      the training system interrelate with each other and the defense
                      system. Progress, delays or changes in the defense system will
                      have an effect on the training system and vice versa.
                                       AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                              Section C
                    Define Planning Requirements

Introduction     Now that you have an outline of the "big picture," you need to
                 define planning requirements. The training development process
                 may require more than one plan. But don‚Äôt let this scare you
                 away. If done correctly, the selected plans will follow an orderly
                 process and will fit into the overall picture like pieces of a puzzle.
                 A "plan" can be a one-page document or a 200-page binder. It
                 depends on the system, the program and the subject.


Where to start   In defining planning requirements, the first question to ask is:
                 "What plans will be required?"


Questions to     If you can answer the question above, you are on the right track.
answer           If you cannot, then further study may be necessary. Find the
                 answers to questions such as:

                    What activities need advance preparation?
                    What do you need to know and have in place in order to do
                    these activities?
                    Is this information needed for the training system
                    implementation plan?
                    Is this information needed for the System Engineering
                    Management Plan (SEMP)?
                    Will there be a courseware/ISD development plan to guide
                    your process?
                    How will you know the training system quality?
                    Will you need a test and evaluation plan?


Who develops     After answering the above questions, you can concentrate on the
plans?           required plans. You need to remember that no single person
                 develops all plans. In a major acquisition, there will be several
                 personnel or several teams of Air Force and contractor personnel
                 developing plans at any given time.

                 Remember to coordinate and communicate.
AFH 36-2235       VOLUME 3     1 NOVEMBER 2002                                           69




Example plans        You touched on some plans that may be important. Some plans
                     you may see are:

                        System training plan
                        Training system implementation plan
                        SEMP
                        Courseware/ISD development plan
                        Training system test and evaluation plan
                        ISD management plan


Air Force tasks      During and following the planning phase, the following actions
                     will occur. The SPO will:

                        Review all contractor products and design decisions.
                        Work with the contractor to effect required changes.
                        Elevate areas of concern.

                     The using command will:

                        Provide SME support as required. (SMEs could be utilized
                        from all applicable positions, such as staff, line and support
                        positions.)

                     AETC will review the ISD management plan to ensure that:

                        ISD methodology provides for sound, objective, systematic,
                        and traceable training decisions.
                        Contractor-planned training meets user-defined requirements.

                     AETC will also identify areas for comparative analysis.


Data                 The contractor may deliver the various plans in accordance with
                     the contract, but plans will normally be delivered following the
                     planning stage. This will include the ISD management plan and
                     the training system implementation plan.
                                                        AFH 36-2235 VOLUME 3               1 NOVEMBER 2002




Metrics           Metrics are standards of measurement or quality indicators that
                  are critical in the acquisition and ISD processes. The purpose of
                  metrics is to give qualitative and quantitative evaluations of
                  processes and products throughout the development of
                  courseware. Acquisition metrics track key factors such as cost,
                  schedule, and performance. Types of metrics include:

                     Qualitative
                       Quality assurance
                       Evaluation criteria
                       Subject matter expert review
                       Quality control
                       Traceability
                       Format guide
                     Quantitative
                       Personnel/skill allocation
                       Schedule
                       Tracking by lesson
                       Action items
                       Test and evaluation data


Instructional     Figure 11 represents a typical metrics development and
systems metrics   measurement process. You may modify it as necessary to fit
overview          your needs.

                  Figure 11 Instructional Systems Metrics Process




                                                                                                 Measure and
                           Select                  Identify                    Measure and
                                                                  Establish                       Describe
                           Phase                 Qualitive and                  Describe         Comparative
                                                                 Comparative
                         To Measure               Quantitive                     Desired
                                                                  Standard                        Standard
                                                  Measures                     Performance       Performance




                                                                                                 Compare Actual
                                                  Analyze                                           vs. Desired
                                                                  Develop                        Performance with
                                                 Results and        and        Recommend           Comparative
                                                  Revise as      Implement      Changes              Standard
                                                 Applicable       Changes                         Performance to
                                                                                                   Identify Gaps




                                      FEEDBACK
AFH 36-2235       VOLUME 3       1 NOVEMBER 2002                                          71




Metrics in this       To assist you in your qualitative and quantitative evaluations,
handbook              some metrics information is provided throughout this handbook
                      at appropriate places. The first application of metrics applies
                      here, following completion of various planning activities.


Metrics in planning   Earlier in this chapter, you were advised of the Air Force tasks.
                      In completing these tasks, the Air Force needs to consider
                      performing the following activities:

                      -   Review all TSRA deliverables incrementally as drafts are
                          written.
                      -   Compare percentage of completion of draft products vs.
                          completion timeline/milestones.
                      -   Evaluate percentage/amount and types of personnel
                          proposed for each development phase vs. AF experience
                          base. Has the contractor budgeted personnel for the
                          corrections required following individual tryouts?
                                       AFH 36-2235 VOLUME 3   1 NOVEMBER 2002


                                Chapter 5
                                ANALYSIS
                                  Overview


Introduction    You should now have available the system training plan or the
                ISD management plan.

                In analysis, the Air Force conducts assessments or monitors
                instructional designers as they conduct various analyses such as
                mission analysis, task analysis, and media analysis.


Purpose         The purpose of this chapter is to describe:

                -    Types of analyses and when they should be conducted
                -    Related plan updates


Where to read   This chapter contains eight sections.
about it

                    Section      Title                                 Page
                      A       Mission Analysis                           73
                      B       Task Analysis                              74
                      C       Training Requirements Analysis             76
                      D       Objectives Analysis                        79
                      E       Media Analysis                             81
                      F       Cost Analysis                              84
                      G       Training System Basis Analysis             87
                      H       Preliminary Syllabus                       90
AFH 36-2235   VOLUME 3     1 NOVEMBER 2002                                   73




ISD phase        You are now in the analysis phase. An ISD model, with the
                 analysis phase highlighted, is provided in Figure 12.

                 Figure 12 Analysis Phase
                                       AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                 Section A
                              Mission Analysis

Introduction       Mission analysis and task analysis normally go hand-in-hand.
                   Mission analysis generally precedes task analysis. A mission
                   analysis will provide basic data on system functions, types of
                   equipment, maintenance requirements, educational goals and
                   other information.


What it is         Mission Analysis is defined as a process of reviewing mission
                   requirements, developing collective task statements, and
                   arranging the collective tasks in a hierarchical relationship.


Contractor tasks   Many tasks are conducted during a mission analysis. Some
                   tasks will occur concurrently, others subsequently. These tasks
                   can include the following:

                      Collect mission data.
                      Perform literature search.
                      Conduct interviews.
                      Conduct on-site visits/reviews.
                      Compare new system to existing system.
                      Compare new system to similar system.
                      Document mission descriptions.
                      Create traceability database.


Air Force tasks    During this stage, the SPO will continue dialogue with the
                   contractor, reviewing products and giving guidance as necessary.
                   As delegated, the user and AETC will review contractor
                   deliverables for validity, process conformance, technical
                   accuracy and completeness.


Data               A report is due at the completion of task analysis, which is
                   discussed in the next section.
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                          75


                                      Section B
                                    Task Analysis

Introduction          Every job in the Air Force consists of tasks that comprise the job.
                      The goal of task analysis is to zero in on the target population or
                      the group needing to be trained and determine what they need in
                      order to do the job.


What it is            The best way to define task analysis is to break it down into two
                      parts.

                         Job Task Analysis - A process of examining a specific job to
                         identify all the duties and tasks that are performed by the job
                         incumbent at a given skill level.
                         Training Task Analysis - The process of examining each
                         unique unit of work from the job task analysis to derive
                         descriptive information used in the design, development, and
                         testing of training products.


Contractor tasks      Task analysis is a more in-depth analysis compared to mission
                      analysis. It is a further breakout, narrowing the analysis to the
                      required tasks. It includes the following activities:

                         Identify tasks for each mission.
                         Describe critical functions.
                         Generate task lists based on defense system crew
                         composition.
                         Document crewmember qualification levels.
                         Create preliminary task hierarchical list.
                         Document expected performance required for each task.
                         Trace tasks back to mission descriptions (using database).


Air Force tasks       The SPO will continue dialogue with the contractor, reviewing
                      products and giving guidance as necessary. The SPO will
                      review and approve the reports as appropriate. They will review
                      the reports for accuracy, ensure that task lists are complete, and
                      verify that correct "assumptions" were made. The SMEs will
                      evaluate draft deliverables from the perspective of the using
                      command requirements.

                                                                     Continued on next page
                                      AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Air Force tasks   Training analysts review for compliance with DIDs and required
(Continued)       processes. They will also review to ensure that traceability has
                  been maintained throughout the Training System Requirements
                  Analysis process (as discussed in Chapter 2).


Data              Following completion of mission and task analysis, a report is
                  required. The report should contain items such as:

                  Mission Items:

                     Objective
                     Scenario
                     Segments
                     Profile
                     Map descriptions

                  Other items:

                     Graph descriptions
                     System and operator requirements
                     Detailed task analysis
                     Task analysis record
                     Master task listing
AFH 36-2235          VOLUME 3      1 NOVEMBER 2002                                         77


                                     Section C
                          Training Requirements Analysis

Introduction            When a new or modified defense system is being designed and
                        developed, training impacts must be considered throughout the
                        process. As you know, no system is fully functional without the
                        trained personnel to operate, maintain, and support it. Ideally,
                        the mission and task analyses have been performed or are in a
                        stage of completion. Now the training requirements analysis
                        begins.


Purpose                 The purpose of the training requirements analysis is to develop
                        the training task list. These are the tasks for which the student
                        lacks the skills, knowledge, or attitudes in order to perform them.


How is a task list      To develop a task list:
developed?
                           First conduct a target population analysis
                               Data determines types of students entering the training
                               system and their current skills/knowledge/attitudes

                           Perform a breakdown of skills, knowledge and attitudes to
                           determine which skills, knowledge and attitudes is performed.
                              Determining which skills can be trained in what setting.

                        This breakdown will be a key factor in conducting objectives
                        analysis (Section D).


Who develops it?        The contractor normally develops the analysis with input from
                        SMEs.


Contractor tasks        Conducting early training requirements analysis, including any
                        preliminary analysis during concept development, will help
                        ensure that the following tasks are done.

                        Assess and analyze:
                           Assess potential sources of students.
                           Assess students‚Äô knowledge and education.

                                                                       Continued on next page
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Contractor tasks      Analyze pertinent data on unique student characteristics to
(Continued)           compare with job expectations.
                      Assess life cycle training and support impact.

                   Create and Document:

                      Finalize procedure for defining training requirement.
                      Create final training task hierarchy list.
                      Document findings by writing Training Requirements Analysis
                      Report (TRAR).
                      Document in a database, tracing requirements back to
                      mission descriptions.

                   Determine:

                      Identify target population (potential students).
                      Determine student experience level.
                      Identify students‚Äô current proficiency and qualifications.
                      Define increase needed in skills, knowledge, and attitudes
                      (SKA).
                      Define performance factors and qualification levels for
                      terminal objectives.


Air Force tasks    The SPO will review the report, comment and approve, as
                   appropriate. The user will:

                      Conduct comparative analysis for comparison to contractor-
                      performed analysis results.
                      Review contractor-training decisions for traceability,
                      completeness, accuracy and reasonableness.
                      Check that target population definition is correct.
                      Verify that performance factors reflect valid performance
                      requirements.


Data               Following the above actions, the contractor will deliver the TRAR
                   and other documentation in accordance with the contract.
AFH 36-2235   VOLUME 3     1 NOVEMBER 2002                                        79




Metrics          The Air Force will be performing continual risk assessment and
                 comparing:

                    Technical staffing vs. plan
                    Differences between planned vs. actual completion
                    Quality factors for products or processes
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                 Section D
                             Objectives Analysis

Introduction       Training objectives are the framework in which training systems
                   are designed and developed. The objectives are directed at
                   meeting all training requirements. The achievement of the
                   training objectives makes up the difference in knowledge, skills,
                   and attitudes needed by the target population to do the job.


Purpose            The purpose of conducting objectives analysis is to clearly state
                   training requirements in terms of conditions, standards, and
                   behaviors, and arrange them in a logical and effective sequence.
                   This becomes very important when structuring the course
                   syllabus and developing the courseware.


How is it          There is no "concrete" way to organize training objectives.
developed?         Objectives are organized in the most effective and efficient way
                   to conduct training. There are several recommended tasks in an
                   objectives analysis.


Who develops it?   The training system requirements analysis contractor performs
                   the objectives analysis with input from SMEs.


Contractor tasks   The contractor will complete many tasks at this stage, including:

                      Develop objectives, including:
                        Top Level
                            Terminal
                            Primary
                        Lower Level
                            Enabling
                            Secondary
                            Supporting
                            Subordinate
                            Developmental

                                                                 Continued on next page
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                          81




Contractor tasks         Translate training requirements into objectives for each
(Continued)              qualification level reflecting:
                            Behavior
                            Conditions
                            Standards

                         Document objectives hierarchy.
                         Using database, trace objectives back to mission
                         descriptions.
                         Sort/organize objectives into hierarchy for each crewmember
                         position.
                         Create flow chart for subordinate/superordinate relationships.
                         Describe qualification levels.
                         Document analysis.


Air Force tasks       The SPO will:

                         Review rationale, justification, and traceability of the training
                         objectives hierarchy.
                         Continue working with the contractor, giving guidance as
                         required.
                         Review and approve any objectives and media reports.
                         Through SMEs, review and coordinate the training objectives
                         hierarchy, as well as the task analysis process.


Data                  The contractor will deliver documentation in accordance with the
                      contract. This will probably not occur until completion of media
                      analysis, which is described in the next section.
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                  Section E
                                Media Analysis

Introduction       Instructional media refers to the different means used to give
                   information to the student. Different types of media have various
                   levels of effectiveness, depending on the complexity of the
                   subject being taught. Media analyses must be conducted to
                   ensure that the most effective media are used to efficiently meet
                   the training requirements.


What it is         Definitions of media vary, but for purposes of this section, media
                   refers to a channel of communication utilized to aid learning.
                   Types of media vary from instructor-based to full mission
                   simulation.

                   Media analysis is defined as the process of examining media
                   requirements and assembling a data bank of information to use
                   for selecting appropriate media for use in instruction.


How is it          Media analysis is not a straightforward process. It is an iterative
developed?         process with various media tradeoffs made prior to determining
                   the media pool. The media pool is the agreed-upon set from
                   which media will be selected. The "preferred" media are
                   selected and scrutinized against possible tradeoffs such as cost,
                   logistics, maintenance support and facilities. Consistency of
                   media is also an important consideration. The goal is an optimal
                   media solution that is feasible to implement within the training
                   system design.


Who develops it?   The contractor conducts media analysis, with some input from
                   SMEs. The contractor normally uses training analysts and
                   systems engineers in the effort.
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                         83




Contractor tasks      Media analysis can be a very detailed process and will vary
                      depending on the program. The contractor will complete tasks
                      such as the following:

                         Conduct surveys of:
                           Instructional systems
                           Educational technologies
                           Authoring systems

                         Define student evaluation techniques.
                         Select media allocation model (if applicable).
                         Conduct media trade study.
                         Analyze media training effectiveness.
                         Allocate candidate media in matrix.
                         Using database, trace media allocations back to mission
                         descriptions.
                         Analyze instructional strategy alternatives.
                         Develop global instructional strategy.
                         Identify levels of courseware required for computer-based
                         training (CBT).
                         Document analysis.


Air Force tasks       The SPO has continued dialogue and guidance with the
                      contractor. The SPO will ensure that documentation meets
                      requirements. They will again ensure that objectives are
                      traceable to and support valid training requirements. They will
                      also check for:

                         Logical objective grouping.
                         Correct objective flow.
                         Verification that all training requirements are covered by
                         objectives.
                         Appropriate media selections for skills taught (with special
                         attention to use of operational aircraft and hardware).
                         Validity of functional and physical fidelity requirements.
                         Validity of media selections and training requirements.
                         Assurance that operational equipment is considered in media
                         analysis.
                         Assurance that AF technical training centers equipment and
                         media needs are addressed.
                         Assurance that if CBT is selected as media, it is procured as
                         part of the acquisition.
                                     AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Steps to media   The above list can be summarized in the following steps to select
selection        media.

                    Develop a list of possible media.
                    Review constraints relative to cost, time, availability,
                    resources, etc.
                    Evaluate media needs relative to content, objectives,
                    instructional strategies, teaching methods, and organizational
                    patterns.
                    Verify that media are compatible with other AF systems and
                    standards.
                    Eliminate media not matching your needs.
                    Verify/revise remaining media options.
                    Select the best media match.

                 Foolproof? Absolutely not! The choice is yours and the
                 parameters depend on the contract and course objectives.


Data             A training media analysis report will be delivered in accordance
                 with the contract.
AFH 36-2235    VOLUME 3      1 NOVEMBER 2002                                        85


                                 Section F
                                Cost Analysis

Introduction      To make intelligent decisions about the selection of a system to
                  fulfill a specific need, you must look beyond the immediate cost
                  of developing and producing that system. You must look at
                  various alternatives in training approaches. You learned earlier
                  about media analysis and various factors you must consider.
                  Closely interrelated with that is cost analysis. In cost analysis,
                  you analyze not only instructional media but also life cycle costs
                  such as operations, maintenance and support. A system that is
                  initially more expensive may be less expensive in the long term.
                  But the reverse tends to be true. Also, the cheapest up-front
                  system generates more support costs and sometimes even more
                  remedial and follow-on training costs.


Purpose           The reason you must conduct cost analysis is to make sure you
                  are acquiring the best long-term value by performing a
                  comparative evaluation of potential instruction methods and
                  media to determine the most efficient alternative.


What it is        You learned earlier that the types of media are defined as the
                  delivery vehicles for presenting instructional material to students
                  to induce learning.

                  Additionally, training effectiveness analysis is defined as the
                  process of measuring and integrating the training effectiveness
                  and the cost effectiveness of existing and/or proposed alternative
                  training system configurations and components in order to
                  determine the optimal mix of new training system components, or
                  to evaluate and improve existing training systems. In other
                  words, to determine the most effective training for the money
                  available.

                  Training strategy is defined as the logical arrangement of
                  course content within a pattern or organization, which will likely
                  cause the most learning to occur. It includes the purpose, target
                  audience, content outline, interaction, feedback, testing,
                  audiovisual options, and other data.
                                        AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




Background       Your goal should be to determine the most effective and efficient
                 training possible. There are alternatives for nearly all training
                 strategies. You may not want to use alternatives, but funding or
                 scheduling may force you to. The best-planned training
                 strategies sometimes are changed due to design, schedule or
                 other modifications. That is why you must analyze life cycle
                 costs compared to training strategy alternatives. It is always
                 better to be prepared up front.


How to prepare   Depending on the training system program being worked, cost
cost analysis    analysis could be performed in a manual mode or by utilizing
                 automated cost analysis models. Cost analysis can be a time-
                 consuming process that, if done properly, will pay long-term
                 dividends. Figure 13 is a simplification of what is involved in cost
                 analysis.

                 Figure 13 Cost Analysis


                     Start

                             Identify           Identify              Collect
                             Problem           Data Needs              Data




                        Revisit Process          Form                Analyze
                        As Necessary           Conclusion             Data
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                          87




What is included?     The first step in the process is to identify the problem. The word
                      "problem" may be a misnomer in that what you really have at this
                      point is a "challenge." Your challenge is: "Do we use classroom
                      instruction or CBT, videotape, or ICW? Or do we use a simulator
                      or actual operational hardware? What is the student load?"
                      Once you identify the problems, you must identify what data will
                      be needed, how to collect the data, and so on. These steps can
                      be modified and expanded as necessary to support the task at
                      hand.


Final report          When you have finished the process, you will have developed a
                      cost analysis report that identifies the best strategy to take to
                      utilize the most efficient and effective training available.


Contractor tasks      The contractor will analyze life cycle costs versus:

                         Training effectiveness
                         Media alternatives
                         Training strategy alternatives

                      They will also document cost analysis in the training system
                      basis analysis report.


Air Force tasks       Following completion of the cost analysis, the SPO continues
                      dialogue and guidance with the contractor. They will review cost
                      analysis documents to ensure that:

                         Comparisons are appropriate.
                         Trade-offs are properly considered and analyzed.
                         Analyses are unbiased.
                         Factors used are complete, appropriate and realistic.
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                Section G
                     Training System Basis Analysis

Introduction       In the planing phase of ISD, you developed a "big picture" outline
                   of instructional needs and potentially required plans. You
                   identified what plans would probably be required and identified
                   elements of the training system implementation plan. Among
                   other things, you decided if there would be a courseware/ISD
                   development plan. The training system basis analysis (TSBA) is
                   one of the activities in this phase.


Purpose            The purpose of the TSBA is to develop the training system
                   concept and define the training system configuration. The TSBA
                   report documents existing training programs and establishes the
                   functional baseline for the design, development, and operation of
                   an integrated training system. The report is used to define
                   training capabilities and establish system requirements for the
                   training system.


Contractor tasks   The contractor will review and update previous work in planning
                   and analysis to complete the following requirements.

                   New Items:

                      Write development plans for the instructional system and for
                      courseware.
                      Define training system conduct.
                      Assess training technologies (incorporate media analysis
                      from objectives and media analysis).
                      Perform problem analysis.
                      Develop success criteria for:
                         Course Readiness Review (CRR).
                         Site Readiness Review (SRR).
                         Training System Readiness Review (TSRR).
                         Using database, trace system requirements back to
                         mission descriptions.
                         Validate requirements analysis database traceability.

                                                                Continued on next page
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                          89




Contractor tasks      Continuing items:
(Continued)
                         Update training system implementation plan.
                         Document existing training system.
                         Document similar training system.
                         Document inputs for System Requirements Document (SRD).
                         Document analysis.


Air Force tasks       The SPO will continue to work with the contractor throughout this
                      process, and must review contractor intermediate products,
                      elevating any areas of concern. Once the report is delivered, the
                      SPO will review and approve as appropriate. The SPO will also:

                         Identify any new requirements and changes reflected in top-
                         level program documents such as:
                            Using command(s) system training plan(s)
                            Operational readiness document
                            Previous TSRA products
                            Program management directive
                            New constraints such as:
                                Cost changes
                                Schedule changes
                                Site changes
                                System changes

                         Verify that all valid training requirements are covered.
                         Verify that all system requirements are traceable to valid
                         training requirements.
                         Check to make sure that if success criteria are met, they will
                         ensure success.

                      The using command(s) will support the SPO as necessary and
                      will support a MOA for SME use.

                      Note: If this contract is not a single, total system development,
                      the SPO must:
                         Write a new RFP.
                         Write SRD providing a functional description of all
                         requirements for the total training system (single-contract
                         programs only). This includes student input/output.
                         Estimate SME requirements.
                         Negotiate MOA with using command(s).
                           AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Data   The contractor delivers documentation according to the contract.
       This documentation, such as a report, may tie together and
       integrate all previous training system requirements analysis
       major activities. This report can cover:

          Planning and scheduling considerations
          Analytical process
          Design goals and requirements selection
          TSBA report results such as:
             Information sources and data collection
             Existing training system analysis
             Similar system analysis
             Training technology assessment
             Problem analysis
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                         91


                                     Section H
                                Preliminary Syllabus

Introduction          Now a preliminary syllabus will be developed. A preliminary
                      syllabus puts into an outline the "big picture" of the training
                      system in a "real world" context.


Purpose               The purpose of the preliminary syllabus is to develop and
                      document a detailed outline of the overall structure of the
                      instruction. The preliminary syllabus will provide a master plan
                      that describes how the training system configuration and overall
                      training concept will be used.


Contractor tasks      Using the previously developed guidance, such as the various
                      plans, analyses, objectives and media analyses, an unrestrained
                      syllabus is developed. This unrestrained document looks at the
                      big picture, covering the entire training system. The goal is to let
                      nothing be overlooked. To develop an unrestrained syllabus, the
                      contractor must:

                         Cluster/sequence objectives.
                         Define course structure.
                         Define course times.
                         Develop course maps.

                      Once this is done, constraints must be applied and the target
                      must be narrowed to best fit the objectives. The contractor must:

                         Update media allocation matrix.
                         Show course times.
                         Indicate facility needs.

                      Once constraints are applied, document the preliminary syllabus
                      while tracing syllabus elements back to mission descriptions. It‚Äôs
                      important that a database be used for this to ease reference and
                      tracking in the future.

                      The final step in this area is to write a comprehensive syllabus
                      report and identify lessons for prototyping.
                                      AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Air Force tasks   The primary job of the using command at this point is to update
                  the STP. The SMEs will review the preliminary syllabus to
                  ensure that their objectives are part of the syllabus.

                  As the contractor develops preliminary syllabus documents, the
                  program office reviews the products, giving guidance as
                  necessary. The final step for the SPO is to review and approve
                  the preliminary syllabus.

                  AETC will review the preliminary syllabus for:

                     Logical flow
                     Coverage of all valid training requirements


Data              A syllabus development report is delivered as required by the
                  contract.
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                           93


                                    Chapter 6
                                     DESIGN
                                     Overview


Introduction       You are now proceeding into the design phase of ISD. This is a
                   good time to remember that you should never consider that the
                   planning and analysis phases are over

                                  The various phases of ISD are
                                        never complete.

                   While you may go progressively from one phase to the next, you
                   will periodically revisit a phase to update a plan, add something
                   you overlooked, or make mid-course corrections. As mentioned
                   in Chapter 1:
                                 ISD is flexible and is not a step-
                                      by-step linear process.

                   But, now that you have developed plans and conducted various
                   analyses, it is time to start designing instruction.


Purpose            The purpose of this chapter is to describe the ISD design phase
                   and the various applications that are specific to acquisition of a
                   defense system.


Where to read      This chapter contains five sections.
about it
                    Section         Title                                    Page
                        A       Start of Development                           94
                        B       Guidance Conferences                           95
                        C       System-Level Development Plans                 96
                        D       Courseware Planning Leading to                100
                                System Readiness Review
                        E       Development Activities                        103
                                AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




ISD phase   Now you are at the design phase. This phase is highlighted in
            Figure 14.

            Figure 14 Design Phase
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                             95


                                     Section A
                               Start of Development

Introduction          This section is included to cover the situations where a Training
                      System Requirements Analysis (TSRA) is not done, or was done
                      by a separate contractor. If the prior TSRA was conducted as
                      part of a single contract, you can skip this section if that contract
                      allows the contractor to proceed into development.

                      Note: Although you are in the design phase, some development
                      actions take place.


Contractor tasks      As mentioned, if prior tasks were performed as part of a single
                      contract for total system development, nothing happens at this
                      stage. But if prior tasks were performed as part of a stand-alone
                      TSRA or of a multi-phased development, the contractor uses the
                      TSRA results and prepares a proposal.

                      The contractor will write a new system specification as part of the
                      proposal.


Air Force tasks       If prior tasks were performed as part of a single contract for total
                      system development, nothing happens here.

                      If prior tasks were performed as part of a stand-alone TSRA or of
                      a multi-phased development, this is the point of a new solicitation
                      (see note on page 88). The program office will then generate:
                          New RFP
                          New SRD (if required)
                          New source selection
                          New contract award

                      If a new RFP is required, the using command will support the
                      source selection actions.

                      If AETC performs ISD (such as in maintenance training), training
                      equipment functional specifications will be provided to the SPO.
                      The SPO will then develop an RFP from the functional
                      specifications. When the RFP is issued, contractor involvement
                      begins. This RFP kicks off actions previously described
                      beginning on page 54, Request for Proposal Development.
                                       AFH 36-2235 VOLUME 3     1 NOVEMBER 2002


                                Section B
                          Guidance Conferences

Introduction      At this point, a contract has been awarded for development of the
                  training system. The Air Force will now schedule guidance
                  conferences (normally two) to assist the contractor.


Purpose           The purpose of the guidance conferences is to ensure that all
                  parties understand and agree on requirements, roles and
                  responsibilities. The goal is to eliminate or minimize
                  misinterpretations and misunderstandings.


Who conducts?     The Air Force will conduct the conferences at the contractor‚Äôs
                  facility shortly after contract award.


Air Force tasks   The program office will:

                     Conduct Program Requirements Guidance Conference
                     (PRGC), including the following tasks:

                        Review contract.
                        Review management issues.
                        Review data issues.
                        Discuss use of System Engineering Master Schedule
                        (SEMS).

                     Conduct System Engineering Guidance Conference (SEGC).
                     Discuss contractor‚Äôs approach to:

                        Writing process descriptions
                        Organization
                        Structured model philosophy
                        Software, hardware, courseware management
                        Test philosophy
                        Working groups establishment
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                          97


                                 Section C
                       System-Level Development Plans

Introduction          Previous chapters covered the development of preliminary plans
                      on various subjects such as a Systems Engineering Management
                      Plan (SEMP), while also defining planning requirements in areas
                      such as quality and life cycle operations. Based on these
                      preliminary reviews and using the various plans and analysis
                      stages, detailed system level development plans will now be
                      written.


Purpose               The purpose of these plans is to have "road maps" or guides,
                      listing various goals and procedures to follow. While ISD in itself
                      is not linear, some of the components of ISD must be followed in
                      a linear process for successful goal accomplishment. Plans that
                      are contractual will "direct." Those not contractual will "guide."


Contractor tasks      The main goal of this stage is to get the contractor to write, or in
                      some cases, update, system level development plans. One of
                      the largest plans to be written is the SEMP. The SEMP includes,
                      but is not limited to:

                      Engineering:

                         System engineering organization (how hardware, software
                         and course organizations interact)
                         System engineering detail schedule (how used)
                         How the work breakdown structure is used by engineering

                      Security issues:

                         Planned trade-off studies and analyses
                         Risk management plan
                         System security plan
                         Integration of reliability, safety, environment, etc.

                                                                      Continued on next page
                                       AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Contractor tasks   Organized input:
(Continued)
                      Incorporation of Integrated Logistic Support (ILS)
                      Technical Performance Measures (TPM)
                      SEMS (how used)

                   Other Issues:

                      Transition to manufacturing
                      Training program metrics (processes and products)
                      Use of working groups
                      Baseline control procedures
                      Conduct of design and technical management reviews
                      Use of prototypes
                      Other information as required

                   Other plans that may be required, depending on the contract and
                   system, include:

                      Software development plan
                      Hardware development plan
                      Configuration management plan
                      Courseware development plan
                      Source data management practices:
                         Weapon system data support plan
                         Required data flow across training system
                         Strategy for data holes and deficiencies
                         Associate contractor agreements
                      SME roles and responsibilities (system-wide)
                      Training System Implementation Plan (TSIP):
                         Transition plan (from previous training system to new
                         system)
                         Life cycle operations and maintenance plan
                         Personnel plans for operations and maintenance
                         Equipment plan:
                             Storage
                             Utilization
                             Maintenance
                         Student grading and evaluation plan
                         Training materials management plan
                         Facilities management plan
                         Instructor and student utilization, training and scheduling
                         plan
AFH 36-2235       VOLUME 3     1 NOVEMBER 2002                                           99




Air Force tasks      The SPO will also review for disconnects between contractor
                     plans/schedules and training need dates for:

                        Developmental Test and Evaluation (DT&E)
                        Operational Test and Evaluation (OT&E)
                        Initial cadre
                        Follow-on training
                        Initial Operational Capability (IOC)
                        Required Assets Available (RAA)

                     The program office will also review for disconnects between:

                        Facilities plans
                        Funding plans/levels
                        Personnel plans/levels
                        Training dates (do they match required delivery date [RDD]?)

                     The using command needs to review the revised SME support
                     MOA and support or negotiate it as required.

                     As the contractor is writing and/or delivering applicable plans,
                     the program office should be conducting a "big picture" review.
                     At this point, the SPO must ensure that the total training system
                     functional requirements and integration are defined for all
                     Configuration Items (CI). Now that plans and workload are
                     becoming more visible, this would be a good time for the SPO to
                     update the MOA defining SME support.

                     Unless delegated in the MOA, the program office will review
                     contractor plans and compare with:

                        STP
                        User training requirements
                        Defense system acquisition and test plans
                                   AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Quality plan   While all plans serve a purpose and are important, the quality
               plan is always critical. It is most important that plans are written
               to be used and followed. "Filling squares" in the plans
               business will eventually catch up with you, especially in the
               quality area. The training system quality plan should include
               items such as:

                  Training management plan

                      Training system support center
                      Training management system

                  System-level evaluation

                      Formative
                      Summative
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                       101


                            Section D
      Courseware Planning Leading to System Readiness Review

Introduction         Although this is the design phase of ISD, you continue to conduct
                     planning updates, evaluations and quality improvement. There
                     are relatively few cases where you finish one task or phase
                     completely, go to the next, and never have to revisit previous
                     work.
                                        ISD is a continuous process.


Purpose              The purpose of planning is to ensure that the required plans have
                     been considered and are now going to be written. These plans
                     will define and organize various tasks and procedures necessary
                     to design, develop, implement, evaluate and support instruction.


Contractor tasks     Some of the major plans that will be written are the courseware
                     development plans. Because of contractual or system
                     requirements, these plans may be written at an earlier stage. If
                     so, now is the time for updates. Courseware development plans
                     should:

                        Define development process.
                        Define organization.
                        Define handoffs.
                        Define controls.
                        Address procedure and criteria for:
                           Individual tryouts
                           Small-group tryouts (SGTO):
                              Design reviews
                              PDR/CDR process

                     These plans should also include:

                        Production resources plan and tracking procedures
                        Courseware configuration management plan
                        Courseware production plan (draft)
                        Courseware development schedules (draft)
                           Tie to integrated master schedule

                                                                   Continued on next page
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002



Contractor tasks      Identification of unique courseware aspects of any system
(Continued)           level plan

                   In addition, a test and evaluation plan may be written, which will
                   include:

                      Formative, summative and operational evaluation plans
                      Data collection procedures
                      Using command roles and responsibilities:
                         Use of SMEs and ITO/SGTO students

                      Review criteria:
                        Course Readiness Review (CRR)
                        Site Training Readiness Review (STRR)
                        Training System Readiness Review (TSRR)

                      Corrective action procedures
                      Prototype lesson planning
                         Completion of prototype lesson selection

                      Definition of support requirements for development and
                      evaluation
                      Definition of course review board (CRB)
                             Function
                             Charter
                             Membership


Air Force tasks    As the contractor develops the plans and delivers according to
                   the contract, the SPO must:

                      Review plans, schedules and criteria.
                      Work with contractor to effect required changes.
                      Alleviate areas of concern.

                   As the SPO reviews various contractor products, the using
                   command representatives will conduct similar reviews. Following
                   their review, written comments will be provided to the SPO.

                                                                  Continued on next page
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                      103




Air Force tasks      AETC will continue to review contractor plans and products.
(Continued)          AETC will also:

                        Check for continuity with system level plans and STP.
                        Verify that development plans and schedules allow meeti1ng
                        training need dates.
                        Validate development process.
                        Verify that quality plans will help ensure quality.
                        Prepare for participation in System Readiness Review (SRR):
                            Concerns
                            Issues to address


Acquisition          Following the planning update, an SRR is conducted. This
milestone            review, attended by the SPO and using commands and
                     contractor, is conducted to ensure that all players understand the
                     requirements and are ready to proceed.


Higher system        By the time of the SRR, ensure that the following contractor tasks
level activities     are done:

                        Work Breakdown Structure (WBS) is complete.
                        Cost schedule control system is in place.
                        Requirements analysis is complete.
                        Functional baseline is updated.
                        System level test requirements are defined, including:
                           System level formative and summative evaluation

                     The contractor should also:

                        Update TSRA documents.
                        Write development plans for training devices.
                        Finalize configuration management plan at SRR.
                                  AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                             Section E
                       Development Activities

Introduction   Development activities are now beginning as you progress
               through the ISD design phase. These development activities are
               segregated into three main groupings to facilitate various actions
               that must occur. The three groupings are listed below.

                  Topic                                                 Page
               Development Activities Leading to System Design           104
               Review (SDR)
               Development Activities Leading to Courseware              107
               Preliminary Design Review (PDR)
               Development Activities Leading to Courseware              110
               Critical Design Review (CDR)


Purpose        The purpose of development at this stage is to ensure that all the
               various required plans, syllabuses, lesson outlines and other
               documents are written, in place and functioning prior to SDR,
               PDR, and CDR.
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                          105


         Development Activities Leading to System Design Review (SDR)


Contractor tasks      In the first stage of development activities leading to SDR, the
                      contractor defines the development processes. If a preliminary
                      syllabus was developed earlier, the update or comprehensive
                      syllabus is written now. Along with the comprehensive syllabus,
                      the contractor should:

                         Define interactive courseware (ICW) production standards:
                            Authoring system capability
                            Delivery platform capability
                            Lesson portability (compliance with DODI 1322.20,
                            Development and Management of ICW for Military
                            Training)
                            CBT levels required
                            Style guide
                            Audiovisual support material standards

                         Define design formats for:
                            Lesson outlines
                            Lesson flow diagrams
                            Lesson plans (for standup instruction)
                            Lesson strategies (lesson specifications for CBT)
                            Storyboards for CBT
                            Mission scenarios

                         Draft checklists for success criteria for:
                            Course Readiness Review (CRR)
                            Site Training Readiness Review (STRR)
                            Training System Readiness Review (TSRR)

                         Write PIDS for all courseware at the course level, including:
                            Purpose of course
                            What is to be taught
                            Required terminal learning objectives
                            Media to be used
                            CBT level to be used
                            Student entry/exit levels
                            Test requirements

                                                                   Continued on next page
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Contractor tasks      Design prototype lessons, including:
(Continued)
                         Lesson outlines
                         Flow diagrams
                         Lesson strategy
                         Storyboards
                         Mission scenarios


Air Force tasks    The SPO must review all contractor products and design
                   decisions, working with the contractor to effect any required
                   changes. Areas of concern should be elevated. The SPO
                   should:

                      Review and comment on ICW production standards.
                      Review and comment on design formats.
                      Review and comment on Prime Item Development
                      Specifications (PIDS).

                   As in the planning update, and for all development activities, the
                   using command will have SMEs review contractor products,
                   providing comments to the SPO.

                   AETC will review the contractor‚Äôs documents for proper
                   incorporation of SRR recommendations and compatibility with
                   user requirements and timeliness. They will then prepare for
                   participation in the SDR.

Acquisition
                   At this point, the system design review is conducted.
milestone


Metrics            During the SDR, the AF will review the contractor‚Äôs responses to
                   action items from the SRR and ask questions such as:

                      What is the seriousness of the priority action items?
                      What is contractor timeliness of response to action items?
                      Are the contractor‚Äôs updated personnel/skill allocations
                      consistent with the program as defined at SRR?
                      Are updated versions of critical planning documents (such as
                      ISD management plan, quality plan, etc.) consistent with SRR
                      decisions?

                                                                 Continued on next page
AFH 36-2235     VOLUME 3        1 NOVEMBER 2002                                      107




Metrics (Continued)      Are course contents planned for each course element and the
                         personnel planned to accomplish it consistently?

                      Note: Ensure consistency with production tracking device,
                      because the levels you establish here will be used as a baseline
                      for tracking future production resource consumption.

                      During the SDR, the AF will also:

                         Verify that the contractor is prepared for the PDR.
                         Verify that the PDR checklist shows that the contractor meets
                         all requirements for PDR. Pay special attention to the system
                         engineering master schedule. For example, how are they
                         doing compared to schedules?


Higher system level By the time of the SDR, the contractor should have accomplished
activities          the following:

                         Allocated baseline is established across the system.
                         Second-level specifications are complete, including:
                             Configuration Item Development Specifications (CIDS)
                             Prime Item Development Specifications (PIDS)

                         System design is complete.
                         System-level development plans are updated.
                                       AFH 36-2235 VOLUME 3    1 NOVEMBER 2002


Development Activities Leading to Courseware Preliminary Design Review (PDR)


Contractor tasks   The contractor has written a syllabus, met with the SPO during
                   the SDR, and will now finalize the syllabus, incorporating any
                   changes required. In addition, the contractor will:

                      Update production standards.

                      Update design formats.

                      Update courseware development plans:
                       Integrate courseware:
                         Within system (with devices)
                         Within courses

                         Courseware test and evaluation:
                           Formative
                           Summative
                           Operational

                         Implementation

                      Continue prototype lesson development.
                        Code, program or write.
                        Show and tell what is available at PDR.

                      Update checklists for success criteria for CRR, STRR, and
                      TSRR.

                      Work with government to assure closeout of prior action items
                      from SRR and SDR.
AFH 36-2235       VOLUME 3      1 NOVEMBER 2002                                         109



Air Force tasks      The SPO must review all contractor products and design
                     decisions, working with the contractor to effect any required
                     changes. Areas of concern should be elevated.

                     The using command will review contractor intermediate products
                     as draft deliverables, providing comments to the SPO.

                     AETC may review contractor drafts, as well as final products.
                     They will check to make sure that:

                        SRR and SDR action items have been incorporated.
                        Syllabus covers training requirements and has logical flow.
                        Plans reflect known changes in defense system schedules.
                        Formative and summative evaluation plans, quality plan, and
                        test plan are logical and coordinated and can reasonably be
                        expected to lead to quality training.


Metrics              During the PDR, the Air Force will verify that the contractor is
                     ready for the PDR.

                        Does the PDR checklist show that the contractor meets all the
                        requirements for the PDR, on schedule and in accordance
                        with the SEMS?
                                       AFH 36-2235 VOLUME 3     1 NOVEMBER 2002



Higher system level By the time of the PDR, the contractor should have completed:
activities
                       System-level evaluation plan for formative and summative
                       phases
                       Hardware and software status reviews at system level
                       System-level development plan updates

                    By the time of the system PDR, the contractor should have
                    completed:

                       Top-level functional design
                       Top-level software design
                       System-level test plan
                       Allocated baseline
                       Synchronized sub-element schedules

                    The contractor also has identified the status of all hardware,
                    software, weapon system data, and training system component
                    hardware and software.

                    Incremental PDRs are conducted across the system for other
                    system components.
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                       111


  Development Activities Leading to Courseware Critical Design Review (CDR)


Contractor tasks      The actual lessons of the course are now being written in draft
                      form. During this stage you will develop lesson outlines and flow
                      charts for all lessons. In addition, you will:

                         Complete prototype lessons.
                         Finalize courseware development plans.
                         Finalize test and evaluation plan for courseware.
                         Finalize production standards.
                         Finalize design formats.
                         Finalize checklists and success criteria.
                         Finalize production schedule.
                         Close out all prior action items.

                      Note: At this point, the contractor is nearly ready for the
                      courseware CDR. To ensure that everything is in order, the
                      contractor should remember that at the CDR the following should
                      be done:

                         Demonstrate prototype lessons.
                         Baseline prime item development specification (written at
                         course level).
                         Review hardware/software/courseware integration.
                         Nominate members for Curriculum Review Board (CRB).


Air Force tasks       The SPO has a critical task at this point since the CDR
                      represents the final hurdle before entering the ISD development
                      phase. The SPO must now review and approve all contractor
                      products, working with the contractor to effect any required
                      changes. The SPO will:

                         Review prototype lessons.
                         Review all development plans.
                         Review design formats and production standards.
                         Place PIDS under configuration control of contractor.
                         Review HW/SW/CW integration of Training Management
                         System (TMS) and Training System Support Center (TSSC).
                         Review courseware integration with training system design;
                         ensure total system integrity.
                         Review status of contractor personnel and resources.

                                                                   Continued on next page
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002



Air Force tasks      The using command will review contractor products as draft
(Continued)          deliverables, providing comments to the SPO.

                     AETC will review contractor documents and pay special attention
                     to lesson outlines and flow charts. They will verify that:

                        There is a logical flow of lessons.
                        Lessons are traceable to ISD analysis.
                        Lessons meet training requirements.
                        Lessons adhere to sound principles of instruction.

                     AETC will also review the test plan to ensure incorporation of
                     comments and necessary changes from previous draft.


Acquisition          The critical design review for courseware is conducted during
milestone            this period.


Metrics              The Air Force should verify that the contractor is prepared for the
                     CDR. Review and answer questions such as:

                        Does CDR checklist show that contractor meets all
                        requirements for CDR in accordance with SEMS?
                        Do syllabus and PIDS reflect traceability and comprehensive
                        course design?


Higher system level At this stage, the contractor:
activities
                        Completes designs for system-level HW/SW/CW integration.
                        System designs integrate:
                           TMS
                           TSSC
                        Writes training system implementation plan.
                        Ensures that all PIDS are authenticated.
                        Participates in incremental CDRs, which are conducted
                        across system for other system components.
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                      113


                                 Chapter 7
                               DEVELOPMENT
                                    Overview


Introduction       You are now at a point where you have completed system and
                   process SRR, SDR, PDR and CDR. You have received
                   guidance and approvals from the SPO and using commands and
                   have developed a good working relationship with the SMEs.
                   Here lessons are produced and products are tested. This is
                   where diligent analysis, planning and design efforts will pay
                   dividends. As development begins, you may need to revisit
                   earlier phases of ISD.


Purpose            The purpose of this chapter is to describe the ISD development
                   phase and the various specific actions required in the application
                   of ISD to defense system acquisition.


Where to read      This chapter contains eight sections.
about it
                    Section         Title                                   Page
                        A       Lesson Outlines/Flow Diagrams                112
                        B       Lesson Strategy/Lesson Plans                 114
                       C        Storyboards                                  116
                       D        Coding, Programming, Writing                 117
                        E       Lesson Tests (Individual Tryouts)            118
                        F       Course-Level Integration Tests               120
                       G        Small-Group Tryouts                          121
                       H        Iterative Remedy and Retest                  123
                              AFH 36-2235 VOLUME 3   1 NOVEMBER 2002



ISD phase   You are now at the development phase. This phase is
            highlighted in Figure 15.

            Figure 15 Development Phase
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                       115


                                 Section A
                        Lesson Outlines/Flow Diagrams

Introduction          Now that you have entered the development phase of ISD, you
                      begin to focus on the development of lessons. In this phase, the
                      contractor will review plans and designs, and will have frequent
                      formal and informal meetings with SMEs, the SPO and using
                      command representatives.


Purpose               The objectives of lesson outlines/flow diagrams are to define
                      scope, content, duration, and schedule of contractor-developed
                      courses.


Contractor tasks      The contractor will:

                         Develop lesson outlines and flow diagrams for each lesson
                         based on course- level PIDS requirements and the course
                         and lesson objectives shown in the syllabus.


Air Force tasks       The SPO will:

                         Review (spot-check) lesson outlines and flow diagrams
                         against PIDS requirements and the course and lesson
                         objectives shown in the syllabus.
                         Arbitrate SME comments.
                         Elevate areas of concern as required to resolve.

                      In addition, SMEs will review all lesson outlines and flow
                      diagrams, elevating areas of concerns as necessary.


Metrics               The Air Force should:

                         Ensure the integrity of the production resource-tracking
                         device used as a tool to evaluate contractor performance
                         throughout the production process.
                         Verify schedule integrity.

                                                                    Continued on next page
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002



Metrics                 Look at number of discrepancies (DR) or revision
(Continued)             recommendation forms for each lesson and the severity of the
                        write-up. Look for trends across production base. Look at
                        volume of SME comments as check of contractor
                        understanding.
                        Review all products for conformance with style and format
                        guide. (Cutting corners? Unauthorized changes?)
                        Ensure that requirements traceability is preserved. Do lesson
                        outlines reflect a comprehensive course design?


Higher system level At this stage, the system-level CDR is conducted.
activities
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                       117


                                  Section B
                         Lesson Strategy/Lesson Plans

Introduction          Outlines and flow diagrams have been developed. Now the
                      contractor will actually build the lesson strategy and write lesson
                      plans for standup instruction.


Purpose               The purpose of lesson strategy and lesson plan development is
                      to provide the subject matter content, instructional strategies and
                      other supportive information for each lesson. A lesson strategy
                      may be a data item but would normally be incrementally reviewed
                      on-line prior to the critical design review.


Contractor tasks      The contractor will:

                         Develop lesson strategy for ICW lessons (also called "lesson
                         spec").
                         Develop lesson plans (standup instruction).
                         Define CBT level and authoring system features to be
                         exploited for each lesson.
                         Review integration of lessons to courses.

                      Note: CBT development lead-time is critical. Plan for it.


Air Force tasks       The Air Force will review (spot-check) lesson strategies and
                      lesson plans against PIDS requirements and the course and
                      lesson objectives shown in the syllabus.

                      The SMEs will review all instructional strategies and lesson
                      plans, elevating areas of concern to the SPO.
                             AFH 36-2235 VOLUME 3    1 NOVEMBER 2002



Metrics   The Air Force should track:

                Lesson production status against the production plan
                baseline:
                By lesson
                By course

                Courseware production personnel staffing numbers and
                experience against production plan baseline
                Number of changes to functional requirements vs. total
                number of functional requirements
                Courseware development action items
                SME discrepancies
AFH 36-2235       VOLUME 3      1 NOVEMBER 2002                                        119


                                     Section C
                                    Storyboards

Introduction         As lesson plans are being written, storyboarding is also occurring
                     where required. Storyboards become the visualization of the
                     training sequence. They are the sketches and narration notes of
                     this visual process. The ultimate goal is to facilitate learning with
                     student participation and individualized instruction.


Purpose              Storyboards provide a blueprint for the production of interactive
                     courseware. This includes scripting information and visual
                     representatives of the materials to be presented. Storyboards
                     also provide information and directions for the programmer and
                     the instructional designer necessary for coding.


Contractor design    The contractor will develop storyboards and integrate courses to
tasks                each other and to the total training system.


Air Force tasks      The Air Force will spot-check storyboards and lesson integration
                     against PIDS requirements and the course and lesson objectives
                     shown in the syllabus. SMEs will review all storyboards,
                     elevating areas of concern.
                                         AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                  Section D
                        Coding, Programming, Writing

Introduction        The ISD development phase is the stage where instructional
                    materials are actually created. Some work in this area may have
                    been started earlier but efforts will now be intensified. At this
                    point, you may be coding, programming, or writing, depending on
                    the scope of the training being developed.


Purpose             The purpose of this stage is to create all audiovisual and other
                    necessary instructional materials. Additionally, mission
                    scenarios and student/instructor lesson guides are produced.


Contractor tasks    Depending on the contract, the contractor will:

                        Write lesson guides for instructors and students.
                        Create all other instructional materials.
                        Create mission scenarios.
                        Code and debug.
                        Prepare for lesson-level tests (individual tryouts).


Air Force tasks     The Air Force will review instructional materials and mission
                    scenarios. SMEs will review all lesson guides and instructional
                    materials.


Higher system level The Air Force will correlate traditional courseware development
activities          with courseware to be embedded within other training devices;
                    for example, mission scenarios incorporated within weapon
                    system trainers.

                    Note: Incremental development of PDRs, CDRs, SGTOs, and
                    CRRs will occur throughout this stage at the direction of the SPO.

                    Figure 16 Example Diagram of Incremental Lesson Production
                    and Evaluation.

                                                                    Continued on next page
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                       121


                                 Section E
                       Lesson Tests (Individual Tryouts)

Introduction          As lessons are completed, tests must be conducted to validate
                      the lessons. These tests will be conducted on an individual
                      basis as well as in small groups. This section will cover
                      individual tryouts (ITO).


Purpose               The purpose of testing is to validate the lessons and courses and
                      to allow adjustments and improvements to be made before full-
                      scale implementation. By using ITOs, the contractor can collect
                      and analyze performance data from representative students to
                      identify major errors in all of the courseware.


Contractor tasks      The contractor will:

                         Test all lessons (in accordance with courseware test plan).
                         Select/provide surrogate students.
                         Evaluate lessons from the SMEs‚Äô perspective, ensuring
                         proper lesson content.
                         Collect/analyze data.
                         Analyze and fix discrepancies.


Air Force tasks       Before the ITOs began, the SPO approved the questionnaire and
                      comment forms. During the test, the SPO has been an active
                      observer, especially through the SMEs. As the tests are
                      ongoing, the SPO will conduct spot checks of actual instruction
                      and data generated by the tests. Upon completion of the tests,
                      the SPO will review the data and select any issues that are
                      appropriate for resolution. Issues need to be addressed by the
                      entire team at a resolution meeting.

                      The using commands are very active in this stage. They will:

                         Provide students for the tests.
                         Monitor contractor conduct of tests (through SMEs).
                         Conduct check of all data generated by tests.
                         Review data to identify issues for resolution.

                                                                   Continued on next page
                                     AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Air Force tasks   AETC may participate in the ITOs as both observers and
(Continued)       students. They will compare the planned lessons and course
                  with the actual performance and will prepare an appropriate
                  report listing comments, issues, and concerns for resolution.


Metrics           The Air Force should ask questions and make comparisons such
                  as the following:

                     Does the contractor intend to test all of the lesson production
                     (ITOs)?
                        If less:
                             Why?
                             Is sample viable?
                        Are courses selected representative?

                     What is the number and severity of test discrepancies
                     identified?
                     Compare actual course length (from tryouts) against planned
                     course length (may imply a major rewrite). Remember that
                     training objectives should determine course length.
                     Track training discrepancy (TD) correction rate.
                         How fast does contractor correct errors?
                         Will corrections be incorporated in time for small-group
                         tryout?
                         Is contractor using resources effectively?
                         Are personnel required for correction consistent with
                         personnel plan?

                     Are incremental tests running in accordance with formative
                     evaluation plan?
AFH 36-2235        VOLUME 3     1 NOVEMBER 2002                                          123


                                  Section F
                         Course-Level Integration Tests

Introduction          Course-level integration tests are conducted between ITOs and
                      Small-Group Tryouts (SGTOs). Actual media and equipment are
                      used so that any discrepancies noted can be fixed before
                      continuing into the next stage.


Purpose               The purpose of these tests is to ensure that the courseware is
                      ready for the SGTO, and that it has been integrated so that all
                      components fit together logically and effectively.


Contractor tasks      The contractor will:
                         Provide government with surrogate student requirements.
                         Use questionnaires and comment forms.
                         Perform test against PIDS.
                         Perform test from a student perspective.
                         Focus on integration across lessons.
                         Use actual (final) delivery media if available; make-do work-
                         arounds may be used if necessary.
                         Collect/analyze data and discrepancies.
                         Document test results.
                         Analyze and fix discrepancies.


Air Force tasks       The Air Force will:
                         Review comment forms and questionnaires developed to
                         support integration tests.
                         Review test resources, schedules, and test support plans;
                         ensure integrity of test activity. Ensure availability of
                         surrogate students.
                         Review (spot-check) data generated by test.

                      The using command will provide students for course level
                      integration tests. The SMEs will:
                          Monitor contractor conduct of tests and comment as
                          necessary.
                          Check data generated by tests.
                          Review data to identify issues for resolution.
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                 Section G
                            Small-Group Tryouts

Introduction       Now that individual tryouts have been conducted and integration
                   tests performed, corrections should have been made to the
                   lessons and other applicable courseware. SGTOs will now be
                   conducted to test the courseware on a larger scale.


Purpose            SGTOs are conducted to collect data from a more realistic
                   sample of the actual student population. These students will be
                   from the target audience and will use actual equipment, identical
                   environment and other parameters that will be found in the full-
                   scale course. This data will be used to make additional
                   modifications as necessary before the large-group tryout.


Contractor tasks   The contractor will:

                          Use questionnaires and comment forms.
                          Integrate production HW/SW media.
                          Conduct test using actual, final lesson materials, and all
                          other supporting media (or work-arounds) required to
                          make the course "whole."
                          Perform SGTO.
                          Verify system function from the student‚Äôs viewpoint.

                          Collect/analyze data and discrepancies.
                          Document test results.
                          Analyze course run time and student performance.
                          Analyze and fix discrepancies.


Air Force tasks    The Air Force will:

                      Review and approve comment forms and questionnaires
                      developed to support SGTOs.
                      Review test resources, schedules, and test support plans.
                      Ensure integrity of test activity.
                      Ensure availability of surrogate students as agreed.
                      Review (spot-check) data generated by test.

                                                                  Continued on next page
AFH 36-2235       VOLUME 3     1 NOVEMBER 2002                                     125




Air Force tasks      The using command will provide students for SGTO. The SMEs
(Continued)          will monitor contractor conduct of tests and check 100% of data
                     generated by the tests.


Higher system level The Air Force will:
activities
                        Check system interfaces and integration.
                        Ensure that courseware delivery media are properly
                        integrated.
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                  Section H
                        Iterative Remedy and Retest

Introduction       Now that incremental tests have been conducted and test results
                   data have been analyzed, corrections must be made to the
                   lessons and courses as required. As with all previous actions,
                   this is a team process involving the contractor, SPO and using
                   commands. This is a critical stage as this is the final step before
                   the final course readiness review (CRR).


Purpose            The purpose is to identify and remedy all discrepancies and
                   retest before getting approval to proceed to implementation.


Contractor tasks   Using data from the ITOs and SGTOs and guidance from the
                   SPO, the contractor will:

                      Conduct a resolution meeting consisting of contractors and
                      Air Force SGTO observers to fix discrepancies.
                      Submit any discrepancies not received at resolution meeting
                      to the Curriculum Review Board (CRB) for approval.
                      Fix discrepancies.
                      Repeat SGTO as necessary.
                      Provide data to support Functional Configuration Audit (FCA).
                      Support conduct of FCA.
                      Finalize all draft data item submissions.
                      Provide final site implementation.


Air Force tasks    The SPO is now at the final approval stages of ISD development.
                   But before implementation can begin, the following must be
                   done.

                      SMEs verify that all courseware materials are ready for
                      training.
                          At conclusion of SGTO retest, program office and user
                          review contractor remedies for all discrepancies. The
                          SPO arbitrates SME comments.

                      Elevate issues of concern as required to ensure resolution.

                                                                 Continued on next page
AFH 36-2235       VOLUME 3      1 NOVEMBER 2002                                          127




Air Force tasks         Conduct FCA and CRR.
(Continued)               Verify that courses meet PIDS.
                          Verify that PIDS:
                             Are performed on-site.
                             Use real students.
                             Use full classes.
                             Have all relevant media work-arounds in place.

                             Verify that FCA is held open until all discrepancies have
                             been corrected and approved by the SPO.

                        The SPO, SMEs, and users review and approve the
                        functional configuration audit.

                     While the SMEs are taking various actions, as members of the
                     government team, as specified above, their primary role is to
                     verify that all courseware materials are ready for training.

                     AETC will verify that lessons and courses are ready for
                     implementation and that:

                        Technical data are available, current and incorporated into
                        the material.
                        Course content reflects actual defense system.
                        User needs are met.


Metrics              Before the CRR, the Air Force will verify the following:

                        Are requirements to complete CRR reflected in CRR
                        checklist?
                        Do CRR checklists meet SEMS criteria?
                        Are any test discrepancies still outstanding? (All should be
                        cleared prior to the CRR.)


Acquisition          The CRR is conducted following completion of the above tasks.
milestone
                                      AFH 36-2235 VOLUME 3    1 NOVEMBER 2002


                               Chapter 8
                           IMPLEMENTATION
                                  Overview


Introduction    This is the reason you began the ISD process in the first place to
                conduct instruction. You have developed and validated an
                instructional system, have developed and validated lessons and
                courses, and now it is time to become operational. This chapter
                covers the system functions and planning required to implement
                instruction.

                Note: Seasoned carpenters say "Measure twice, cut once."
                Learn from that and do a double check to make sure system
                functions are in place, working and ready (see System Functions,
                page 10). Ensure that all previous planning has been completed.
                It will be time well spent.


Purpose         The purpose of this chapter is to explain the major events that
                must occur to help ensure successful implementation.


Where to read   This chapter contains four sections.
about it

                 Section         Title                                    Page
                     A       Site Training Readiness Review               127
                     B       Implementation of System Functions           128
                    C        Full-Class Tryouts                           133
                    D        Mature System Performance Review             134
AFH 36-2235   VOLUME 3     1 NOVEMBER 2002                                129



ISD phase        Now you are at the implementation phase. This phase is
                 highlighted in Figure 17.

                 Figure 17 Implementation Phase
                                       AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                 Section A
                     Site Training Readiness Review

Introduction       After all CRRs for a training site have been completed, a site
                   training readiness review (STRR) [also called Site Readiness
                   Review (SRR) or on-site review] will be conducted.


Purpose            This review is conducted at each site to confirm that the site is
                   ready to conduct training.


Contractor tasks   The contractor will support the STRR by showing that the desired
                   objectives and standards have been achieved. The contractor
                   will:

                      Collect data supporting summative evaluation.
                      Document results in test report.
                      Provide data to support STRR.


Air Force tasks    The SPO will approve the STRR based on success criteria and
                   checklists, and participate in contractor‚Äôs status briefings. In
                   addition, the SPO will conduct the STRR if in a non-guaranteed
                   student program.

                   The using command will provide support as required.

                   AETC will participate in on-site reviews and ensure that the site
                   is ready for training, reviewing items such as:

                      Facilities (training and support)
                      Training equipment
                      Courseware
                      Instructors

                   AETC will ensure that all are in place and ready, or that suitable,
                   temporary work-arounds are developed.


Milestone          A site training readiness review is conducted.
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                         131


                               Section B
                   Implementation of System Functions

Introduction       Earlier, you learned that the system functions must be in place
                   before a training system can operate. You also learned that
                   there were four basic system functions:

                      Management
                      Support
                      Administration
                      Delivery

                   These functions are critical to the instructional system
                   implementation process. This process will operate effectively
                   and efficiently only if these functions are in place and in use.


Purpose            The purpose of the system functions is to support the
                   instructional infrastructure.


Where to read      This section covers four topics.
about it

                      Topic                                                   Page
                   Management Function                                         129
                   Support Function                                            130
                   Administration Function                                     131
                   Delivery Function                                           132
                                    AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                             Management Function


What it is      Management is considered to be the practice of directing or
                controlling all aspects of the instructional system. These
                activities are an integral part of conducting instruction. A system
                cannot be properly implemented without the system management
                function in place.


Who is          Each level within the instructional activity has various
responsible?    management responsibilities depending on the activity or
                program.

                Example: A program or project manager may have overall
                management responsibility, while an instructor may have
                management responsibility more focused on the teaching or
                learning activity.


Categories of   Management activities required to support implementation of an
management      instructional system can be categorized into five areas, as shown
activities      below.

                 Category               Management Activity Examples
                                   Develop Preliminary Training System T&E
                Planning           Plan
                                   Develop Defense System Data Support Plan
                                   Establish ISD Management Team
                Organizing
                                   Schedule People, Work and Resources
                                   Conduct Meetings with Staff and Contractor
                Coordinating
                                   Conduct On-Site Visits and Reviews
                                   Monitor Milestones, Budgets, Deliveries
                Evaluating
                                   Collect and Analyze Data
                                   Provide Status Briefings
                Reporting
                                   Develop Program Reports
AFH 36-2235    VOLUME 3     1 NOVEMBER 2002                                         133


                               Support Function


What it is        Support may be defined as the maintainer of the system. This
                  includes long-range planning, as well as day-to-day activities.

                  Like the other systems functions, support is a vital component of
                  the ISD system team.


Example           Support can include:

                     Providing student services
                     Maintaining equipment
                     Providing spare parts
                     Maintaining courseware


Who is            Management has overall responsibility for support, but various
responsible?      activities have different responsibilities. Examples are:

                     Acquisition ‚Äì acquires equipment
                     Logistics ‚Äì maintains and supports
                                   AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                         Administration Function


What it is     Administration is the function that is concerned with the day-to-
               day tasks of operating an instructional system. Administration is
               a form of management or supervision that absorbs tasks not
               clearly appropriate elsewhere. Administration contributes
               significantly to the overall effectiveness of the instructional
               system. In fact, every phase of ISD is affected by administration.


Example        Systems administration can include:

                  Maintaining documentation
                  Processing reports
                  Filing data


Who is         All activities get involved in administration. Key offices include:
responsible?
                  Registrar section
                  Instructor staff
                  Clerical staff
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                          135


                                    Delivery Function


What it is            Delivery is the means used to provide instruction to students.


Examples of           Examples of system delivery methods include:
delivery methods
                         Instructors
                         Computers
                         Workbooks
                         Simulators


Who is                All activities directly involved in the instructional system have
responsible?          responsibilities such as the following.

                         The training manager in system acquisition ensures that
                         adequate planning has been done before selecting the
                         delivery method.
                         Instructional designers select the most appropriate delivery
                         method and report to management.
                         Instructional staff use and evaluate the selected delivery
                         method for effectiveness.
                                      AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                   Section C
                              Full-Class Tryouts

Introduction       Full-Class Tryouts (also called Large-Group Tryouts) are
                   conducted to complete one final test before the training system is
                   considered fully implemented. This tryout is conducted on-site in
                   the real environment, using real students, full classes and all
                   required media.


Purpose            The purpose is to fully test the training system, identifying
                   discrepancies and remedying as necessary. It also validates the
                   training system‚Äôs capability to accommodate incremental and
                   systematic integration of all components of a training system
                   without degradation to system performance and training
                   effectiveness.


Contractor tasks   The contractor will conduct the tryout in a guaranteed student
                   program. The contractor will support the Air Force in a non-
                   guaranteed student program. The contractor will also collect
                   data supporting summative evaluation and document results in a
                   test report.


Air Force tasks    The Air Force will:

                      Approve tryouts based on success criteria and checklists.
                      Conduct tryouts in non-guaranteed student program.
                      Support tryouts if contractor conducts.
AFH 36-2235        VOLUME 3      1 NOVEMBER 2002                                         137


                                  Section D
                      Mature System Performance Review

Introduction          After the system has been operational and functional for a period
                      for time, it will be continually evaluated. At a certain point in its
                      operation, however, a mature system performance review will be
                      conducted. It will be conducted on-site in a real environment,
                      using full classes of real students.


Purpose               The purpose of this review is to see if the system is performing
                      as designed and expected and to identify any modifications
                      needed.


Contractor tasks      The contractor will conduct or support the review, depending on
                      the contract. The contractor will also collect data supporting
                      summative evaluation, documenting any changes required and
                      test results.


Air Force tasks       The using command will conduct or support operational
                      evaluation throughout the life cycle of the system.
                                      AFH 36-2235 VOLUME 3    1 NOVEMBER 2002


                               Chapter 9
                              EVALUATION
Overview
Introduction    Evaluation occurs throughout the ISD process. Once instruction
                has been conducted, the Air Force will be specifically concerned
                with determining how well the training is achieving its objectives.
                Evaluation is the feedback that helps ensure that training
                objectives are achieved and the quality of graduates‚Äô
                performance is acceptable. The process continuously evaluates
                the course to determine if it is operating as designed. For
                example, six months after students graduate, are they still able to
                meet job performance requirements? If not, why not? Is it
                because of shortfalls in the course? Should changes in the
                course be undertaken? These are the kinds of questions you
                must ask and reviews you must make to ensure that the training
                that was developed is effective and efficient. You have already
                begun evaluation as you started with formative evaluation at the
                beginning of planning with development of the evaluation plan.
                Now you must do more.


Purpose         The purpose of this chapter is to describe the evaluation process
                that is used to continually evaluate the effectiveness and
                efficiency of training.


Where to read   This chapter contains three sections.
about it

                 Section         Title                                    Page
                     A       Formative Evaluation                          137
                     B       Summative Evaluation                          138
                     C       Operational Evaluation                        140
AFH 36-2235   VOLUME 3     1 NOVEMBER 2002                                    139



ISD phase        As you can see in Figure 18, evaluation has the most emphasis.
                 It is conducted throughout the ISD process, with each ISD phase
                 being involved with evaluation.

                 Figure 18 Evaluation
                                       AFH 36-2235 VOLUME 3     1 NOVEMBER 2002


                                 Section A
                             Formative Evaluation

Introduction        Evaluation occurs throughout the ISD process, but formative
                    evaluation occurs during development, production, and test
                    activities. It is the period from the beginning of planning to
                    course readiness review or validation of materials. Formative
                    evaluation should be part of the T&E plan, which was developed
                    earlier.


Purpose             The purpose of formative evaluation is to evaluate lesson/course
                    development during the "formative" stages and allow for
                    corrections (remedies) to be made before training is fully
                    implemented.


What is included?   As part of the T&E plan, formative evaluation is conducted during
                    development, ITO, and SGTO. It is not a process in itself, but a
                    series of events that are all part of formative evaluation. The
                    data collected is used to make lesson/course corrections and is
                    provided to the SPO for their review and action as necessary.


Air Force tasks     The SPO reviews data and provides guidance as requested.


Additional          Formative evaluation includes acceptance testing of equipment
information         and software, performance verification of system components,
                    formative evaluation of courseware, and training system
                    development.

                    The courseware formative evaluation plan is outlined in the
                    system test plan. The results of formative evaluation are
                    reviewed in the CRR.
AFH 36-2235     VOLUME 3       1 NOVEMBER 2002                                        141


                                Section B
                            Summative Evaluation

Introduction        Summative evaluation begins at the Courseware Readiness
                    Review (CRR), overlaps the formative evaluation phase, and
                    terminates at the Training System Readiness Review (TSRR).

                    Summative evaluation assesses the training system in the
                    operational environment to validate the baseline established
                    earlier. It will check for full system integration.


Purpose             The primary purpose of summative evaluation is to determine
                    whether the training developed for the students is effective and
                    efficient. It is the process of collecting data from students,
                    instructors, and other key evaluation interfaces as they use
                    instructional media in the actual training environment. Its
                    purpose is also to identify instructional materials, training media
                    or instructional management system components that result in
                    poor learning, inefficiency, or poor student acceptance. This
                    data will then drive improvements.

                    Summative evaluation answers questions such as:

                       How well has the training been accomplished as reflected by
                       operational requirements?
                       Do graduates of a course meet established training system
                       and operational performance standards?
                       Are the training system performance standards correct?
                       How can the training be better accomplished?


What is included?   Summative evaluation is a form of quality improvement that
                    evaluates the "summed" effect of the total training program. It
                    should begin at CRR/validation and go through TSRR. In the
                    acquisition business, this is both internal and external.
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Internal evaluation   Internal evaluation determines the adequacy of each component
                      of the training program such as instruction, materials, equipment,
                      and facilities. It is conducted within the instructional system or
                      "schoolhouse." It determines how well each component
                      contributes to the production of quality graduates. Internal
                      evaluation for a contractor-operated training system is usually
                      conducted by the contractor and reviewed/approved by the SPO
                      and/or using command.


External evaluation   External evaluation determines how well graduates are meeting
                      job performance requirements ‚Äì the reason for training them in
                      the first place. In external evaluation, the contractor will assess
                      the degree to which skills obtained during the course are
                      generalized effectively to the operational unit environment.
                      External evaluation utilizes information from the field, such as
                      feedback from supervisors of graduates. The SPO and/or using
                      command will review/approve results of external evaluations.


How to conduct        Internal evaluations can be conducted by reviewing:
evaluations
                         Course documents
                         Resources
                         Instructional facilities
                         Instructor performance
                         Measurement programs

                      External evaluations can be conducted by:

                         Questionnaires
                            For graduates
                            For supervisors
                         Field notes
                         Job performance evaluations
AFH 36-2235     VOLUME 3     1 NOVEMBER 2002                                       143


                                 Section C
                           Operational Evaluation

Introduction       Operational evaluation is the continuation of evaluation
                   throughout the life of the fully operational training system.
                   Operational evaluation occurs on a system regardless of whether
                   it is contractor- or Air Force-operated. The operational
                   evaluation is similar to summative evaluation except it is
                   continuous and reflects long-term operational data.


Purpose            The purpose of operational evaluation is to provide real-time
                   data for use in reviews, updates and quality improvement of
                   training systems. It is continuous improvement.


Who conducts?      The training source and the contract determine who conducts the
                   operational evaluation. The best way to do this is to use the
                   following table.

                                    Then
                   If training      operational
                   system is        evaluation is    And supported    With approval
                   operated by:     conducted by:    by:              evaluation by:
                   Contractor       Contractor       Using            SPO
                   (Total                            Commands
                   Contractor
                   Training)
                   USAF             USAF             Using            SPO
                   (Contractor-                      Commands
                   developed,
                   USAF-
                   operated
                   Turnkey)
                                         AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




What is included?   Operational evaluation is a continuation of the procedures and
                    data collection begun in summative evaluation. It usually starts
                    at the TSRR and lasts throughout the life cycle of the program.
                    The emphasis shifts from establishing the instructional value of
                    the courses to detecting flaws or deterioration. The prime goal is
                    to maintain and improve course quality throughout the life cycle.
                    The following issues should be addressed in operational
                    evaluation:

                       Measurement and assessment of student learning in
                       comparison to established training requirements and
                       objectives
                       Measurement of terminal objectives (qualification/certification)
                       Identification and resolution of discrepancies and deficiencies
                       in courseware
                       Assessment of training in light of modification/upgrades in the
                       defense system


How is it           Operational evaluation is conducted by both internal and external
conducted?          means.

                    Internal evaluation can be conducted by reviewing:

                       Course documents
                       Resources
                       Instructional facilities
                       Instructor performance
                       Measurement programs
                       Other sources as necessary

                    External evaluations can be conducted by using:

                       Questionnaires

                          For graduates
                          For supervisors

                       Field visits
                       Job performance evaluation
                       Other methods (i.e., SIMCERT Simulator certification)
AFH 36-2235      VOLUME 3      1 NOVEMBER 2002                                      145


                              Chapter 10
              AIR FORCE-DEVELOPED MAINTENANCE TRAINING
                                     Overview


Introduction        This chapter is for the individual who has been charged with
                    managing or being on a team responsible for applying ISD to Air
                    Force-developed maintenance training concurrently with defense
                    system acquisition. This application would normally take place
                    during the early acquisition cycles of a new defense system, but
                    could occur late in the cycle or even after the system has been
                    fielded. Regardless of when your involvement begins, this
                    chapter will help ensure that you do the right thing at the right
                    time.


Description         Maintenance training is performance- or knowledge-based
                    training for people who will maintain defense systems. The term
                    defense system is all-encompassing. A defense system is not
                    limited to aircraft, bombs, guns or missiles, but can include
                    anything needed to support the mission. For example, radars,
                    computers, trucks and engines can be considered part of these
                    systems.


Where to read       This chapter contains two sections. The first section explains the
about it            major functions in applying ISD to Air Force-developed
                    maintenance training. Section B provides an explanation of a
                    15-step ISD process used in maintenance training.

                     Section        Title                                    Page
                       A         Major Functions in Applying ISD to Air       143
                                 Force- Developed Maintenance
                                 Training
                         B       ISD Process Applied to Maintenance           149
                                 Training Acquisition Environment

                    Note: This chapter is primarily applicable to maintenance
                    training. However, it contains several management tools that can
                    assist you in various ISD tasks, no matter what your involvement
                    or function. Use and adapt as you desire.
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                  Section A
                      Major Functions in Applying ISD to
                  Air Force-Developed Maintenance Training

Introduction          The major functions of applying ISD concurrently with acquisition
                      are general in nature and cover the normal management-type
                      areas. This section has been written as if you are to be a team
                      chief or project manager of a portion of the ISD team. Modify the
                      guidance provided to best suit your needs.


Critical              There are many functions that determine the level of success.
Factors               These include the following topics:

                         Planning
                            Scheduling
                            Collecting/reviewing data

                         Building work teams
                         Training
                         Managing workloads
                         Conducting meetings
                         Timeliness
                         Quality


Planning              Planning for training development is one of the most important
                      activities required for successful program management. Proper
                      planning requires you to make time on your calendar for this
                      critical item. Planning includes anything needed for successful
                      completion of the project.
                                     Make time on your calendar.



Planning activities   Some key planning activities are:

                         Scheduling
                         Collecting and reviewing data
AFH 36-2235        VOLUME 3            1 NOVEMBER 2002                                                                       147




Scheduling            Scheduling is the process of defining what needs to be done and
                      when it must be done to facilitate the smooth progression of work
                      leading to project completion. Several tools are available to
                      assist you. The ones you select will depend on your needs.


Scheduling tools      Some scheduling tools include Gantt charts and PERT charts.
                      Examples are shown in Figures 19 and 20, respectively.


Gantt charts          Gantt charts are an organizing tool to visually indicate resources
                      and activities with a designated time frame. They are used to
                      compare planned completion dates with actual performance.
                      These charts consist of a list of tasks to be accomplished and the
                      time allowed for each. Some tasks are sequential and some are
                      overlapping. A Gantt chart contains:

                         Horizontal time scale that depicts the length of the project
                         Vertical axis with list of all activities involved in the project
                         Horizontal bar indicating duration of each activity


Example               Figure 19 Example Gantt Chart

                                                                                 Task Duration (months)
                                 Activity Description
                                                                     1   2   3   4    5     6    7    8   9   10   11   12

                         1. Assess Instructional Needs
                         2. Define Planning Requirements
                         3. Conduct Mission Analysis
                         4. Conduct Task Analysis
                         5. Conduct Training Requirements Analysis
                         6. Conduct Media Analysis
                         7. Conduct Cost Analysis
                         8. Conduct Training System Analysis
                         9. Deliver Task Analysis Report
                         10. Deliver Training Requirements Report
                                                         AFH 36-2235 VOLUME 3        1 NOVEMBER 2002




PERT charts      PERT charts also show key data but include dependencies of
                 activities. They can help determine critical events and dates,
                 and help decide where resources can be better utilized and
                 where management attention is most needed. A PERT chart
                 contains:

                    Box for each sequentially arranged activity required to
                    complete the project
                    Depiction of input-output contingencies among activities


Example          Figure 20 Example PERT Chart

                                         2                                      6



                         1                        3                     5           7          10




                                                                4           8       9

                       1. Project Needs Analysis 6. Design Procedures
                       2. Feasibility Study 7. Development Procedures
                       3. Proposal Submission 8. Site Preparation
                       4. Project Approval 9. Hardware Delivery
                       5. Training Analysis 10. Implementation




Collecting and   Collecting and analyzing data occurs during the life of the
analyzing data   acquisition, beginning at concept exploration. Data can be
                 anything from blueprints to performance characteristics to
                 various training reports. The SPO Program Manager (PM),
                 through the Data Maintenance Office (DMO), is responsible for
                 acquiring the contractor data necessary to manage all aspects of
                 the program. The data collection and analysis process is
                 depicted in Figure 21.

                                                                                    Continued on next page
AFH 36-2235      VOLUME 3               1 NOVEMBER 2002                                          149




Collecting and   Figure 21 Data Collection and Analysis Process
analyzing data
(Continued)



                      DMO issues
                                              Sent to functional offices in SPO and
                       Data Call
                                              all participating commands and agencies


                  Functional Managers
                     Identify Data            Data Item Description (DID) may be tailored
                      Using DIDs              to eliminate unnecessary requirements


                     DMO Compiles
                    All Requirements          Eliminate unnecessary requirements (with coordination


                   PM conducts Data
                   Requirements Review        Serves as final review of data
                     Board (DRRB)             requirements justification


                   DMO Finalizes Contract
                   Data Requirements           Final check
                       List (CDRL)


                      CDRL to PM
                     for Inclusion in         Contracting office includes CDRL in contract
                         Contract



                     Data Delivered            As contract performance proceeds, data is delivered I
                      Per Contract             contract


                       Review and
                        Analysis
                                   AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                               Data Process


Data examples   Some examples of data include:

                   Program schedules
                   Training and Training Equipment Plan (TTEP)
                   Training support data
                   Course critiques
                   ISD management plan

                Note: Do not request or accept data unless you have a valid
                need. Data is expensive, more so when it is unnecessary. See
                Discussion on MIL-HDBK-29612 on Page 52.


Building work   Once you have developed a schedule and begun to collect and
teams           analyze data, you will have a better understanding of the type of
                staff members you will need to pull onto your team. Do this very
                carefully. Success depends on teamwork and dedication to the
                project. There are limitless numbers of books on team building
                and leadership secrets to success; but there are no short cuts.
                Check the books, review your leadership and management
                course materials, and talk to your network of trusted bosses and
                former bosses, as well as successful peers. Learn from them.
                Team members may come from unit resources or from TDY
                locations.


Training        Never assume that your team knows all there is to know. If they
                have worked on ISD projects before, and have been successful,
                then you can assume that they know what needs to be done.
                What you must do is set the work and training standards up front.
                Explain the project to the team and establish your rules of order.
                Survey the team and develop appropriate training. It may be
                something as simple as telephone etiquette to something as
                complex as preparing a wiring diagram of electrical components
                in a widget. The goal of ISD is to produce training and increase
                skills and knowledge, but you should do it for your team first and
                not neglect your own training.
AFH 36-2235   VOLUME 3      1 NOVEMBER 2002                                         151




Managing         If you‚Äôve been a manager for any length of time, you have heard
workloads        that the best way to manage workloads is to "delegate." But
                 delegation alone does not foster success. You must know what
                 and to whom to delegate. You must follow up, and know when to
                 follow up. No one ever admits to having extra personnel that
                 they want to give you to help out. So you must use your staff to
                 manage as necessary. Use the computer or manual systems for
                 tracking.


Conducting       "Everyone hates meetings." Not true! Everyone hates non-
meetings         productive, worthless meetings. Make sure yours are productive.
                 Don‚Äôt have a meeting for the sake of a meeting. Have an
                 agenda, stick to it, take notes, publish minutes and follow up.
                 Expect the same of your staff. Meetings are for communication,
                 but they also keep the team informed and on the right track.


Timeliness       "The early bird gets the worm." Maybe, but earlier you built a
                 schedule. If you built it right, stick to it and deliver as promised.
                 If there is a legitimate reason for slippage in the schedule, inform
                 your superiors. They would rather have an honest person who
                 wants to build and deliver a quality product with a schedule delay
                 than an inferior product on time.


Quality          "If you don‚Äôt have time to do it right the first time, when will you
                 have time to fix it?" Quality and timeliness go hand-in-hand and
                 cannot be overemphasized. Your team should focus on quality
                 and continuous improvement.
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                                Section B
                         ISD Process Applied to
               Maintenance Training Acquisition Environment

Introduction        If you read AFMAN 36-2234, you learned that the original ISD
                    model was considered to be a five-step process, even though the
                    "steps" were not intended to be followed in a typical "step-by-
                    step" process but in phases. The revised model has phases that
                    should be done in a continuous process, not step-by-step. You
                    learned earlier that ISD is a never-ending process. You will learn
                    through practical experience that in your particular operation or
                    program, modification of the phases may facilitate application of
                    ISD. If this helps ensure a quality product that meets the user‚Äôs
                    training objectives, then do it.

                    The following example is a synopsis of a 15-step ISD process
                    provided for your information and use as appropriate. This 15-
                    step process was designed to fit the peculiarities and limitations
                    of the AF-developed maintenance training acquisition
                    environment. It is taken from the procedures used by the 374
                    Training Development Squadron (TDS) of Air Education and
                    Training Command (AETC) as they apply ISD to new or
                    significantly modified defense systems. The ISD phases with the
                    15-step process are listed below. Note: This is only an
                    example of one organization‚Äôs application of ISD.

                                                                  Continued on next page
AFH 36-2235    VOLUME 3      1 NOVEMBER 2002                                       153




Introduction         Air Force        Step         374 Training Development
(Continued)
                    ISD Phases                       Squadron ISD Process
                  Analysis              1      Identify System Requirements
                                        2      Identify Characteristics of the Target
                                               Audience
                                        3      Determine Task-Based Training
                                               Requirements
                                        4      Determine Concept-Based Training
                                               Requirements
                                        5      Determine Media and Methodology
                  Design                6      Develop Instructional Strategies
                                        7      Identify Hardware Fidelity
                                               Requirements
                                        8      Identify Interactive Courseware
                                               Fidelity Requirements
                                        9      Identify Instructional Features
                  Development          10      Prepare Training Equipment
                                               Functional Specification
                                       11      Prepare Course Control Documents
                                       12      Prepare Instructional Materials and
                                               Tests
                                       13      Validate Instruction
                  Implementation       14      Conduct Training
                                       15      Evaluate Training


Applications      The maintenance training system (or portions) for most major Air
                  Force defense systems has been developed using these
                  procedures. Examples include:

                     B-1B
                     B-2
                     C-17
                     Joint STARS
                                         AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Other uses           By modifying the 15-step process (deleting or changing steps),
                     these procedures have been successfully applied to operator
                     training, ISD training, additional duty training, TQM training and
                     others. Use and modify as appropriate for your project.

                     Note: This section is a synopsis only. The 15-step process is
                     explained in further detail in Attachment E.


Background           As you learned earlier, the acquisition ISD process actually
                     begins when HQ USAF issues a PMD to the major commands.
                     This document directs the SPO to begin system acquisition. The
                     SPO then develops a PMP, which outlines responsibilities and
                     general management objectives. HQ AETC provides inputs to
                     Section II, Training, of the PMP. Part of AETC‚Äôs responsibility, in
                     support of a new defense system, is test participation. The
                     Director of Technical Training Operations makes the assignment
                     of a Responsible Agency (RA), HQ AETC/TTO (example, the 374
                     TDS was the RA for the B-1 program).


374 TDS              The 374 TDS, Plans and Evaluation branch, submits a proposed
participation plan   Participation Plan (PP) to HQ AETC/TTO. The PP shows how the
                     374 TDS proposes to manage ISD and test activities.

                                                                    Continued on next page
AFH 36-2235          VOLUME 3      1 NOVEMBER 2002                                     155




374 TDS                  Section         Description
participation plan
(Continued)                 1        Section 1 of the PP outlines objectives,
                                     background, related documents, schedules, and
                                     resources required for planning purposes.
                             2       After coordination with prime and associate
                                     centers, HQ AETC approves the PP and forwards
                                     Section 2 (ISD Objectives, Methodology,
                                     Schedules, and Resource Requirements) to the
                                     prime training center for inclusion in the Training
                                     Participation Plan (TPP) as the ISD Annex.
                             3       Section 3 [Operational Test and Evaluation (OT&E)
                                     Objectives, Methodology, Schedules, and Resource
                                     Requirements] is forwarded to either the Air Force
                                     Operational Test and Evaluation Center (AFOTEC)
                                     or the Responsible Test Organization (RTO). HQ
                                     AETC then approves required personnel
                                     authorizations (identified in the PP) for the 374
                                     TDS. Personnel for these authorizations may be
                                     drawn from the prime center, using command, or
                                     others as directed by the Field Training Group
                                     (FLDTG).


ISD team                The 374 TDS begins the ISD effort by organizing an ISD team.
organization            Whenever possible, a senior NCO experienced in squadron and
                        test management procedures is assigned as Acquisition Training
                        Development Manager (ATDM). The ATDM leads the ISD team.
                        This person is responsible for meeting the objectives outlined in
                        the PP. The ATDM also ensures that the ISD team is trained in
                        the 15-step ISD process for acquisition and is familiar with
                        AFMAN 36-2234, AFH 36-2235, and AFI 36-2201. When
                        trained, the ISD team begins compiling data on system
                        maintenance requirements. Once the data are collected, the ISD
                        team begins the analysis.
                                  AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




TRRRM          The results of the ISD analysis [training and Technical Training
               Material (TTM) requirements] are then reviewed by participating
               agencies in a Training Requirements Recommendation Review
               Meeting (TRRRM). The TRRRM is designed to review, consoli-
               date, and coordinate AETC TTM recommendations. It is also
               used to review proposed Course Control Documents (CCDs)
               prior to further action.


TRRRM          Participants in the TRRRM, other than 374 TDS, may include the
participants   HQ AETC Training Staff Officer (TSO), HQ AETC Training
               Equipment Resources Manager, Field Training Group, Training
               Support Squadron, prime center Plans and Resources
               personnel, and associate center Plans and Resources
               personnel.


TRRRM          The result of the TRRRM is a consolidated package of
results        recommended TTM/CCDs. The package is reviewed by the
               Program Office (PO) (for funding and procurement action), who
               develops system specifications for each hardware training
               device. Specifications are then reviewed by HQ AETC and
               placed on contract by the PO.


Purpose        The purpose of allowing AETC personnel to review these
               specifications (before contracting) is to ensure that the TTM
               satisfies objectives identified during analysis. Subject matter
               specialists (SMS) may also participate in design reviews to
               ensure that required characteristics, as well as changes, are
               incorporated. Changes for training hardware after the TRRRM
               may require modification to the contract. Transition of SMSs to
               the first training site is scheduled to support operational
               requirements. SMSs become the nucleus of the initial instructor
               cadre. Procured TTM is delivered, installed, and operationally
               checked at the training site by contractors. When all resources
               are in place, the final ISD step can begin. By actively seeking
               system information during the early phases of System Acquisi-
               tion/Test and Evaluation and applying ISD, AETC can provide
               the PO with appropriate TTM requirements.
AFH 36-2235       VOLUME 3     1 NOVEMBER 2002                                       157




Synopsis of          The following is a synopsis of the 15-step ISD process.
15-step process

                      Step        Title           Description
                       1     Identify System During this step, references are
                             Requirements    researched and data gathered
                                             concerning a given Air Force Specialty
                                             Code (AFSC); specifically, the duties
                                             and tasks performed in an AFSC.
                       2     Identify        This step is used to analyze previously
                             Characteristics learned skills and knowledge to
                             of the Target   determine a target population
                             Population      definition.
                       3     Determine       What students can already do (Step 2)
                             Task-Based      is subtracted from what they must do
                             Training        (Step 1). The remainder is defined as
                             Requirements    "potential training requirements," or
                                             what students must be taught.
                       4     Determine       Here, the analyst determines if
                             Concept-Based fundamental concepts (ideas) need to
                             Training        be taught, such as principles of fluid
                             Requirements    dynamics or atomic structure.
                       5     Determine       The best media class to satisfy a given
                             Media and       training requirement is chosen in this
                             Methodology     step as well as the best method of
                                             instruction for the media class.
                       6     Develop         Skills and knowledge are
                             Instructional   re-sequenced into activities, activities
                             Strategies      into tasks, and tasks into units of
                                             instruction. Also, written guidance is
                                             developed for important/ specific
                                             aspects of the training scenario.
                       7     Identify        Procedures within this step enable
                             Hardware        analysts to determine how closely
                             Fidelity        (functionally and physically) hardware
                             Requirements    training devices must mimic actual
                                             system hardware
                       8     Identify ICW    Behavioral training requirements and
                             Fidelity        concepts, with ICW, are analyzed to
                             Requirements    determine peculiar features.

                                                                  Continued on next page
                                    AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Synopsis of       9    Identify           Analysts determine who/what provides
15-step process        Instructional      stimuli, response, feedback, and next
(Continued)            Features           activity for training requirements.
                  10   Prepare            This document compiles conclusions
                       Training           of the ISD effort. It specifies the
                       Equipment          number and type of hardware trainers,
                       Functional         functional and physical capabilities,
                       Specification      instructional features, etc.
                  11   Prepare            This step involves preparation of
                       Course Control     Course/Specialty Training Standards,
                       Documents          Course Charts, and Plans of
                                          Instruction.
                  12   Prepare            Study guides, workbooks, handouts,
                       Instructional      and any other materials required for
                       Materials and      the course are developed at this time.
                       Tests
                  13   Validate           This is accomplished while presenting
                       Instruction        a "dry run" of the entire training
                                          scenario. The training is "fine-tuned"
                                          for maximum transfer of learning and
                                          continuity.
                  14   Conduct            The training is actually conducted.
                       Training
                  15   Evaluate           This is the evaluation of training, both
                       Training           internal and external. Remember that
                                          evaluation has been occurring
                                          throughout the process.
AFH 36-2235    VOLUME 3     1 NOVEMBER 2002                                       159


                            Chapter 11
                  INTERACTIVE COURSEWARE (ICW)
                                   Overview


Introduction      There are additional considerations when you acquire interactive
                  courseware (ICW). A successful acquisition of ICW requires a
                  team effort. You can get assistance from within your
                  organization and by referring to:
                     AFH 36-2235, Volume 5, Interactive Courseware (ICW)
                     Design, Development, and Management Guide
                     MIL-HDBK-284-1, Interactive Courseware (ICW) for Military
                     Training, Manager‚Äôs Guide for Development, Acquisition, and
                     Management
                     MIL-HDBK-284-2, Interactive Courseware (ICW) for Military
                     Training, Probability Practices
                     MIL-HDBK-284-3, Interactive Courseware (ICW) for Military
                     Training, Glossary


Purpose           The purpose of this chapter is to give general guidance for
                  contractor-developed ICW.


AFH 36-2235,      This USAF Handbook provides information and guidance for
Volume 5A         applying current instructional technology and the ISD process as
                  described in AFMAN 36-2234. It is useful in helping to decide
                  when to use ICW and provides many guidelines and decision
                  aids.


MIL-HDBK-         This handbook (MIL-HDBK-284-1) describes all phases of ICW
284-1             analysis, design, development, implementation, and logistic and
                  life cycle support. This handbook is intended to be used in
                  conjunction with MIL-HDBK-29612, Military Training Programs.


MIL-HDBK-         Also to be used with MIL-HDBK-29612, this handbook (MIL-
284-2             HDBK-284-2) can help ICW device manufacturers, developers,
                  and users implement the mandatory software interface and
                  command requirements for ICW and authoring systems.
                  Information and guidance is provided for:

                                                               Continued on next page
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




MIL-HDBK-              Personnel responsible for defining operational training
284-2 (Continued)      requirements
                       Life cycle support
                       ICW development
                       Acquisition
                       Implementation


MIL-HDBK-           This handbook (MIL-HDBK-284-3) is a comprehensive glossary
284-3               containing definitions of all key terms used in the above
                    handbooks. Key terms used in MIL-HDBK-29612 are repeated in
                    this handbook to provide a single comprehensive glossary of
                    terms and definitions related to military training and interactive
                    courseware.


Tailoring           Detailed information on the process for acquiring ICW should be
                    tailored to agree with your acquisition strategy. This tailoring
                    should include deliverable data requirements.


Proposal grading    The following table presents a method for evaluating ICW
                    proposals leading to a contract award.

                          MIL-HDBK-284-1 PROPOSAL GRADING SCHEME
                     Proposal     Evaluation Level           Evaluation Scoring
                     Division
                    Volume       Evaluation area      Color (assign colors to denote
                                                      area is exceptional,
                                                      acceptable, marginal or
                                                      unacceptable)
                    Chapter      Evaluation item      Adjective (exceptional,
                                                      acceptable, marginal or
                                                      unacceptable)
                    Section      Evaluation factor    Points
                    Paragraph    Sub-factor           Check (acceptable), Plus
                                                      (exceeds requirements), or
                                                      Minus (deficient)
AFH 36-2235       VOLUME 3       1 NOVEMBER 2002                                      161




Negotiated           There are many activities that occur during a negotiated
acquisition          acquisition of ICW. An extended list is available in MIL-HDBK-
process              284-1, and is summarized below.

                                  NEGOTIATED ACQUISITION PROCESS
                     Milestone        Activities
                          1           Define requirements and gather background
                                      data.
                                      Prepare acquisition plan and gain approval.
                          2           Prepare solicitation package.
                          3           Conduct reviews and gain funding commitment.
                          4           Issue synopsis in Commerce Business Daily
                                      (CBD).
                                      Conduct pre-solicitation conference.
                          5           Issue solicitation.
                                      Conduct pre-proposal conference.
                          6           Evaluate proposals.
                                      Prepare evaluation report.
                         7            Negotiate contract.
                         8            Carry out pre-award actions.
                         9            Award contract.
                       Post-          Conduct kick-off meeting.
                       Award          Carry out in-process reviews.

Other contracts      Besides negotiated contracts, other ICW contracts include:

                        ICW front-end analysis (FEA) contracts
                        ICW design, development and implementation (DD/I)
                        contracts
                        ICW integrated logistic support (ILS) contracts

Portability          MIL-HDBK-284-1, Part 2, provides guidance on implementing the
practices            software interface and command requirements established by
                     DODI 1322.20 and MIL-HDBK-29612. The goal is to ensure that
                     ICW and authoring systems are portable.




                                          RICHARD E. BROWN III, Lt. General, USAF
                                          DCS/Personnel
                                          AFH 36-2235 VOLUME 3     1 NOVEMBER 2002


                                   ATTACHMENT 1

      GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

                                      REFERENCES
AFPD 36-22               Military Training
AFI 36-2201              Developing, Managing and Conducting Military Training
AFI 36-2301              Professional Military Education
AFMAN 36-2234            Instructional System Development
AFMAN 36-2236            Handbook for Air Force Instructors
AFH 36-2235              Information for Designers of Instructional Systems (12 Volumes)
                 Vol 1   ISD Executive Summary for Commanders and Managers
                 Vol 2   ISD Automated Tools/What Works
                 Vol 3   Application to Acquisition
                 Vol 4   Manager‚Äôs Guide to New Education and Training Technologies
                 Vol 5   Advanced Distributed Learning: Instructional Technology and
                         Distance Learning
                 Vol 6   Guide to Needs Assessment
                 Vol 7   Design Guide for Device-based Aircrew Training
                 Vol 8   Application to Aircrew Training
                 Vol 9   Application to Technical Training
                Vol 10   Application to Education
                Vol 11   Application to Unit Training
                Vol 12   Test and Measurement Handbook


      Walter Dick, Lou Carey, James O. Carey (2000). The Systematic Design of
                   th
      Instruction 5 Edition. Addison-Wesley Pub Co. ISBN 0321037804.

      Patricia Smith & Tillman Ragan (1999). Instructional Design, 2nd Edition. John
      Wiley & Sons 399 pages. ISBN 047136570X.

      Ruth Clark (1999). Developing Technical Training 2nd Edition: A Structured
      Approach for Developing Classroom and Computer-based Instructional
      Materials. International Society for Performance Improvement. 238 pages. ISBN 1-
      890289-C7-8
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                      163




   Robert M. Gagn√©, Leslie J. Briggs, and Walter W. Wager. (1992). Principles of
   Instructional Design 4th Edition. Wadsworth Pub. Co. 365 pages. ISBN
   00300347572

   Robert M. Gagn√© (1985). The Conditions of Learning and Theory of Instruction
    th
   4 Edition. Holt, Rinehart and Winston. ISBN 0-03-063688-4

   Jeroen J. G. van Merri√´nboer (1997). Training Complex Cognitive Skills.
   Educational Technology Publications.

   Ruth Clark (1998). Building Expertise: Cognitive Methods for Training and
   Performance Improvement. International Society for Performance Improvement.
   204 pages. ISBN 1890289043.

   Bernice McCarthy (1996). About Learning. Excel, Inc. 452 pages. ISBN 0-
   9608992-9-4

   Malcolm Fleming & W. Howard Levie (Editors) (1993). Instructional Message
   Design: Principles from the Behavioral and Cognitive Sciences 2nd Edition.
   Educational Technology Publications. 331 pages. ISBN 0-87778-253-9.

   Ellen D. Gagn√©, Frank R. Yekovich, and Carol Walker Yekovich (1993). The
   Cognitive Psychology of School Learning. 2nd Edition. Addison Wesley
   Longman, Inc. 512 pages. ISBN 0673464164.

   Marcy P. Driscoll (1999). Psychology of Learning for Instruction 2nd Edition.
   Allyn & Bacon. 448 pages. ISBN 0205263216

   Ann E. Barron & Gary W. Orwig (1997). New Technologies for Education: A
                      rd
   Beginner‚Äôs Guide 3 Edition. Libraries Unlimited. ISBN 1563084775

   Robert Heinich & Michael Molenda (1998). Instructional Media and Technologies
   for Learning 6th Edition. Prentice Hall. 428 pages. ISBN 0138591598.

   Diana Laurillard (1993). Rethinking University Teaching: A Framework for the
   Effective Use of Educational Technology. Routledge. ISBN 0415092892.

   Tom Boyle and Tim Boyle (1996). Design for Multimedia Learning. Prentice Hall.
   275 pages. ISBN 0132422158.

   William W. Lee and Diana L. Owens (2000). Multimedia-Based Instructional
   Design: Computer-Based Training, Web-Based Training, and Distance
   Learning. Jossey-Bass. 304 pages. ISBN 0787951595.
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




      Margaret Driscoll & Larry Alexander (Editor) (1998). Web-Based Training: Using
      Technology to Design Adult Learning Experiences. Jossey-Bass Inc. 288
      pages. ISBN 0787942030.

      Brandon Hall (1997). The Web-Based Training Cookbook. John Wiley & Sons.
      496 pages. ISBN 0471180211.

      Stephen M. Alessi & Stanley P. Trollip (2000). Computer Based Instruction. Allyn
      & Bacon. 432 pages. ISBN 0205276911.

      Andrew S. Gibbons & Peter G. Fairweather (1998). Computer-Based Instruction.
      Educational Technology. 570 pages. ISBN 0877783012.

      Douglas M. Towne (1995). Learning and Instruction in Simulation
      Environments. Educational Technology. 351 pages. ISBN 0877782784.

      Thomas M. Duffy & David H. Jonassen (Editors) (1992). Constructivism and the
      Technology of Instruction: A Conversation. Lawrence Erlbaum Associates. 221
      pages. ISBN 0805812725.

      Brent G. Wilson (1995). Constructivist Learning Environments: Case Studies in
      Instructional Design. Educational Technology Publications. ISBN 0877782903.

      Roger C. Schank (Editor) (1997). Inside Multi-Media Case Based Instruction.
      Lawrence Erlbaum Associates. 451 pages. ISBN 080582538X.

      David H. Jonassen, Wallace H. Hannum & Martin Tessmer (1999). Task Analysis
      Methods for Instructional Design. Lawrence Erlbaum Associates. 275 pages.
      ISBN 0805830863.

      Allison Rossett (1999). First Things Fast: A Handbook for Performance
      Analysis. Jossey-Bass. 241 pages. ISBN 0787944386.

      Charles M. Reigeluth (Editor) (1983). Instructional-Design Theories and Models:
      An Overview of their Current Status. Lawrence Erlbaum Associates. 487 pages.
      ISBN 0-89859-275-5.

      Charles M. Reigeluth (Editor) (1999). Instructional-Design Theories and
      Models: A New Paradigm of Instructional Theory. Vol. II. Lawrence Erlbaum
      Associates. 715 pages. ISBN0-8058-2859-1.

      Tennyson, Robert D., Schot, Franz, Norbert, Seel & Dijkstra, Sanne. (Editors)
      (1997). Instructional Design International Perspective Vol. 1 Theory,
AFH 36-2235     VOLUME 3      1 NOVEMBER 2002                                    165


   Research, and Models. Lawrence Erlbaum Associates. 475 pages. ISBN 0-8058-
   1397-7.

   Sanne Dijkstra, Norbert Seel, Franz Schott & Robert D. Tennyson (Editors) (1997).
   Instructional Design: International Perspective Vol. 2: Solving Instructional
   Design Problems. Lawrence Erlbaum Associates. 418 pages. ISBN 0805814000.

   George M. Piskurich, Peter Beckschi, and Brandon Hall (Editors) (1999). The ASTD
   Handbook of Training Design and Delivery: A Comprehensive Guide to
   Creating and Delivering Training Programs -- Instructor-led, Computer-based.
   McGraw Hill. 530 pages. ISBN 0071353105.

   Charles R. Dills & A. J. Romiszowski (Editors) (1997). Instructional Development
   Paradigms. Educational Technology Publications. 882 pages. ISBN
   08777882954.

   Sanne Dijksra, Bernadette van Hout Wolters & Pieter C. van der Sijde (Editors)
   (1990). Research on Instruction: Design and Effects. Educational Technology
   Publications. ISBN 0877782210.

   David H. Jonnassen (Editor) (1996). Handbook of Research on Educational
   Communications and Technology. Macmillan. 1267 pages. ISBN 0028646630.

   Byron Reeves & Clifford Nass (1996). The Media Equation: How People Treat
   Computers, Television, and New Media Like Real People and Places.
   Cambridge University Press. 305 pages. ISBN 1-57586-053-8.

   Donald A. Norman (1990). The Design of Everyday Things. Doubleday &
   Company. 256 pages. ISBN 0385267746,

   Bills, C. G. and Butterbrodt, V. L. (1992). Total Training Systems Design
       Function: A Total Quality Management Application. Wright-Patterson AFB,
       Ohio.

   Briggs, L. J. and Wager, W. W. (1981). Handbook of Procedures for the Design
      of Instruction (2nd Ed.). Glenview, Illinois: Harper Collins Publishers.

   Carlisle, K. E. (1986). Analyzing Jobs and Tasks. Englewood Cliffs, New Jersey:
      Educational Technology Publications.

   Davies, I. K. (1976). Objectives in Curriculum Design. London: McGraw Hill.

   Dick, W. and Carey, L. (1990). The Systematic Design of Instruction (3rd Ed.).
      Glenview, Illinois: Harper Collins Publishers.
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


  Fishburne, R. P., Williams, K. R., Chatt, J. A. and Spears, W. D. (1987). Design
     Specification Development For The C-130 Model Aircrew Training System:
     Phase I Report. Williams AFB, Arizona: Air Force Human Resources Laboratory
     (AFHRL-TR86-44).
Gagn√©, R. M. (1985). The Conditions of Learning (4th Ed.). New York: Holt, Rinehart
     and Winston.

Gagn√©, R. M., Briggs, L. J., and Wager, W. W. (1992). Principles of Instruction (4th
     Ed.). New York: Harcourt Brace Jovanovich College Publishers.

Gagn√©, R. M. and Merrill, M. D. (1990). Integrative Goals for Instructional Design.
     Englewood Cliffs, New Jersey: Educational Technology Publications.

JWK International Corp. (1990). Final Training System Baseline Analysis Report
     (EWOT). Dayton, Ohio: JWK International Corp.

Keller, J. M. (1987). The Systematic Process of Motivational Design. Performance
       and Instruction, 26(9), 1-8.

Kibler, R. J. (1981). Objectives for Instruction. Boston: Allyn and Bacon.

Knirk, F. G. and Gustafson, K. L. (1986). Instructional Technology: A Systematic
       Approach to Education. New York: Holt, Rinehart, and Winston.

Leshin, C. B., Pollock, J., and Riegeluth, C. M. (1992). Instructional Design
      Strategies and Tactics. Englewood Cliffs, New Jersey: Educational Technology
      Publications.

Mager, R. F. (1962). Preparing Objectives for Instruction (2nd Ed.). Belmont,
      California: Fearon Publishers.

O'Neil, H. F., Jr., and Baker, E. L. (1991). Issues in Intelligent Computer-Assisted
       Instruction: Evaluation and Measurement. In T. Gutkin and S. Wise (Eds.),
       The Computer and the Decision-Making Process. Hillsdale, New Jersey:
       Erlbaum Associates.

Reigeluth, C. M. (1983). Instructional Design: What Is It and Why Is It? In C.M.
      Reigeluth (Ed.), Instructional Design Theories and Models? An Overview of
      Their Current Status. Hillsdale, New Jersey: Erlbaum Associates.

Rossett, A. (1987). Training Needs Assessment. Englewood Cliffs, New Jersey:
      Educational Technology Publications.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                      167


Spears, W. D. (1983). Processes of Skill Performance: A Foundation for the
      Design and Use of Training Equipment. (NAVTRAEQ-VIPCEN 78-C-0113-4).
      Orlando, Florida: Naval Training Equipment Center.

Tennyson, R. D. and Michaels, M. (1991). Foundations of Educational Technology:
     Past, Present and Future. Englewood Cliffs, New Jersey: Educational
     Technology Publications.

Williams, K. R., Judd, W. A., Degen, T. E., Haskell, B. C., & Schutt, S. L. (1987).
       Advanced Aircrew Training Systems (AATS): Functional Design
       Description. Irving, Texas: Seville Training Systems (TD-87-12).

Wolfe, P., Wetzel, M., Harris, G., Mazour, T. and Riplinger, J. (1991). Job Task
      Analysis: Guide to Good Practice. Englewood Cliffs, New Jersey: Educational
      Technology Publications.

                          Abbreviations and Acronyms
AETC                    Air Education and Training Command
AF                      Air Force
AFH                     Air Force Handbook
AFI                     Air Force Instruction
AFMAN                   Air Force Manual
AFMC                    Air Force Materiel Command
AFOTEC                  Air Force Operational Test and Evaluation Center
AFP                     Air Force Pamphlet
AFPD                    Air Force Policy Directive
AFSC                    Air Force Specialty Code
ASP                     Acquisition Strategy Panel
ATDM                    Acquisition Training Development Manager
ATS                     Aircrew Training System
CALS                    Computer-Aided Logistics Support
CBI                     Computer-Based Instruction
CBT                     Computer-Based Training
CCD                     Course Control Document
CDC                     Career Development Course
CDR                     Critical Design Review
CDRL                    Contract Data Requirements List
CI                      Configuration Item
CIDS                    Configuration Item Development Specifications
CRB                     Course/Curriculum Review Board
CRR                     Course Readiness Review
CW                      Courseware
DEMVAL                  Demonstration and Validation
DID                     Data Item Description
                          AFH 36-2235 VOLUME 3   1 NOVEMBER 2002


DMO        Data Maintenance Office or Data Management Office/Officer
DoD        Department of Defense
DODI       Department of Defense Instruction
DR         Discrepancy Report
DT&E       Developmental Test and Evaluation
FCA        Functional Configuration Audit
FEA        Front-End Analysis
FLDTG      Field Training Group
GFE        Government Furnished Equipment
GFP        Government Furnished Property
HQ         Headquarters
HW         Hardware
IAW        In Accordance With
ICW        Interactive Courseware
ILS        Integrated Logistic Support
IMPACTS    Integrated Manpower, Personnel & Comprehensive Training &
                Safety
IOC        Initial Operational Capability
IPP        IMPACTS Program Plan
ISD        Instructional System Development
ITO        Individual Tryout
JPR        Job Performance Requirement
LGTO       Large-Group Tryout
LSA        Logistics Support Analysis
MAJCOM     Major Command
MER        Manpower Estimate Report
MIL-HDBK   Military Handbook
MIL-STD    Military Standard
MOA        Memorandum of Agreement
MOU        Memorandum of Understanding
NCO        Noncommissioned Officer
OPR        Operational Test and Evaluation
OT&E       Operational Test and Evaluation
PDR        Preliminary Design Review
PEM        Program Element Monitor
PERT       Program Evaluation and Review Technique
PIDS       Prime Item Development Specifications
PM         Program Manager
PMD        Program Management Document/Directive
PMP        Program Management Plan
PO         Program Office
PP         Participation Plan
PRGC       Program Requirements Guidance Conference
QA         Quality Assurance
QAF        Quality Air Force
AFH 36-2235   VOLUME 3    1 NOVEMBER 2002                                169


QC                 Quality Control
QI                 Quality Improvement
RA                 Responsibility Agency
RAA                Required Assets Available
RDD                Required Delivery Date
RFI                Request for Information
RFP                Request for Proposal
RTO                Responsible Test Organization
SEGC               System Engineering Guidance Conference
SEMP               System Engineering Management Plan
SEMS               System Engineering Master Schedule
SFI                Search for Information
SGTO               Small-Group Tryout
SIMCERT            Simulator Certification
SKA                Skills, Knowledge, and Attitudes
SME                Subject Matter Expert
SMS                Subject Matter Specialist
SOW                Statement of Work
SPO                System Program Office
SRD                System Requirements Document
SRR                System/Site Readiness Review
STP                System Training Plan
STRR               Site Training Readiness Review
STS                Specialty Training Standard
SW                 Software
TD                 Training Discrepancy
TDS                Training Development Squadron
TDY                Temporary Duty
T&E                Test and Evaluation
TMS                Training Management System
TO                 Technical Order
TPM                Technical Performance Measure
TPP                Training Participation Plan
TPT                Training Planning Team
TRAR               Training Requirements Analysis Report
TRRRM              Training Requirements Recommendation Review Meeting
TSBA               Training System Basis Analysis
TSC                Training System Concept
TSIP               Training System Implementation Plan
TSO                Training Staff Officer
TSR                Training System Requirements
TSRA               Training System Requirements Analysis
TSRR               Training System Readiness Review
TSSC               Training System Support Center
TTEP               Training and Training Equipment Plan
                                           AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


TTM                     Technical Training Material
TTO                     Technical Training Operations
USAF                    United States Air Force
WBS                     Work Breakdown Structure

                                          Terms
The following list of definitions includes those terms commonly used to discuss training,
training systems and acquisition of training systems as they relate to instructional
system development and as used in this handbook. The list is not to be considered all-
inclusive.

Acquisition. The procurement by contract, with appropriated funds, of supplies or
services (including construction), by and for the use of the Federal Government
through purchase or lease, whether the supplies or services are already in existence or
must be created, developed, demonstrated, and evaluated.

Acquisition Management Systems and Data Requirements Control List (AMSDL).
A listing of Source Documents and Data Item Descriptions (DID) which have been
approved for repetitive contractual application in DoD acquisitions and those that have
been canceled or superseded. The AMSDL is identified as DoD Directive 5010.12-L.
Also see Data Item Description.

Acquisition Plan. A document that records program decisions; contains the user‚Äôs
requirements; provides appropriate analysis of technical options; and includes life cycle
plans for development, testing, production, training, and logistic support of material
items.

Acquisition Program. A directed, funded effort that is designed to provide a new or
improved material capability in response to a validated need.

Administration. The function concerned with the day-to-day tasks of operating an
instructional system. Administration is a form of management or supervision that tends
to be a catchall activity to absorb tasks not clearly appropriate elsewhere.
Administration contributes significantly to the overall effectiveness of the instructional
system.

Affective Domain. A classification of educational objectives that focus on the
development of attitudes, beliefs, and values.

Affective Learning. A domain of learning concerned with the acquisition of desired
perceptions by the trainee; that is, the order and discipline required within the military.
It is that part of trainee learning objectives which requires the acquisition of perceptions
by the trainee; promoting, for example, self-confidence, responsibility, respect,
dependability, and personal relations. Also see Learning Objective.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                       171




Aircrew Training System (ATS). A contractor-produced, -operated and -maintained
ground-based system used to train aircrew members. ATS includes training
equipment, software, firmware, hardware, devices, courseware, training system
support, logistics support, ground-based instruction, media, and facilities. It typically
does not include flight training or aircraft support.
Analysis. (a) Examination of system requirements. (b) Separation of a whole into its
component parts for detailed study or examination; for example, a job is broken down
into all its observable components, duties, tasks, task elements, and skills. (c) A level
of cognitive domain in which people are able to break down complex organizational
structures into their component parts. (d) Assembly of a complete data bank of
information.

Ancillary Materials. Documents that integrate the use of instructional media materials
by directing the instructor and trainee use of the materials and providing supplemental
information. Ancillary materials may be self-study workbooks, lecture guides, trainee
guides, exercise controller guides, or instructor utilization handbooks.

Ancillary Training. Training in subjects that pertain to the duty performance of
personnel but are separate from the individual‚Äôs primary job. Included is training in
those subjects not identified by the individual‚Äôs job description. A program identified as
ancillary training may not necessarily be ancillary training to all personnel who receive
it.

Application. (a) The way in which technology is used. (b) A level of cognitive domain
in which trainees are able to use learned material in new and concrete situations. (c)
Application software. (d) The process of reviewing and selecting from available
specifications, standards and related documents those which have application to
particular material acquisitions, and contractually invoking them wholly or in part at the
most advantageous time in the acquisition cycle.

Attitude. (a) The emotions or feelings that influence a learner‚Äôs desire or choice to
perform a particular task. (b) A positive alteration in personal and professional beliefs,
values, and feelings that will enable the learner to use skills and knowledge to
implement positive change in the work environment. Also see Knowledge and Skill.

Audiovisual Medium. Any delivery device or system that provides both audio and
visual presentations.

Automated Task Analysis. A computer-assisted environment that prompts the analyst
to input the required information and stores the results in a computer-managed
database.

Baseline Comparison System. An existing or predecessor system or combination of
systems with characteristics similar to the proposed weapons system. The system is
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


normally used during front-end analysis to help predict proposed weapons system
manpower, personnel, and training (MPT) requirements.

Baseline Data. Valid and reliable information about the current level of performance of
the intended trainee population. Baseline data can be used to confirm the need to
develop new instruction or can be used as a comparison in ascertaining differences
between the trainees‚Äô performance before and after instruction.
Behavior. Any activity, overt or covert, capable of being measured.

Behavioral Objective. See Learning Objective.

Behavior Analysis. (a) The process by which a complex behavior is broken down into
teachable components. (b) The analysis of each task or subject area to determine what
the trainee must do upon completion of instruction, how and how well the trainee must
be able to do it, and what skills and knowledge must be taught in order for the trainee
to meet the end of instruction.

Behavior Indicator. See Sample of Behavior.

Block of Instruction. A group of related instructional units or modules covering a
major subject area.

Block Update. A process whereby several modifications may be grouped together to
maintain training system concurrency in the most cost-effective, efficient manner, while
reducing training device downtime.

Cadre Training. Training of an initial (nucleus) group of personnel, such as
instructors.

Case Study. A learning experience in which trainees encounter a real-life situation
under the guidance of an instructor/computer in order to achieve an instructional
objective.

Class Capacity. The number of trainees that may be trained per class, expressed in
terms of three constraining factors: (1) Personnel allowance ‚Äì number of trainees that
may be trained per class based on the number of instructor and non-instructor
billets/authorizations contained in the manpower authorization and used locally to
support the course; (2) Equipment ‚Äì number of trainees that may be trained per class
based on the amount of equipment available per scheduled class period; and (3)
Classroom space ‚Äì number of trainees that may be trained per class based on the
availability of classroom space for a specific class. Classroom/training space includes
laboratory, shop, hangar, or any other space configured for training purposes.

Class Frequency. The number of times a course will convene during a fiscal year.
AFH 36-2235       VOLUME 3        1 NOVEMBER 2002                                         173


Cognitive Domain. A classification of educational objectives characterized by their
dependence upon the manipulation of the mental process (thinking, understanding).

Collective Training. Instruction and applied exercises that prepare an organizational
team (such as a squad, aircrew, battalion, or multi-service task force) to accomplish
required military tasks as a unit.

Competency-Based Instruction. Instruction that is derived from and organized
around an agreed-upon set of competencies and that provides learning experiences
designed to lead to the attainment of these competencies.

Computer-Assisted Instruction (CAI). The use of computers to aid in the delivery of
instruction. A variety of interactive instructional modes are used including tutorial, drill,
practice, gaming, simulation, or combinations. Students interact with instruction
presented through a variety of media, usually computer-controlled or -monitored. CAI
is an integral part of computer-based instruction (CBI) and computer-based training
(CBT). Also called Computer-Aided Learning.

Computer-Based Instruction (CBI). The use of computers to aid in the delivery or
management of instruction. Also called Computer-Based Education and Computer-
Based Learning.

Computer-Based Training (CBT). Training in which computers are used for both
training delivery and training management. The management functions often include
scheduling, lesson selection, score keeping, and quality of student responses.

Computer-Based Training System. A training system consisting of computers that
provide instruction. It is an automated, integrated instructional system that includes the
design and development of instructional materials (authoring system), the management
and administration of training, and the delivery of that instruction.

Computer-Managed Instruction (CMI). The use of computers to manage the
instructional process, generally including tasks such as registration, pretesting,
diagnostic counseling, prescription of learning experiences, progress testing, post
testing, determination of student mastery of objectives, and disenrollment.

Condition. That portion of the learning objective that describes the
situation/environment in which the trainees write/express/perform the specified
behavior. Conditions include any pertinent influence on task performance, including
any or all of the following: location of performance, environment, equipment, manuals,
or supervision required.

Contract. A mutually binding legal relationship obligating the seller to furnish the
supplies or services (including construction) and the buyer to pay for them. Contracts
include all types of commitments that obligate the Government to an expenditure of
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


funds and that, except as otherwise authorized, are in writing. In addition to bilateral
instruments, contracts include (but are not limited to) awards and notices of awards; job
orders or task letters issued under basic ordering agreements; letter contracts; orders,
such as purchase orders, under which the contract becomes effective by written
acceptance or performance; and bilateral contract modifications.

Contract Data Requirements List (CDRL). A list of the data requirements authorized
to be acquired for a specific acquisition, which is made a part of the contract. Uses DD
Form 1423.

Contract Deliverables. Materials delivered by a contractor. Examples of contract
deliverables are lesson plans, trainee guides, and test packages. Also see
Deliverable Data.

Contracting Activity. An element of an agency designated by the agency head, and
delegated broad authority regarding acquisition functions.

Contract Logistics Support (CLS). A preplanned method used to provide all or part
of the logistics support to a system, subsystem, modification, or equipment throughout
its life cycle. CLS covers depot maintenance and, as negotiated with the operating
command, necessary organizational and intermediate (O&I) level maintenance,
software support, and other operation and maintenance (O&M) tasks.

Contractor. An individual or organization outside the Government, which has
accepted any type of agreement or order for providing research, supplies, or services
to a Government agency.

Contractor-Acquired Property. Property acquired or otherwise provided by the
contractor for performing a contract and to which the Government has title. Also see
Contractor-Furnished Equipment and Government Property.

Contractor-Furnished Equipment. Items manufactured or purchased by the
contractor for inclusion in or support of contract work. Also see Contractor-Acquired
Property.

Contractor Plant Service. See Factory Training.

Contractor Specialized Training. See Factory Training.

Contractor Support. A generic support method of supplementing Air Force logistics
resources either for a temporary period or for the life of a system or equipment.

Contractual Data Requirement. A data requirement that applies by virtue of the terms
of a contract.
AFH 36-2235       VOLUME 3        1 NOVEMBER 2002                                         175


Controlled Testing. A controlled study to test or evaluate an item or subject, used, for
example, for obtaining validation data.

Correspondence Course. A self-study course consisting of instructional material and
an assignment booklet (or lessons) for administration to nonresident trainees. Also see
Extension Training.

Cost/Benefit Tradeoff Analysis. An analytic approach to solving problems of choice.
It requires definition of objectives, identification of alternative ways of achieving each
objective, and identification for each objective of that alternative that yields the greatest
benefit for a given cost or produces the required level of benefits at the lowest cost.
When the benefits or outputs of the alternatives cannot be quantified in terms of
dollars, this process is referred to as cost-effectiveness analysis. Also see Cost-
Effectiveness Analysis.

Cost-Effectiveness. (a) A comparative evaluation of potential instruction methods and
media to determine the most efficient alternative. (b) A measure of the operational
capability added by a system as a function of its life cycle cost.

Cost-Effectiveness Analysis. A comparative evaluation of potential instruction
methods and media to determine the most efficient alternative.

Course. (a) Logically grouped instruction on a subject, designed to achieve predefined
learning objectives. Usually concerns a single job or task (job-skills-type training) or a
section of organized knowledge (information-type training). (b) A complete series of
instructional units identified by a common title or number. (c) An ordered arrangement
of subject matter designed to instruct personnel in the knowledge, skills, or attitudes
required for the performance of tasks in a designated area of specialization. A course
consists of one or more modules. Also see Curriculum; Lesson; and Module.

Course Chart. A qualitative course control document that states the course identity,
length, and security classification, lists major items of training equipment, and
summarizes the subject matter covered.

Course Documentation. Information describing the current content of a course
(instructional materials, tests, instructor‚Äôs guide, evaluation plan, trainee guide) and its
developmental history (job analysis, criteria for selecting tasks for training, previous
revisions).

Course Evaluation. A critique of the course to include course effectiveness, instructor
effectiveness, technical documentation effectiveness, and effectiveness of training
media.

Course Map. A chart that identifies tasks to be learned, sequences them in desirable
order for learning, and indicates possible routes for learning when bottlenecks develop.
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Course Mission. The ultimate purpose of the course including whom is to be trained,
what is to be trained, the degree of qualification brought about by the training, and
where and under what general conditions the graduate will perform on the job.

Course Readiness Review (CRR). An Air Force review on a course-by-course basis
for the purpose of checking whether the course is ready for use in training. Following
successful CRR, the course is approved for use either by a training contractor or by the
Government training organization. CRR commences after completion of small-group
tryouts. Successful completion of CRR marks the beginning of system-level formative
evaluation.

Course Training Standard. A document that identifies the most common tasks and
knowledge requiring advanced training of specific equipment or systems, within an Air
Force Specialty.

Courseware. Training materials such as technical data, textual materials, audiovisual
instructional materials, and computer-based instructional materials.

Courseware Integration. Mixing of interactive courseware with other training media
(for example, classroom, laboratory, simulators, on-the-job training).

Courseware Maintenance. (a) Revision of curriculum after implementation. (b)
Repairing, changing, replacing, or any other manipulation of implemented courseware
after a customer has accepted it, or after it is determined to be correct in accordance
with the Statement of Work (SOW).

Criterion. (a) The standard by which something is measured. (b) In test validation, the
standard against which test instruments are correlated to indicate that accuracy with
which they predict human performance in some specified area. (c) In evaluation, the
measure used to determine the adequacy of a product, process, behavior, and other
conditions.

Criterion Behavior. Performance required of the course graduate, which is described
by the terminal objective and measured by the criterion test.

Criterion-Referenced Instruction. (a) A way of organizing and managing instruction
in which pre-specified performance criteria are achieved by each qualified trainee. (b)
Instruction designed to teach only those performances that are specified as critical to
the successful accomplishment of a defined task. Also see Performance-Oriented
Training.

Criterion-Referenced Objective (CRO). (a) A performance-oriented tool identifying
criteria and actions required to demonstrate mastery of a task. (b) An objective with
prescribed levels of performance. Each CRO contains a behavior (task statement), a
AFH 36-2235       VOLUME 3        1 NOVEMBER 2002                                         177


condition (available equipment, checklists, and governing directives, or the situation
requiring the task), and a standard (regulation, operating instruction) for the task.

Criterion-Referenced Test (CRT). The process of determining, as objectively as
possible, a student‚Äôs achievement in relation to a fixed standard based on criterion
objectives.

Critical Sequence. In training development, sequencing of topics or objectives
according to their importance.
Curriculum. A set of courses constituting an area of specialization. The curriculum
includes all training conducted within a school, outlined into specific topics, along with
detailed training objectives, to include behavior, conditions, and standards. Also see
Course.

Curriculum Materials. All materials required for the presentation of information and
the development of skills in formal training. The materials are a collection of various
visual, printed (that is, instructor and trainee guides), and audiovisual materials (that is,
instructional media), including interactive courseware, used in direct support of a
curriculum.

Curriculum Outline. A detailed chronological listing of units/modules and lesson
topics with estimated times of coverage in sequential order with the learning objectives
they support.

Data. (a) Recorded information, regardless of form or characteristics. (b) Basic
elements of information that can be processed, stored, or produced by a computer. (c)
Facts or numerical values resulting from measurement (observation) of situations,
objects, people, or events.

Database. (a) A collection of information, having one or more common elements,
organized for sorting and quick retrieval. (b) A program that files information. (c)
Systematically organized computer data files for central access, sorting, quick
searching, retrieval, and update.

Data Item Description (DID). A completed form that defines the data required of a
contractor. The form specifically defines the data content, preparation instructions,
format, and intended use (see DOD-STD-963). In acquisition of training for defense
systems, see MIL-HDBK-29612. Uses DD Form 1664. Acquisition Management
Systems and Data Requirements Control List.

Data Product. Information that is inherently generated as the result of work tasks
described in a Source Document or contract. Such information is treated as a separate
entity (for example, drawings, specifications, manuals, reports, records, or parts list).
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


Decision Point. (a) A point at which a program path can go in two or more directions.
(b) A point at which there are two or more options.

Decision Support System. The Joint Service ISD/LSAR Decision Support System.
The system is a set of software tools that provides training decision support analysis for
front-end instructional system development analysis (task selection for training,
learning objectives analysis, instructional setting selection, instructional sequencing,
media selection, and training equipment requirements analysis).

Decision Tree. (a) A flowchart or graphic representation of the sequence of a specific
activity or operation. (b) A system based on the premise that decisions spawn
outcomes that require other decisions. Choices feed into a network of other decisions
(usually represented by branches).

Defense Audiovisual Support Activity. An organization designated by the
Department of Defense to produce and acquire audiovisual products and to provide
audiovisual support for all DoD components in a specified geographic area.

Defense Instructional Technology Information System (DITIS). A standard, DoD-
wide database designed to facilitate ICW resource sharing within the DoD components
by providing a central source of ICW information. The DITIS database provides
information on all DoD-owned ICW programs, whether fielded or under development,
including information on delivery systems, operating software, authoring tools and
courseware for both planned and fielded ICW systems.

Defense School or Course. A school or course used by two or more military services
that is administered by a coordinating service/agency and that presents a curriculum
developed under the policy guidance and approval authority of an element of the Office
of the Secretary of Defense.

Defense System. Any weapon system, support system, or end item that supports a
specific military mission, therefore requiring operations, maintenance, or support
personnel training.

Defense System Training. Organized training conducted in a formal situation on
weapons, weapon systems, and related equipment for both operations and
maintenance personnel. Also called Weapon System Training.

Defense Technical Information Center (DTIC). The organization that acquires,
stores, retrieves, disseminates, and enhances technical information for research and
development for Government and industry.

Deliverable. See Deliverable Data.
AFH 36-2235      VOLUME 3       1 NOVEMBER 2002                                       179


Deliverable Data. For purposes of this handbook, the task outputs/data items
identified in DoD Inst. 29612 for acquisition. These outputs/data items are identified
and defined by DIDs and are intended to be cited on the Contract Data Requirements
List (CDRL) for delivery by the contractor. Also see Contract Deliverables.

Desktop Training Device. An off-the-shelf, commercially available, computer-based
training system consisting of both hardware and software.

Developmental Testing. The initial stage in which the instructional material is tried
out with individuals and small groups of trainees to determine if the product teaches the
subject and to locate portions of the instructional materials that need to be revised.
Also see Formative Evaluation.

DoD Index of Specifications and Standards (DODISS). The publication that lists
federal and military specifications, standards, and related standardization documents
and non-Government documents that is used by the military departments and agencies.

Domain of Learning. A generic classification of learning outcomes into one of three
primary but not necessarily mutually exclusive categories: cognitive (thinking,
understanding), affective (attitudes, values), and psychomotor (physical skills).

Effectiveness. The degree to which a training product or program meets its stated
training objectives.

Electronic Media. Devices used in the application of computer and communications
technologies to automate and support the free exchange of digitized technical data in
support of the development, delivery, and maintenance of training materials.

Embedded Training. Training provided in capabilities not specifically required for
mission completion, but that are built into or added onto operational systems,
subsystems, or equipment to enhance or maintain user skill proficiency.

Enabling Objective. A learning objective describing what is expected of the trainee in
demonstrating mastery of the skills and knowledge necessary for achievement of a
terminal objective or another enabling objective.

Entry-Level Training. The introductory and indoctrination training given to individuals
upon initial entry into a new job.

Entry Skills. Specific, measurable behaviors that have been determined through the
process of analysis of learning requirements to be basic to subsequent knowledge or
skill in the course.

Environment. The physical conditions and surroundings in which a job is performed,
or in which learning takes place, including tools, equipment, and job aids.
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Equipment. (a) Any device that supports any system or subsystem. (b) A major unit of
a subsystem for which operation and maintenance can be performed.

Evaluation. A judgment expressed as a measure or ranking of trainee achievement,
instructor performance, process, application, training material, and other factors (see
MIL-HDBK-29612). It includes Formative Evaluation; Operational Evaluation; and
Summative Evaluation.

Evaluation Information. Information collected for the purpose of assessing
performance of trainees, conduct of instruction, support of instruction, or any other
aspect of the instructional process.

Evaluation Plan. A method or outline of a set of procedures that will be used to gather
data and information for the purpose of assessing a course of instruction or other
training product.

Evaluation Program. A schedule for the coordinated, systematic, and continuous
assessment of the efficiency and effectiveness of the training system, its processes and
products.

Exercise. (a) An act that is performed or practiced in the learning experience to
develop, improve, or display a specific knowledge, skill, or aptitude. (b) The total
instruction that a trainee receives from a training experience.

Expert Opinion. Those opinions, impressions, or judgments of individuals considered
to be well qualified in relation to the item under evaluation. Also see Subject Matter
Expert.

Extension Training. Training, either individual or collective, that is usually conducted
at locations other than service schools or training centers. Also see Correspondence
Course.

Extension Training Material. All exportable training products, including materials that
are exported from one resident school to another as well as to operational units.

External Evaluation. The acquisition and analysis of feedback data from outside the
formal training environment to evaluate the graduate of the instructional system in an
operational environment. Also see Operational Environment.

Factory Training. Training or instruction provided by a vendor or manufacturer on
how to maintain or operate a specific piece of equipment. Also called Contractor
Plant Service and Contractor Specialized Training.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                          181


Familiarization Training. Field training to acquaint personnel with a specific system
or to keep personnel abreast of changing concepts and requirements.

Feedback. (a) Information that results from or is contingent upon an action.
Information on trainee performance is "fed" back to the trainee so he/she can improve
that performance; to the instructional designer so he/she can improve materials and
procedures on the basis of trainee needs; and to the management system so it can
monitor the internal and external integrity of the instruction and make appropriate
revisions. (b) Computer response to trainee input.

Fidelity. The degree to which a task or a training device represents actual system
performance, characteristics, and environment.

Field Test. Tryout of any training course on a representative sample of the target
population to gather data on the effectiveness of instruction in regard to error rates,
criterion test performance, and time to complete the course. Also see Individual
Tryout and Small-Group Tryout.

Field Training. Technical, operator or other training conducted at operational
locations on specific systems and associated direct-support equipment.

Field Validation. The point in training product development when the product is
administered to a representative sample of job incumbents. The intent is to exercise
the product in a realistic environment to determine the administrative feasibility and the
appropriateness of the product for the target population; determination that tasks taught
in residence and extension are, in fact, applicable to the trainee‚Äôs job. Also see
Individual Tryout and Small-Group Tryout.

Firmware. The combination of a hardware device and computer instructions or
computer data that reside as read-only software on the hardware device. The software
cannot be readily modified under program control.

Flowchart. (a) Documentation of the courseware instructional strategy. (b) A visual
method of indicating the many relationships of the sub-parts of a process, including
steps and decision points. (c) A programming guide that is a graphic representation of
all branching and data processing required for the interactive courseware. (d) A
diagram that depicts the events or actions and their sequence in the program. (e) A
map of interactive logic, representing the possible paths a user can take in the
courseware, and comprising standard symbols for program segments, decision points,
clues, responses, and logic flow. (f) A diagram representing the logic flow of a
computer program, using standard graphic shapes and symbols joined by straight lines
and representing program segments, decision points, execution flow, and other
information.
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


Flow Diagram. A graphic representation of actions/events required in accomplishment
of a task (for example, lesson development). Frequently accompanied by a narrative
description, the flow diagram provides specific instructions and precise sequencing for
task/goal accomplishment.

Follow-on Training. Training conducted after initial training.

Formal Lecture. A structured and often rehearsed teaching session with no verbal
participation by trainees.

Formal Training. Training in an officially designated course conducted or
administered in accordance with appropriate course outline and training objectives.

Formal Validation. The process of determining if the instructional and learning
objectives of the courseware are being met.

Format. (a) Organization of data in a specific way to meet an established system
standard. (b) The process of readying a new floppy or hard disk by having a computer
program write file mark and track mark codes on the disk. (c) The magnetic
arrangement of a disk into areas or sectors so it can receive and store data from the
operating system that formatted the disk. (d) The desired organization, structure, or
arrangement of the content of the data product described by the Data Item Description
(DID); related to the shape, size, makeup, style, physical organization, or arrangement
of the data product described in the DID. (e) In print and audiovisual, the distinctive and
recurring treatment, shape, size, and style of a publication‚Äôs page or sections achieved
through stylized composition and typographic make-up (for example, line length, type
face, and size).

Formative Evaluation. An activity that provides information about the effectiveness of
training materials to meet training objectives and the trainees‚Äô acceptance of training
materials as they are being developed. Also see Developmental Testing and
Evaluation.

Front-End Analysis. A process that evaluates requirements for manpower, personnel,
and training (MPT) during the early stages of the military system acquisition cycle. Its
purpose is to determine manpower, personnel, training and safety requirements under
alternative system concepts and designs and to estimate the impact of these MPT
requirements on system effectiveness and life cycle costs.

Functional Grouping. The organization of instruction so that tasks related to the
same procedures or equipment are presented together.

Gantt Chart. A visual representation of project tasks showing the duration of each task
along a timeline.
AFH 36-2235      VOLUME 3       1 NOVEMBER 2002                                         183


Generic Courseware. Courseware that is not specific to one organization and that
appeals to a broader market.

Government-Furnished Equipment (GFE). Equipment that has been selected to be
furnished by the Government to a contractor or Government activity for installation in,
or for use with, or in support of the system/equipment during production, conversion, or
modification. Also see Government Property.

Government-Furnished Information (GFI). Information to be furnished by the
Government to a contractor. Also see Government Property.

Government-Furnished Material (GFM). Documents, equipment, facilities, and
services supplied to a contractor before and during the execution of a contract. Also
see Government Property.

Government-Furnished Property (GFP). Property (real and personal, including
facilities, material, special tooling, special test equipment, and agency-specific
property) in the possession of or directly acquired by the Government and
subsequently made available to the contractor. GFP includes documents, equipment,
facilities, and services supplied to a contractor before and during the execution of a
contract. Also see Government Property.

Government/Industry Data Exchange Program (GIDEP). A cooperative data
interchange among Government and industry participants seeking to reduce or
eliminate expenditures of time and money by making maximum use of existing
knowledge. GIDEP provides a means to exchange certain types of data essential
during the life cycle of systems and equipment.

Government Property. All property (real and personal, including facilities, material,
special tooling, special test equipment, and agency-specific property) owned by or
leased to the Government or acquired by the Government under the terms of the
contract. It includes Contractor-Acquired Property; Government-Furnished
Equipment; Government-Furnished Information; Government-Furnished Material;
and Government-Furnished Property.

Group-Paced Instruction. Instructor-centered training with fixed periods of instruction.
All class members or small groups are instructed on the same task at the same time.

Guaranteed Student. The product of a contracted training system that assures that
graduates achieve specific performance levels according to the approved user tasks
and standards documents.

Guided Discussion Method. A learning experience in which students participate in an
instructor-controlled, interactive process of sharing information and experiences related
to achieving an instructional objective.
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Hardware. (a) The physical components of a system. (b) The physical components
and equipment that make up a computer system (everything except the programs or
software), including peripheral devices.

Higher Levels of Learning. Those levels of learning above the comprehension level
that may be considered as the practical application of concepts and principles to
complex real problems.

Human Factors Engineering. The application of human performance principles,
models, measurements, and techniques to system design. Human performance is an
integral part of system design characteristics that affect the efficiency of operating,
servicing, programming, and repairing the system.

Human Systems Integration. The process of effective integration of human factors
engineering, manpower, personnel, training, health hazards, and safety considerations
into the acquisition of defense systems to improve total system performance and
reduce costs by focusing attention on the capabilities and limitations of humans.
Illustration. The use of graphics, animation, or any kind of visual demonstration within
a lesson.

Implementing Command. The command or agency designated by the Air Force
Acquisition Executive to manage an acquisition program.

Individualized Instruction. (a) Instruction that attends to the individual needs of, and
differences among, students. (b) A method of training in which the subject, content,
presentation rate, and presentation media are tailored to the needs of the individual
student. (c) A lesson design that accommodates diverse ability levels or desires. (d)
Training that allows each student to determine the pace, start time, amount, and kind of
instruction based on individual goals or objectives, entry-level skills, choice of learning
media, and criterion-referenced measures for determining mastery. Also see Self-
Paced Instruction.

Individual Training. Instruction provided to an individual military member, in either a
centralized training organization or an operational unit, which prepares the trainee to
perform specified military tasks.

Individual Training Standards (ITS). The standards used to specify individual
training proficiency requirements (tasks) that support unit mission performance.

Individual Tryout (ITO). A test of the effectiveness of a unit of instruction on individual
students who are representative of the intended target population and revision of these
materials as necessary. Also see Field Test and Field Validation.
AFH 36-2235       VOLUME 3        1 NOVEMBER 2002                                        185


Informal Training. (a) Training accomplished by actions for which structuring
("programming") is not specifically planned beforehand. (b) "On-the-job training" or "on-
board training" by which skills or knowledge are acquired or improved while assigned
productive tasks. (c) Training that takes place in the work environment during the
normal day-to-day contacts between a supervisor and subordinates. (d) Training
accomplished by self-instruction, as contrasted to supervised or instructor-led training.

Initial Design. The first basic concept, usually expressed as a flowchart and treatment,
that deals with a block of information and the manner in which the blocks will interact,
rather than with portions of a lesson or procedure.

Initial Operational Capability (IOC). The first attainment of capability to effectively
employ a weapon, item of equipment, or system of approved specific characteristics,
and which is manned or operated by an adequately trained, equipped, and supported
military unit or force.

Initial Qualification Training. Initial training that qualifies a student to a certain
knowledge and skill level required before the student can take additional, more
advanced training.

In-Process Review (IPR). (a) A joint meeting between the Government and contractor
personnel to review program status; a periodic evaluation or assessment held at a
specific point in the stages of contractual work. (b) A scheduled formative evaluation
conducted during or at completion of the different production sequences to ensure that
the product or development process meets the acquisition requirements.

Institutional Training. Individual training conducted in a school or training center of a
centralized training organization.

Instruction. (a) The delivery of information to enable learning. (b) The process by
which knowledge and skills are transferred to students. Instruction applies to both
training and education.

Instructional Aid Equipment. See Training Aid Equipment.

Instructional Conditions. (a) The instructional atmosphere including environmental,
physical, and psychological factors. (b) The amount of participation which the
instruction requires of the trainee. Instructional conditions may be active (the trainee
produces or practices) or passive (the trainee sits and listens).

Instructional Design. The philosophy, methodology, and approach used to deliver
information. Some interactive courseware aspects include question strategy, level of
interaction, reinforcement, and branching complexity.
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


Instructional Material. All items of material prepared, procured, and used in a course
or program as part of the teaching or general learning process.

Instructional Media. The means used to present information to the trainee.

Instructional Module. A self-contained instructional unit that includes one or more
learning objectives, appropriate learning materials and methods, and associated
criterion-referenced measures.

Instructional Program. A course of study designed and validated within the context of
an approved ISD model that meets a training requirement.

Instructional Requirements. The knowledge, skills, and attitudes that are necessary
to satisfy job performance.

Instructional Setting. The location and physical characteristics of the area in which
instruction takes place. The setting can be in a classroom, laboratory, field, or
workplace. An example is a clean, well-lighted, temperature-controlled classroom
equipped with individual desks, chairs, and video monitors.

Instructional Software. The actual instructional presentation including both content
and technique delivered by a computer-driven system.
Instructional Strategy. The general concept and methodology by which instruction is
to be delivered to the student. Methodologies include tutorial, drill and practice,
simulation, and gaming.

Instructional Support. Learning resources; different kinds of material, number of
instructors, amount of time, and other resources, that will contribute to completion of
the learning process.

Instructional System. An integrated combination of resources (students, instructors,
materials, equipment, and facilities), techniques, and procedures performing effectively
and efficiently the functions required to achieve specified learning objectives.

Instructional System Developer. A person who is knowledgeable of the instructional
system development (ISD) process and is involved in the analysis, design,
development, implementation, and evaluation of instructional systems. Also called
Instructional Designer, Instructional Developer, Curriculum Developer, Curriculum
Development Manager, and other terms.

Instructional System Development (ISD). A deliberate and orderly, but flexible,
process for planning, developing, implementing, and managing instructional systems.
ISD ensures that personnel are taught in a cost-efficient way the skills, knowledge, and
attitudes essential for successful job performance.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                      187


Instructional Technology. (a) The study of instruction and its techniques for the
purpose of enhancing its systematic organization and dependability. (b) A systematic
way of designing, carrying out, and evaluating the total process of learning and
teaching in terms of specific objectives, based on research in human learning and
communication, and employing a combination of human and non-human resources to
bring about more effective instruction.

Instructor. An individual, military or civilian, tasked with teaching.

Integrated Logistic Support (ILS). A disciplined approach to the activities necessary
to: (1) Cause support considerations to be integrated into system and equipment
design; (2) Develop support requirements that are consistently related to design and to
each other; (3) Acquire the required support; and (4) Provide the required support
during the operational phase at minimum cost.

Integrated Manpower, Personnel and Comprehensive Training and Safety
(IMPACTS). The Headquarters US Air Force/Manpower and Organization-sponsored
acquisition management program that implements the specific Human Systems
Integration policy outlined in DODI 5000.2. It impacts a comprehensive management
and technical approach for addressing the human-centered elements of manpower,
personnel, training, safety, health hazards, and human factors engineering in the
acquisition of new or improved systems.

Interactive Courseware (ICW). Computer-controlled training designed to allow the
student to interact with the learning environment through input devices such as
keyboards and light pens. The student‚Äôs decisions and inputs to the computer
determine the level, order, and pace of instructional delivery, and forms of visual and
aural outputs.

Interactive Learning. Instruction characterized by an interchange between the user
and the material. The user learns the instruction through this interchange with the
material.

Interactive Media. (a) Media that involve the viewer as a source of input to determine
the content and duration of a message, permitting individualized program material. (b)
A philosophy of media production designed to take maximum advantage of random
access, computer- or equipment-controlled videotape and videodisc players.

Interactive Training System. An instructional system that requires a student to
interact with it through the learning process.

Interactive Video. Video that uses analog and digital databases to present
instructional material in the ICW environment.
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


Internal Evaluation. The acquisition and analysis of feedback and management data
from within the formal training environment to assess the effectiveness of the
instructional system. Also see Operational Evaluation.

Interpretation. A sub-level of the comprehension level of learning in which students
develop sufficient understanding to see relationships between various aspects of a
communication and are able to perform such activities as inferences, generalizations,
and summations.

Introduction. A major section of a lesson designed to: (1) establish a common ground
between the presenter and students; (2) capture and hold attention; (3) outline the
lesson and relate it to the overall course; (4) point out benefits to the student; and (5)
lead the student into the body of the lesson. The introduction segment usually contains
attention step, motivation step, and overview. It covers the course content, the target
population, the reason that the student is studying the material, and appropriate
motivation to gain the student‚Äôs attention.

Job. The duties, tasks, and task elements performed by an individual. The job is the
basic unit used in carrying out the personnel actions of selection, training,
classification, and assignment.

Job Aid. A checklist, procedural guide, decision table, worksheet, algorithm, or other
device used by a job incumbent to aid in task performance. Job aids reduce the
amount of information that personnel must recall or retain.

Job Analysis. The basic method used to obtain salient facts about a job, involving
observation of workers, conversations with those who know the job, analysis
questionnaires completed by job incumbents, or study of documents involved in
performance of the job.

Job Performance Requirements (JPR). The tasks required of the human component
of a system, the conditions under which these tasks must be performed, and the quality
standards for acceptable performance. JPRs describe what people must do to perform
their jobs.

Job Performance Test. A test used to determine whether or how well an individual
can perform a job. It may include all job performance measures for a job or a subset of
the job performance measures. Also see Performance Test.

Job Task Analysis. A process of examining a specific job to identify all the duties and
tasks that are performed by the job incumbent at a given skill level. Also called Task
Analysis.

Job Task Inventory. (a) The results of information gathering in job analysis. (b) Lists
of duties and tasks, varying in refinement from basic input data to duties and tasks,
AFH 36-2235       VOLUME 3       1 NOVEMBER 2002                                        189


which constitute the job performed by incumbents within an Air Force Specialty Code
(AFSC). Also called Task Inventory.

Joint School or Course. A school or course used by two or more military services that
has a joint faculty and an appointed director (commandant) who is responsible, under
the direction of the Joint Chiefs of Staff, for developing and administering the
curriculum.

Knowledge. Use of the mental processes that enable a person to recall facts, identify
concepts, apply rules or principles, solve problems, and think creatively. Knowledge is
not directly observable. A person manifests knowledge through performing associated
overt activities. Also see Attitude and Skill.

Knowledge-Level Summary. A reiteration of key points of content in a knowledge-
level lesson designed to enhance a student‚Äôs ability to remember facts.

Knowledge of Results. Feedback information provided to the student indicating the
correctness of the response.

Learning. A change in the behavior of the student as a result of stimulus or
experience. The behavior can be physical and overt, or it can be intellectual or
attitudinal.

Learning Analysis. A procedure to identify task sub-elements and the related
skills/knowledge that must be learned before a person can achieve mastery of the task.

Learning Hierarchy. A graphic display of the relationships among tasks in which some
tasks must be mastered before others can be learned.

Learning Objective. A statement of the behavior or performance expected of a trainee
as a result of a learning experience, expressed in terms of the behavior, the conditions
under which it is to be exhibited, and the standards to which it will be performed or
demonstrated. Also called Behavioral Objective or Training Objective. Also see
Objective.

Lesson. (a) A segment of instruction that contains an objective, information (to be
imparted to the student), and an evaluation instrument (test). (b) A segment of
instruction that covers a specific maintenance task, procedure, or idea. (c) That
element of a module that is designed to teach one or more learning objectives. Also
see Course and Module.

Lesson Guide. An organized outline of a single lesson topic taken from the course of
study and serving as a blueprint of what is to be accomplished in class. It is complete
in detail and states all objectives, topics, subtopics, references, training aids, methods,
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


procedures, and other supplemental information as needed. In general, the lesson
guide is the formal lesson plan.

Lesson Plan. An approved plan for instruction that provides specific definition and
direction to the instructor on learning objectives, equipment, instructional media
material requirements, and conduct of training. Lesson plans are the principal
component of curriculum materials in that they sequence the presentation of learning
experiences and program the use of supporting instructional material.

Life Cycle Management. The process of administering a system from the time it is
initially developed until it is terminated, with emphasis on strengthening early decisions
that shape costs and effectiveness.

Logistic Support. Resources required supporting instructional delivery.

Logistic Support Analysis (LSA). The selective application of scientific and
engineering efforts undertaken during the acquisition process, as part of the system
engineering and design process, to assist in complying with supportability and other
integrated logistic support (ILS) objectives. LSA task descriptions and data item
descriptions are prescribed by MIL-STD-1388-1.

Maintenance Trainer. A training device designed to train maintenance personnel on
specification systems or subsystems.

Maintenance Training Simulator. A device, usually computer-controlled, that
simulates operational equipment and allows trainees to practice maintenance tasks or
procedures.

Major Command (MAJCOM). A major subdivision of the Air Force that is assigned a
major part of the Air Force mission. MAJCOMs report directly to Headquarters US Air
Force.

Management. The practice of directing or controlling all aspects of the instructional
system.

Management Information System (MIS). A system that includes training
databases/database networks for the management of training-related data. The
database may encompass an organization‚Äôs administration of personnel training data,
training resource data, and training research data.

Management Materials. Materials that define training requirements and provide an
overall plan for the accomplishment of these requirements.
AFH 36-2235      VOLUME 3       1 NOVEMBER 2002                                           191


Management Plan. A program for the assignment, monitoring, and assessment of the
personnel, materials, and resources dedicated to a specific mission, operation, or
function.

Manning. The specific inventory of people at an activity in terms of numbers, grades,
and occupational groups.

Manpower. The requirements or authorizations needed in an organization to
accomplish a task or service. Also see Personnel.

Master Schedule. A schedule of instruction, prepared by the training activity, to
indicate the period-by-period program for each day and week of the course.

Measurement Process. The operations involved in determining the amount of an
attribute (for example, skill, knowledge, or attribute) possessed by a student.

Media. The delivery vehicle for presenting instructional material or basic
communication stimuli to a student to induce learning. Examples are instructors,
textbooks, slides, and interactive courseware (ICW).

Media Alternative. A substitute means for presenting materials.

Media Analysis. The process of examining media requirements and assembling a
data bank of information to use for selecting appropriate media for use in instruction.

Media Mix. A combination of different media used to present a unit of instruction.

Media Selection. The process of selecting the most effective means of delivering
instruction.

Memorandum of Understanding (MOU). A jointly prepared and authenticated
document between participants in a joint project. Also called Memorandum of
Agreement (MOA).

Method of Instruction. The means, techniques, procedures, and other provisions for
the delivery of instruction. There are many appropriate methods. Included may be
such processes as lecture, recitation, laboratory, examination, study periods,
demonstration, use of training aids, group discussion, role playing, case studies,
programmed instruction, and coach and pupil methods.

Milestone. A major point in the development of a project.

Military Standard (MIL-STD). Documents issued within the Department of Defense in
accordance with the basic policy of the Defense Standardization Program (MIL-STD-
962). MIL-STDs establishes engineering and technical requirements for items,
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


equipment, processes, procedures, practices and methods that have been adopted as
standards. For military training programs, see MIL-HDBK-29612.

Mission Analysis. A process of reviewing mission requirements, developing collective
task statements, and arranging the collective tasks in a hierarchical relationship.

Mission Trainer. A trainer that provides the trainees with a simulated warfare
environment that is specifically mission-oriented to the type of weapon system involved.
The trainer can provide specific weapon system operator modes or a mission mode that
requires tactical decision-making. Does not include pilot or copilot flight dynamics
training.

Mobile Training Set (MTS). A portable set of system training equipment, consisting of
trainers, training aids, operational equipment, bench training sets, support equipment,
technical publications, computer software, standard Air Force material, audiovisual
products, and audiovisual equipment designed primarily for use by Air Education and
Training Command for on-site training of maintenance personnel in field training
programs.

Mobile Training Team (MTT). Any group of personnel and training equipment
gathered together to provide instruction on a subject or in an area of endeavor,
available for movement from place to place in order to provide instruction at the various
locations concerned.

Mockup. A three-dimensional training aid designed to represent operational
equipment. It may be a scaled or cutaway model and may be capable of disassembly
or operational simulation.

Mode of Instruction. A method of scheduling materials presentation. The
instructional mode may be individualized, self-paced, or group.

Module. (a) A stand-alone instructional unit designed to satisfy one or more learning
objectives. (b) A separate component complete within itself which can be taught,
measured, and evaluated for a change or bypassed as a whole; one which is
interchangeable with others, used for assembly into units of differing size, complexity,
or function. (c) An individualized self-instructional package, usually containing all the
necessary materials a student needs to meet some or all of a learning objective/task. A
module consists of one or more lessons. Also see Course and Lesson.

Motivational Device. A design element that arouses and sustains interest or regulates
activity for the purpose of causing the student to perform in a desired way.

Multimedia. The use of more than one medium to convey the content of instruction.
Media available for use may include, but need not be limited to: text, programmed
instruction, audio and video tapes/discs, slides, film, television, and computers.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                        193




Needs Analysis. The systematic, in-depth analysis and verification of training
discrepancies identified by a needs assessment, the results of which are the definition
of performance deficiencies and the isolation of potential solutions; integral to front-end
analysis. This analytical process addresses the specific nature of the deficiency.

Needs Assessment. (a) The systematic process for identifying the causes of
discrepancies between what exists and what is currently required, and for identifying
the causes of potential discrepancies between current and future requirements. (b)
The process in which performance discrepancies are focused upon to determine where
the discrepancies exist (for example, environmental, training, instruction, personnel,
and equipment).

Objective. A statement that specifies precisely what behavior is to be exhibited, the
conditions under which behavior will be accomplished, and the minimum standard of
performance. Objectives describe only the behaviors that directly lead to or specifically
satisfy a job performance requirement. An objective is a statement of instructional
intent. Also see Learning Objective.

Objectivity. (a) A characteristic of evaluation, which requires that measurement in an
educational environment be correct, factual, and free from instructor bias. (b) The
degree to which something is evaluated the same by two or more evaluators acting
independently. (c) In testing, the degree to which a test is scored the same by two or
more scorers acting independently.

Occupational Analysis. Data interpretation regarding an occupational designator (Air
Force Specialty Code) to determine what jobs are performed within the occupation and
which tasks are performed within these jobs. Occupational analysis may be used to
assess the accuracy of classification and training documents.

Office of Primary Responsibility (OPR). The office or person that serves as the
primary point of contact and is responsible for a contract or a project.
On-the-Job Training (OJT). Individual training in designated job skills provided to
individual members when serving in job positions in operational units.

Operating Command. The command primarily operating a system, subsystem, or item
of equipment; generally, an operational command or organization designated by
Headquarters US Air Force to conduct or participate in operations or operational
testing.

Operational Evaluation. The process of internal and external review of system
elements, system requirements, instructional methods, courseware, tests, and process
guide revision as needed to enhance the continued training effectiveness and
efficiency of the training system during full-scale operations. The process begins at the
training system readiness review and continues throughout the life of the training
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


system. It includes Internal Evaluation and External Evaluation. Also see
Evaluation.

Operational Training. Training that exercises previously acquired functional
knowledge and system employment (operational) skills, to enhance proficiency and to
identify deficiencies within a systematic training structure in the operational
environment or in the simulated operational environment such as at a trainer.

Operator Trainer. A trainer on which students learn the methods and procedures
necessary to operate specific equipment (for example, radar trainer, operational flight
trainer).

Operator Training. Instruction in which students are taught the methods, procedures,
and skills necessary to manipulate the controls of specific systems and equipment.

Overview. (a) A description of content, basic structures, learning objectives, and other
fundamentals of the next portion of interactive courseware to be presented. (b) A
segment of a lesson introduction in which the presenter provides a clear and concise
presentation of the objective, the key ideas or main points of the lesson, and an
indication of the teaching method to be employed.

Participating Command. A command or agency designated by the Air Force
Acquisition Executive to advise the program manager and to take an active part in the
development of a weapon system. The supporting command is also a participating
command.

Part-Task Trainer (PTT). Operator trainers that allow selected aspects of a task (fuel
system operation, hydraulic system operation, radar operation, etc.) to be practiced and
a high degree of skill to be developed independently of other elements of the task.

Part-Task Training. Subordinate skills training (operations/procedures) that
resembles portions, or subtasks, and response of the actual system operation. A type
of two-dimensional simulation.

Part-Task Training Device. A device that permits selected aspects of a task to be
practiced independently of other elements of the task. Its purpose is to provide
economical training on certain elements that require special practice but are not
dependent upon the total equipment.

Performance. Part of a criterion objective that describes the observable student
behavior (or the product of that behavior) that is acceptable to the instructor as proof
that learning has occurred.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                        195


Performance Evaluation. A process of data collection and analysis to determine the
success of students on a specific individual or collective task as a result of a training
program.

Performance Evaluation Tools. Competency tests that allow the trainer to profile the
student‚Äôs proficiency and identify weak areas so that training can be efficiently planned
for the areas of most critical need.

Performance Exercise. A proficiency (criterion-referenced) test used to evaluate
mastery of a task as specified by the criterion-referenced objective.

Performance Gap. An operationally significant discrepancy between design
effectiveness and actual effectiveness indicative of system performance
ineffectiveness. A performance gap is indicative of training subsystem, hardware
subsystem, trainee characteristics, trainer characteristics, and training environment
problems that must be identified and corrected.

Performance Objective. A precise statement of the performance expected of a trainee
as the result of instruction, expressed in terms of the standards to which it will be
performed or demonstrated. Also see Objective.

Performance-Oriented Training. (a) Training that emphasizes the skills and
knowledge needed to perform a task or job through individual practice and constant
evaluation. (b) The conduct of individual or collective training in which one or more
tasks are performed under specified conditions to a specified standard. It differs from
traditional practical exercises in that performance is measured against a specific
standard. Also see Criterion-Referenced Instruction.

Performance Requirements. The separate acts that are required to satisfactorily
complete an individual‚Äôs performance on the job. These include the act (behavior), the
conditions under which the behavior is performed, and the standard of performance
required by the incumbent.

Performance Test. (a) A sample work situation that tests how well the student has
mastered the psychomotor and cognitive skills required for job performance. (b) A test
in which the performance of a task is the criterion of skill mastery. Such a test is
prepared in terms of the specific task to be performed, the conditions under which it will
be performed, and the absolute standards for acceptable performance. (c) A test that
measures the skills and knowledge needed to perform the terminal learning objectives
against specific standards. For some circumstances, this could be a written test if
designed as a job sample for personnel whose responsibilities involve only paper
procedures. Also see Proficiency Test.
                                            AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


Personnel. The individuals who accomplish specific tasks. Personnel connote
individuals, whereas manpower connotes requirements or authorizations. Also see
Manpower.

Phased Approach. A process whereby training capability of a training system and its
components are incrementally fielded based on user training need and required assets
available, ready for training and initial operational capability dates, maturity of the
weapon system and associated tactics, and capability to provide logistical support.

Physical Fidelity. The degree of structural or dynamic correspondence of a training
device to the operational system/equipment it represents.

Pilot Course. A full-length course conducted in a target environment (facilities,
instructors and trainees) using the curriculum and supporting training material prepared
for that course. It has as its purpose the "shaking down" or "validating" of the
curriculum and materials in a classroom situation to determine their effectiveness in
attaining the approved learning objectives or training goals.

Pipeline. The total time involved in training personnel once they are designated as
trainees. It includes time traveling to the training activity, time awaiting instruction, time
of actual training, time from termination of training until reporting to the ultimate duty
station, and possibly more than one training activity.

Plan of Instruction (POI). A qualitative course control document designed for use
primarily within a school for course planning, organization, and operation. Generally,
criterion objectives, duration of instruction, support materials, and guidance factors are
listed for every block of instruction within a course. Also called Syllabus.

Plans. Documents developed and revised throughout the ISD process detailing
requirements, operating goals, and procedures.

Posttest. A test administered to a student upon completion of a course or unit of
instruction to measure learning achieved and to assess whether a student has
mastered the objectives of the course or unit of instruction.

Prerequisite Training. The training that personnel must have previously completed
successfully in order to be qualified for entry into training for which they are now being
considered.

Presentation Media. Different media used to convey or communicate information to
individuals engaged in learning. These media may include printed materials,
audiovisual devices, hardware simulators, or stimulators.

Pretest. A test administered to a student prior to entry into a course or unit of
instruction to determine the technical skills (entering behaviors) the student already
AFH 36-2235       VOLUME 3       1 NOVEMBER 2002                                         197


possesses in a given subject. Pretests are often used to identify portions of instruction
that the student can bypass.

Proficiency. A specific standard of performance in which the trainee demonstrates a
predetermined skill ability or expertise.

Proficiency Test. A test designed to measure a trainee‚Äôs capabilities in terms of the
job. It measures both psychomotor and cognitive skills. A performance test is
sometimes understood to mean a skill demonstration, while a proficiency test is
understood to be a comprehensive procedure used to examine the trainee‚Äôs capability
to do what the job requires. Also see Performance Test.

Proficiency Training. (a) Training conducted to improve or maintain the capability of
individuals and teams to perform in a specified manner. (b) Training to develop and
maintain a given level of skill in the individual or team performance of a particular task.

Program Evaluation Review Technique (PERT). (a) A visual representation of the
tasks of a project, showing the relationship between the tasks and defining the critical
path. (b) A planning technique that arranges events and their duration into a flow
graph to examine the entire program and to aid in decision making on sequencing
priorities, total time for plan completion, preparation (lead) time for specific events, and
other determinations.

Program Management Responsibility Transfer (PMRT). The transfer of program
management responsibility for a system (by series) or equipment (by designation) from
the implementing command to the supporting command.

Programmed Instruction. A student-centered method of instruction that presents the
information in planned steps or increments, with an appropriate response immediately
follows each step. The student is guided step-by-step to the successful completion of
the assigned task or training exercise.

Project Officer (PO). The operating command coordinator at a site assigned to ensure
that AF activities and the contractor have a central point of contact for contract
administration, logistic support, and security support as determined from the contract.
PO duties can be assigned to personnel within the organization or to the Quality
Assurance Representative.

Quality Air Force (QAF). A management philosophy and a methodology that work
together to produce continuous process improvements. QAF implements Total Quality
Management (TQM) in the Air Force. Also see Total Quality Management.

Quality Assurance (QA). Actions taken by the Government to assure that services
meet the requirements in the Statement of Work (SOW).
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


Quality Assurance Representative (QAR). The person responsible for checking and
evaluating contractor performance.

Quality Improvement (QI). The organized creation of beneficial change;
improvements made in products, procedures, learning, etc.

Ready for Training (RFT). The dates on which sufficient equipment, training
capabilities, personnel, and logistics elements are available to support full operational
training.

Ready for Use (RFU). The date on which the training system can be used for
productive training.

Reliability. (a) A characteristic of evaluation which requires that testing instruments
yield consistent results. (b) The degree to which a test instrument can be expected to
yield the same result upon repeated administration to the same population. (c) The
capability of a device, equipment, or system to operate effectively for a period of time
without a failure or breakdown.

Required Assets Available (RAA). The date agreed to by the operating command
and Headquarters Air Force Materiel Command when sufficient equipment, personnel,
and logistics elements will be available to the operational command to begin a trial
period for equipment operation and support capability before initial operational
capability. Logistics elements include approved operational support equipment, critical
spares, verified technical manuals, and training programs and courses.

Required Delivery Date (RDD). The date on which items are needed.

Resident Training Course. A course conducted at a training location (such as a
training center) where the trainee is a full-time student, as compared to training
conducted at the trainee‚Äôs duty location.

Resource Requirements List. An overall list that identifies the texts, references,
equipment, films, graphics, and instructional media materials required to support the
curriculum.

Sample of Behavior. Student behavior which the instructor will accept as evidence of
learning. The specific behavior in each sample is the variable; the taxonomy level
(cognitive, affective, or psychomotor) of the set of samples is the constant and serves
as the common denominator of each sample. Also called Behavior Indicator.

Self-Paced Instruction. Instruction that permits progress at the individual student‚Äôs
own desired rate of learning. Also see Individualized Instruction.

Self-Study. Individual study on the job site or duty location.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                       199




Self-Study Workbook/Guide. (a) A document containing a series of lessons arranged
in discrete steps with self-test questions that allow the instructor to monitor the
students‚Äô progress. It is used to guide the student through a controlled path of study
and specific job tasks with a minimum amount of supervision. (b) An instructional
document that provides the student study material in support of objectives. This
document contains the objectives, sub-objectives, subject matter content, reference to
adjunct reading or study material, review exercises with feedback, and directions to
interact with training media including an instructor.

Simulation. A technique whereby job environment phenomena are mimicked, in an
often low-fidelity situation, in which costs may be reduced, potential dangers
eliminated, and time compressed. The simulation may focus on a small subset of the
features of the actual job environment.

Simulator. (a) Hardware and software designed or modified exclusively for training
purposes involving simulation or stimulation in its construction or operation to
demonstrate or illustrate a concept or simulate an operational circumstance or
environment. Training simulators and devices are considered part of the overall
training system that may or may not be identified as part of the parent defense system.
(b) Training equipment that imitates operational equipment both physically and
functionally, such as a cockpit procedures trainer, operational flight trainer, or weapon
systems trainer. Also see Training Device.

Simulator Certification (SIMCERT). The process of ensuring through validation of
hardware and software baselines that a training system and its components provide the
capability to train personnel to do specific tasks. The process ensures that the device
continues to perform to the delivered specifications, performance criteria, and
configuration levels. It will also set up an audit trail regarding specification and
baseline data for compliance and subsequent contract solicitation or device
modification.

Skill. The ability to perform a job-related activity that contributes to the effective
performance of a task. Skills involve physical or manipulative activities that often
require knowledge for their execution. All skills are actions having specific
requirements for speed, accuracy, or coordination. Also see Attitude and Knowledge.

Skill Level. (a) A list of proficiency requirements for performance of a specific job. (b)
The level of proficiency at which an individual qualifies in that occupational specialty.

Small-Group Tryout. Tryout of a training course on a representative sample of the
target population to gather data on the effectiveness of instruction in regard to error
rates, criterion test performance, and time to complete the course. Also see Field Test
and Field Validation.
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


Software. (a) The programs and routines that tell the computer and its peripherals
what to do; any system of instructions that direct computer operation; a category of
computer components restricted to instructions to the equipment (hardware); the
programs for the computer. Typically, software can be divided into operating systems,
computer languages including authoring tools, and application programs. (b) The
media that store software, such as floppy disks, flowcharts, manuals, and other
computer programming documentation. (c) Non-equipment training material, such as
pamphlets, handouts, schematics, charts, audiovisual products, and guide sheets.

Source Data Integrity Program. A program designed to ensure that timely, quality
military standard baseline source data is provided by the defense system developer to
the training system or component developer throughout the life cycle of the system.

Specialty Training Standard (STS). A standard that identifies the most common tasks
of an enlisted Air Force Specialty (AFS) that requires training.

Specification. A document prepared specifically to support acquisition, which clearly
and accurately describes essential technical requirements for purchasing material.
Procedures necessary to determine that the requirements for the materials covered by
the specification have been met are also included. Military specifications are
documents issued within the Department of Defense in accordance with the basic
policy of the Defense Standardization Program. A military specification covers
systems, subsystems, components, items, materials, or products that are intrinsically
military in character or are used in, or in support of, weapons systems and involve an
essential system function of interface (see MIL-STD-961).

Standard. A document that establishes engineering and technical requirements for
items, equipment, processes, procedures, practices, and methods that have been
adopted as standard. Standards may also establish requirements for selection,
application, and design criteria for material. Military standards are documents issued
within the Department of Defense in accordance with the basic policy of the Defense
Standardization Program (see MIL-STD-962).

Standard of Performance. A statement that establishes criteria for how well a task or
learning objective must be performed. The standard specifies how well, completely or
accurately a process must be performed or a product produced. The standard reflects
task requirements on the job or learning requirements in the classroom. A product
standard is expressed in terms of accuracy, tolerance, completeness, format, clarity,
errors, or quantity. A process standard is expressed in terms of sequence,
completeness, accuracy, or speed. Both product and process must be observable and
measurable. Also see Standards Statement.
Standards Statement. A part of a criterion objective that describes the qualitative and
quantitative criteria against which student performance or the product of that
performance will be measured to determine successful learning. Also see Standard of
Performance.
AFH 36-2235       VOLUME 3       1 NOVEMBER 2002                                        201




Statement of Work (SOW). A document that establishes and defines all non-
specification requirements for contractor efforts, either directly or with the use of
specific cited documents.

Stimulation. The process whereby operational equipment can be artificially induced to
replicate the operational environment to exploit additional training capabilities of the
weapon system.

Storyboard. A layout with detailed graphic description of a single frame or series of
frames, arranged sequentially, that describes the action and content of a visual medium
of instruction in interactive courseware (ICW). A storyboard specifies all details such
as graphics, text, visuals, video, audio, and special effects. It is a graphic depiction
that visually shows the courseware presentation.

Strategy. The logical arrangement of course content within a pattern or organization
that will likely cause the most learning to occur. It includes the purpose, target
audience, content outline, level of interaction, feedback, testing, audiovisual options,
and other data.

Student. The individual being trained, the individual learning from the course, or an
individual who has been placed in a learning situation in order to acquire knowledge
and skills required for accomplishment of specific tasks. Also called Trainee.

Student Guide. (a) A generic term for the various printed materials developed for
student use. (b) A publication that provides each student with the supplementary
material (in addition to technical manuals) judged to be required for successful
completion of a course of study.

Subject Matter Expert (SME). (a) An individual who has thorough knowledge of a job,
duties/tasks, or a particular topic, which qualifies him/her to assist in the training
development process (for example, to consult, review, analyze, advise, or critique). (b)
A person who has high-level knowledge and skill in the performance of a job. Also see
Expert Opinion.

Summary. A segment of a lesson conclusion during which the presenter reiterates key
points of lesson content (knowledge level) or reviews and expands on key material and
develops relationships that lead to generalizations (comprehension level).

Summative Evaluation. The overall assessment of a program at the completion of the
developmental process. It is designed and used after the instructional system has
become operational. Also see Evaluation.
Support Equipment. All equipment required to perform the support function except
that which is an integral part of mission equipment. Does not include any equipment
required to perform mission operations functions.
                                         AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Supporting Command. The command (usually Headquarters Air Force Materiel
Command) responsible for providing logistics support for a system and assuming
program management responsibility from the implementing command.

Supportive Relationship. In instructional systems development, skills and knowledge
in one learning objective which have some relationship to those in another learning
objective. The learning involved in mastery of one learning objective transfers to the
other, making learning involved in the mastery of the other easier.

Surveillance. A process that provides ongoing evaluation of training or training
materials to ensure continued effectiveness and currency of content to meet the
training requirements as dictated by the operational systems, support systems, mission,
and threats.

Syllabus. See Plan of Instruction.

System. A composite of skilled people and equipment (hardware and software) that
provides an operational capability to perform a stated mission.

System Engineering Management Plan (SEMP). Typically the most important plan in
a training acquisition. The SEMP covers the entire system engineering process and
includes integration of internal processes and interfaces with external processes.

System Training Plan (STP). The specific document that includes program
information and data concerning the system or equipment program, event, or situation
that originated the training requirement, and describes the training required and the
training programs to satisfy the requirement. The STP is designed to provide for
planning and implementation of training and to ensure that all resources and supporting
actions required for establishment and support are considered.

System Engineering Process. A logical sequence of activities and decisions
transforming an operational need into a description of system performance parameters
and a preferred system configuration.

Tailoring of Data Requirements. The deletion of data requirements, from an
approved Data Item Description (DID) or source document, that are unnecessary to
meet the needs of a specific contract.

Task. A unit of work activity or operation which forms a significant part of a duty. A
task usually has clear beginning and ending points and directly observable or
otherwise measurable processes, frequently but not always resulting in a product that
can be evaluated for quantity, quality, accuracy, or fitness in the work environment. A
task is performed for its own sake; that is, it is not dependent upon other tasks,
although it may fall in a sequence with other tasks in a duty or job array.
AFH 36-2235      VOLUME 3        1 NOVEMBER 2002                                        203




Task Analysis. See Job Task Analysis and Training Task Analysis.

Task Description. A verbal description in column, outline, decision table, or timeline
format that describes the required job behavior at the highest level of generality. It is
intended to provide an overview of the total performance.

Task Description Worksheet. A tool used to document specific task factors including
training factors, stimuli, subtasks, steps and activities, standards of performance, and
job aids.

Task Fidelity. The degree of correspondence of cues and responses accompanying
task performance on a training device to those characteristics of analogous
performance on the operational system/equipment.

Task Inventory. See Job Task Inventory.

Task Statement. A written description of task performance that contains an action
verb and an object, and must express the conditions under which the task is performed
and the standard that the performance must meet.

Taxonomy. A system for categorizing things in a hierarchical order.

Taxonomy of Educational Objectives. A systematic classification scheme for sorting
learning outcomes into three broad categories (cognitive, affective, and psychomotor)
and rank-ordering these outcomes in a developmental hierarchy from least complex to
most complex.

Technical Data. Recorded information, regardless of forms or characteristics, of a
scientific or technical nature. It may, for example, document research, experimental,
developmental, or engineering work. It may be used to define a design or process or to
acquire, support, maintain or operate material. The data may be graphic or pictorial
delineations in media such as drawings or photographs, text in specifications, related
performance or design-type documents, or computer printouts. For purposes of this
handbook, technical data includes research and engineering data, engineering
drawings and associated lists, specifications, standards, process sheets, technical
reports, catalog item identifications and related information, documentation related to
computer software, and computer-generated databases. Technical data does not
include computer software or financial, administrative, cost and pricing, and
management data, or other information incidental to contract administration.

Technical Manual (TM). A publication that contains instructions for the installation,
operation, maintenance, training, and support of a weapon system, weapons system
component, and support equipment. TM information may be presented in any form or
characteristic, including but not limited to hard printed copy, audio and visual displays,
                                           AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


magnetic tape, discs, and other electronic devices. TMs normally include operational
and maintenance instructions, parts list or part breakdowns, and related technical
information or procedures exclusive of administrative procedures.

Technical Training. Training in specific skills and knowledge essential to performance
of those tasks and duties related to a technical specialty.

Terminal Behavior. The output performance for a system; graduate performance.

Terminal Objective. A learning objective describing what is expected of the trainee
upon completion of a lesson, topic, major portion of a course, or course completion.
Also called Terminal Learning Objective.

Test. Any device or technique used to measure the performance, skill level, and
knowledge of an individual.

Test Fidelity. The degree to which the test resembles the actual task performed. The
closer the resemblance, the higher the fidelity of the test.

Topic. The basic organizational unit of instruction covering one or more closely
related learning objectives.

Topical Outline. An outline of the topics to be included in the instructor guide. It
provides course learning objectives, a listing of part, section, and topic titles, and
statements of rationale to explain or justify the training. It is used by the curriculum
designer to develop the instructor guides.

Total Contract Training (TCT). A training concept that includes contract support for a
contractor- operated training system. It includes instructors, curriculum, courseware,
facilities, trainers, aircraft, spares, support equipment, and other support elements of
contractor logistics support. The purpose of a TCT is to produce a trained student.

Total Quality Management (TQM). The philosophy and a set of guiding principles
that represent the foundation of a continuously improving organization. TQM is the
application of quantitative methods and human resources to improve the material and
services supplied to an organization, all the processes within an organization, and the
degree to which the needs of the customer are met, now and in the future. TQM
integrates fundamental techniques, existing improvement efforts, and technical tools
under a disciplined approach focused on continuous improvement. TQM is
implemented in the Air Force by the Quality Air Force (QAF) program. Also see
Quality Air Force.

Trainee. See Student.
AFH 36-2235       VOLUME 3       1 NOVEMBER 2002                                           205


Training. Instruction and applied exercises presented in a structured or planned
manner, through one or more media, for the acquisition and retention of skills,
knowledge, and attitudes required to meet job performance requirements. It includes
Collective Training; Individual Training; Institutional Training; On-The-Job
Training; and Unit Training.

Training Agency. An office, command, or headquarters exercising command of and
providing support to some major portion of a formalized training effort.

Training Aid. Any item that is developed and/or procured with the primary intent that it
will assist in training and the process of learning.

Training Aid Equipment. Logistic support equipment that is used to display training
aids but which is not itself the subject of instruction. It includes items such as motion
picture projectors, slide projectors, tape recorders and playback units, sound film
readers, record players, sound/slide projectors, overhead projectors, and opaque
projectors. It also includes secondary items such as easels and projector stands. Also
called Instructional Aid Equipment.

Training Concept. Statement of how the required training is to be accomplished in
terms of type of training, presentation environment, presentation techniques,
presentation media, pipeline, location, and other considerations.

Training Device. Hardware and software designed or modified exclusively for training
purposes involving simulation or stimulation in its construction or operation to
demonstrate or illustrate a concept or simulate an operational circumstance or
environment. Also see Simulator.

Training Effectiveness. (a) The training benefit gained in terms of operational
readiness. (b) The thoroughness with which training objectives have been achieved,
regardless of training efficiency. Also see Training Efficiency.

Training Efficiency. (a) The extent to which training resources (including time) are
used economically while achieving training effectiveness. (b) Resource investments
required to achieve specific training objectives or requirements. Resources may
include time, instructor assets, training device assets, equipment assets, and costs.
Training efficiency is directly related to training effectiveness. There can be no
efficiency if there is no effectiveness, because effectiveness implies a benefit from the
resources invested. Also see Training Effectiveness.

Training Effort. (a) The sum of actions taken to establish and operate training
activities, provide training programs, or otherwise contribute positively to the overall
posture of training. (b) The man-hours and dollars expended to provide for trained
personnel, training services, and instructional materials.
                                           AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


Training Exercise. A "practice problem" conducted in the field, for example, a
simulation of the real situation (operational situation), conducted in an environment
approximating the significant features of the real (operational) environment. A series of
training scenarios whose purpose is to increase the level of expertise within a particular
area.

Training Facility. A permanent or semi-permanent military real property or contractor
property used for the purposes of conducting training.

Training Fidelity. The extent to which cue and response capabilities in training allow
for the learning and practice of specific tasks so that what is learned will enhance
performance of the tasks in the operational environment.

Training Management System (TMS). A set of operational tools to help training
system managers in controlling and enhancing the evolution of a training system during
the life cycle. Modules consist of administration, curriculum management, resource
management, schedule management, performance measurement, configuration
management, logistics management, and reports.

Training Materials. Plans, control documents, and instructional materials.

Training Need Date (TND). The specified date or milestone (from the requirements
documentation as amended) when the training system should be ready for training.

Training Objective. See Learning Objective.

Training Plan. A document that includes program information and data concerning the
system or equipment program, event, or situation that originated the training
requirement, and describes the training required and the training program to satisfy the
requirement. Training plans are designed to provide for planning and implementation
of training and to ensure that all resources and supporting actions required for
establishment and support are considered.

Training Planning Team (TPT). An action group composed of representatives from all
pertinent functional areas, disciplines, and interests involved in the life cycle design,
development, acquisition, support, modification, funding, and management of a specific
defense training system. The TPT uses the system-training plan to ensure that training
considerations are adequately addressed in the defense system acquisition and
modification processes.

Training Program. An assembly or series of courses or other requirements that have
been organized to fulfill a broad overall training objective.

Training Readiness. (a) The quality of being ready to undertake the scheduled
training (that is, ready to be instructed and to benefit from the instruction). (b) The
AFH 36-2235       VOLUME 3        1 NOVEMBER 2002                                         207


quality of being up to date (that is, able to provide training on the very latest model,
device, version, technique, information, and other essentials). (c) The capability to train
(that is, having the means to train in a specified content area).

Training Requirement. The skills and knowledge that are needed to satisfy the job
performance requirements and that are not already in the students‚Äô incoming repertoire.

Training Requirements Analysis. A determination of the requirements to resolve a
training deficiency.

Training Resource Requirements. The training staff and student authorizations,
training equipment and devices, test equipment and spare parts, training services and
materials, construction for (or modification of) training facilities, technical services, and
other resources necessary to conduct required training.

Training Resources. The manpower, equipment, material, facilities, funds, and other
resources required for the conduct and support of training.

Training Schedule. The planned use of instructors, students, facilities and equipment
within a school.

Training Site. The geographic location at which a course or training is conducted.

Training Site Selection. In training analysis and design, the decision regarding where
a task should be trained (that is, resident or institution versus unit or job site).

Training Staff. The administrators and instructors assigned to a training activity.

Training Standard. (a) A quantitative or qualitative measure for the determination of
a level of competence or readiness. (b) A standardized procedure or exercise.

Training Support. The providing of resources, such as authorizations, personnel,
funds, facilities, hardware, course materials, and services, for use of the training
activity.

Training System. A systematically developed curriculum including, but not necessarily
limited to, courseware, classroom aids, training simulators and devices, operational
equipment, embedded training capability, and personnel to operate, maintain, or
employ a system. The training system includes all necessary elements of logistic
support.

Training System Readiness Review (TSRR). The meeting between the contractor
and the Air Force to support an Air Force decision on the suitability of the training
system based on the results of summative evaluation.
                                          AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


Training System Requirements Analysis (TSRA). A systematic approach to front-
end analysis of a defense system based on an integrated instructional system
development or system engineering process that develops data items to document the
training and preliminary system requirements.

Training System Support Center (TSSC). A consolidated function that contains the
personnel, equipment, facilities, tools, and data necessary to provide life cycle
hardware and software support for a training system.

Training Task. A task selected for training.

Training Task Analysis. The process of examining each unique unit of work from job
task analysis to derive descriptive information (for example: procedural steps,
elements, task conditions, standards, and other information) used in the design,
development and testing of training products. Also called Task Analysis.

Training Task List (TTL). Documentation of total training tasks developed for a
defense system and its respective mission. It includes the entire spectrum of tasks in
each functional area (operations, maintenance and support) requiring training. The
TTL provides the training task baseline for all acquisition, modification, support,
management and funding actions through comparison with predecessor or future
weapon systems.

Tutorial. (a) An instructional program that presents new information to the trainee in
an efficient manner and provides practice exercises based on that information. (b) A
lesson design used to teach an entire concept. (c) Interactive instruction that asks
questions based on the information presented, requests trainee responses, and
evaluates trainee responses. Tutorials are self-paced, accommodate a variety of
users, and generally involve some questioning, branching, and options for review.

Unit Training. Individual or collective training conducted by an operational unit.

Upgrade Training. Training administered for the purpose of upgrading skill level.

Validation. The process of developmental testing, field testing, and revision of the
instruction to be certain the instructional intent is achieved. The curriculum materials
and instructional media materials are reviewed for instructional accuracy and
adequacy, suitability for presentation, and effectiveness in providing for the trainees‚Äô
accomplishment of the learning objectives.

Validity. The degree to which a criterion test measures what it was designed to
measure.

Verification. (a) The process by which previously validated curriculum materials and
instructional media materials are proved to be adequate in the actual training
AFH 36-2235      VOLUME 3       1 NOVEMBER 2002                                        209


environment. (b) A review of the tape, videodisc, and software to ensure that the
content is correct. Verification is usually accomplished during the pilot course. The
materials are revised as necessary as a result of the verification. Also see Validation.
Videodisc. A medium of audiovisual information storage for playback on a television
monitor.

Videotape. A magnetic tape that can record and play back audio (sound) and video
(pictures).

Weapon System. A combination of one or more weapons with all related equipment,
materials, services, personnel, training, and means of delivery and deployment (if
applicable) required for self-sufficiency. For purposes of this handbook, a weapon
system is that portion of the system that conducts the mission.

Weapon System Trainer. A device that provides an artificial training/tactics
environment in which operators learn, develop, improve and integrate mission skills
associated with their crew position in a specific defense system.

Weapon System Training. See Defense System Training.

Work Sample. A sample problem representative of the job as a whole, chosen and
adapted for the purpose of testing performance on important operations of the job as
nearly under normal conditions as possible apart from an actual tryout on the job.
Performance on a work sample is frequently used as a criterion against which
prediction devices in evaluation are validated.

Workstation. A physical location containing equipment that allows a user to develop
or execute interactive courseware lessons. A display console with input devices.
                                    AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




                                 Attachment 2

      Cross-Walk Mapping of MIL-HDBK-29612 Tasks to AFH 36-2235,
                               Volume 3

Introduction      The chart on page 175 illustrates relationships between the
                  major tasks of MIL-HDBK-29612 and the various activity blocks
                  of AFH 36-2235, Volume 3. Volume 3 activity blocks are
                  numbered across the top of the chart; MIL-HDBK-29612 tasks
                  are numbered down the left side. Intersecting squares are
                  shaded where one or more MIL-HDBK-29612 sub-tasks
                  reasonably map to one or more of the Volume 3 sub-tasks.

                  Over 80% of all MIL-HDBK-29612 sub-tasks correlate with sub-
                  tasks of the Volume 3 activity blocks. However, 40% of the
                  Volume 3 sub-tasks have no MIL-HDBK-29612 coverage at all.

                  The chart also illustrates that the Volume 3 process groups
                  common sub-tasks differently than the MIL-HDBK-29612
                  process. While some Volume 3 activities map fairly cleanly to
                  MIL-HDBK-29612 tasks on a one-to-one basis (Activity Block 23
                  to Task 301, for example), other Activity Blocks map to as many
                  as seven major tasks in MIL-HDBK-29612.
AFH 36-2235         VOLUME 3      1 NOVEMBER 2002                                      211



Volume 3 activity      There are 33 Activity Blocks that are mapped against MIL-HDBK-
blocks                 29612. These are listed below. The numbers in brackets [ ]
                       refer to the page number in Volume 3 where you may find
                       information on the activity.

                       1.    Evaluate Constraints and Opportunities [17]
                       2.    Acquisition Planning [49]
                       3.    RFP Development [54]
                       4.    Proposal Writing [57]
                       5.    Source Selection [59]
                       6.    Training System Planning [61]
                       7.    Mission and Task Analysis [73]
                       8.    Training Requirements Analysis [76]
                       9.    Objectives Analysis [79]
                       10.   Media Analysis [81]
                       11.   Cost Estimation (Analysis) [84]
                       12.   Training System Basis Analysis [87]
                       13.   Develop Preliminary Syllabus [90]
                       14.   Start of Development [94]

                                                                    Continued on next page
                                        AFH 36-2235 VOLUME 3     1 NOVEMBER 2002



Volume 3 activity    15. Guidance Conferences [95]
blocks (Continued)   16. Write Detailed System-Level Development Plans [96]
                     17. Courseware Planning Leading to System Requirements
                         Review [100]
                     18. Development Activities Leading to System Design Review
                         [104]
                     19. Development Activities Leading to Preliminary Design
                         Review [107]
                     20. Development Activities Leading to Critical Design Review
                         [110]
                     21. Lesson Outlines; Flow Diagrams [114]
                     22. Lesson Strategy Development (Lesson Plans) [116]
                     23. Storyboard [118]
                     24. Code; Program; Write [119]
                     25. Lesson Tests; Individual Tryouts [120]
                     26. Course-Level Integration (Tests) [123]
                     27. Small-Group Tryouts [124]
                     28. Iterative Remedy and Retest, Functional Configuration Audit
                         (FCA) and Course Readiness Review (CRR) [126]
                     29. Site Training Readiness Review (STRR) [130]
                     30. Full-Class Tryouts (Large Group) [136]
                     31. Training System Readiness Review (TSRR) [141]
                     32. Mature System Performance Review [137]
                     33. Ongoing, Life Cycle Evaluation and Update [144]


MIL-HDBK-29612       There are hundreds of tasks listed in MIL-HDBK-29612. For
tasks                brevity of this handbook, they are not listed on the following
                     chart; however, their overall task numbers are listed. You must
                     review MIL-HDBK-29612 to understand the applicable tasks and
                     sub-tasks. The chart on the following page will make it easier.
AFH 36-2235    VOLUME 3            1 NOVEMBER 2002                                                                     213



                                                  CROSS-WALK MAPPING
                                                    AFP 50-68 Vol. 3 Activity Block
               1   2   3   4   5   6   7   8   9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33

         101
         102
         103
         104
 1
         105
 3       106
 7       107

 9       201
         202
 D       203
         204
         205
 T       206
 A       207
 S       208
         209
 K       210
 S       211

         301
         302
         303
         304

         401
         402
                                   AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                             Attachment 3

                           Lessons Learned

Introduction   As you progress through the ISD process in acquiring training
               with the acquisition of defense systems, you will ask yourself:
               "How can I prevent a mistake?" or "Just what do I look for?" or
               "Do I really have to coordinate this change?" The following
               lessons learned examples were extracted from actual successes
               and not-so-successful experiences in past Air Force programs.
               They are provided not only for your information, but also to serve
               as an introduction to the Air Force Materiel Command Lessons
               Learned Program. This "Program" is a database of information
               located at the Air Force Materiel Command (AFMC)
               Headquarters at Wright-Patterson AFB, Ohio. This floppy disk
               and hard copy data bank can pay immeasurable dividends if
               used properly. Call AFMC at DSN 785-3454, Commercial
               513/255-3454, or write:

                              AFMC Lessons Learned Program
                                         ASC/CYML
                                  263rd Street, Building 17
                           Wright-Patterson AFB, OH 45433-6503


Examples       The following are examples of lessons learned:
                  The F-100 jet engine (used on the F-15 and F-16) was not
                  available for training until eight years after deployment. As a
                  result, efficient and effective maintenance was not available.
                  (Source: AFSC/HSD.)

                  Operational E-3As were used as trainers because no trainers
                  were initially purchased with the weapon system. In addition,
                  built-in test (BIT) was eliminated without evaluating its impact
                  on training. These two decisions resulted in costly corrective
                  actions. In the interim, the Air Force paid $3 million a year for
                  contract training. Had they bought the trainer on time they
                  could have saved $9 million. (Source: Tech Report ADB099-
                  970. April 1981, Logistics Mgmt. Inst.; also MANPRINT
                  Bulletin Sep/Oct 88.)

                  The F100-AW-220-equipped F-15 aircraft at Eglin AFB in
                  August 1988 were a success story. They had 100%
                  serviceable spares, no holes, and plenty of combat-ready
AFH 36-2235   VOLUME 3     1 NOVEMBER 2001                                      215




Examples
(Continued)         spare modules. Activating -220-equipped F-15 aircraft in July
                    1986, the 33rd TFW received their 59th aircraft in May 1988.

                    Throughout the 36,000-plus hours flown since that time,
                    reliability and maintainability parameters remained at
                    unprecedented high levels. The engine operated so well that
                    maintaining skill proficiency levels became a problem similar
                    to the "Maytag repairman" story. Since the engine seldom
                    broke, there was little chance to work on it. Much of this
                    success story was attributed to the overall strategy of strong
                    user involvement during ASD field visits to finalize the
                    technical data requirements and maintenance concept to
                    starting production engine deliveries. (Source: AFSC/ASD-
                    AL, AL Staff Digest, Aug 88.)

                    The GLCM program had a number of MPT-related problems:
                       A training system designed for 340 students per year that
                       had to train 1400 troops per year
                       Inadequate field survival training
                       Other training-related problems
                       (Source: AFSC/ASD Paper, Nov 88.)

                    Changing scenarios for the A-10 deployment location from
                    one base to two bases required an additional 165 personnel
                    slots. This requirement, however, was not coordinated early
                    enough to have trained people available for the first two
                    overseas deployment exercises. (Source: MANPRINT
                    Bulletin Sep/Oct 88.)
                                     AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                Attachment 4

        374 Training Development Squadron 15-Step ISD Process

Introduction     Before you start, it‚Äôs important to understand the structure of the
                 Air Force Specialty Code (AFSC). Each AFSC is considered to
                 be a job. Each job is completed by performing related duties;
                 each duty is completed by performing a given number of tasks;
                 each task is completed by performing a sequence of activities;
                 and each activity is completed by applying skill and knowledge
                 behaviors. By approaching acquisition training development in
                 this manner and using the right analysis parameters, accurate
                 and cost-effective training/TTM can be determined. Also note
                 that results of analysis performed using this handbook may be
                 documented by either completing forms you develop or using
                 forms developed by the 374 TDS and provided at the end of this
                 attachment. While these forms are not mandatory, they are
                 provided because they have been successfully used for these
                 procedures. You may also document data by using the ISD
                 Automation software (IBM-compatible) titled "ISDA" available
                 from the 374 TDS (for address, see page 259).

                                              WARNING

                          If working with classified data, the media used to
                        document the data become classified. Always check
                         with the Program Security Officer for instructions
                             concerning storage and disposition of media.
AFH 36-2235    VOLUME 3      1 NOVEMBER 2001                                          217


                              STEP 1
                  IDENTIFY SYSTEM REQUIREMENTS


Substep 1         Identify Duties


Purpose           The purpose of this substep is to identify and compile a list of
                  duty statements associated with an AFSC as related to the
                  weapon system. A list of duty statements can be assembled by
                  referring to the ‚ÄúSpecialty Summary‚Äù and ‚ÄúDuties and
                  Responsibilities‚Äù sections of AFR 39-1, Occupational Survey
                  Reports (OSRs), applicable specialty training standards, and
                  contractor data. Duty statements are created from two parts of
                  speech, a verb and an object. The verb and the object describe
                  the duty in a broad sense.


Example           For example, one of the duties of an avionics technician might be
                  to ‚ÄúMaintain the J-4 Compass System.‚Äù In this example, the verb
                  and the object don‚Äôt tell how (remove, replace) or what (flux
                  valve, main amplifier) to maintain. Together they simply tell
                  technicians that they are required to keep an aircraft system in
                  working order. Don‚Äôt hesitate to assume that some duties are re-
                  quired even if they‚Äôre not stated in available data sources. Use
                  your experience as well as the experience of other SMSs.
                  However, be sure to document all assumptions as such. You
                  may document your data in any format you desire; just document
                  it.


Verification      Once the list of duty statements is complete, have it verified by
                  applicable agencies such as the using command and test force
                  technicians. After verification, a periodic review/reevaluation
                  should be accomplished to ensure accuracy. As the system
                  matures, new information becomes available.


Substep 2         Gather Data
                                         AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Purpose              The purpose of this substep is to gather all data pertaining to a
                     given AFSC which, in turn, is related to a given weapon system.
                     This data will eventually be used to help structure a complete
                     task list. The complete task list is used to identify and define
                     potential training requirements.


Limit your           The database should be limited to data relating to entries
database             identified in the duties list (developed in the first substep). For
                     example, while performing ISD for an avionics system of an
                     aircraft, data related to the engine isn‚Äôt considered unless these
                     systems interact with one another.


Additional sources   Many ‚Äúnew‚Äù or ‚Äúadvanced‚Äù systems are actually the next
                     generation of existing equipment. Similarities may exist in opera-
                     tor/maintenance functions and performance requirements. Data
                     obtained from similar systems can be used to project new job
                     performance requirements.


Examples             Data can be collected from:

                        User-defined operational concepts
                        User-defined maintenance concepts
                        PDRs
                        CDRs
                        LSA
                        Maintenance Level Analysis (MLA)
                        Contractor engineering drawings
                        SMS/contractor engineer interviews
                        SMS/contractor technician interviews
                        Developmental Program Manuals (DPMs)
                        Preliminary Technical Orders (PTOs)


Forms of data        Data can take many forms. Use any and all available data to
                     compile as comprehensive a database as possible.


Substep 3            Identify Tasks
AFH 36-2235    VOLUME 3      1 NOVEMBER 2001                                            219




Purpose           The purpose of this substep is to identify and record a compre-
                  hensive list of task statements necessary to perform all related
                  duties. A good task description will include a definition of what is
                  to be done, why, when, where, by whom, and how well. The
                  entire training scenario is built upon written task descriptions,
                  which detail job performance requirements. Use the following
                  criteria to identify tasks.

                     A task is a group of related activities directed toward a goal.
                     A task usually has a definite beginning and end.
                     A task involves a person‚Äôs interaction with equipment, other
                     personnel, and/or media.
                     A task, when performed, results in a meaningful product.
                     A task includes a mixture of decisions, perceptions, and/or
                     motor activities.
                     A task may be any size or degree of complexity, but must be
                     directed toward a specific purpose or discrete portion of the
                     total duty.


Example           Identify the tasks for every duty statement from Substep 1.
                  Referring to the example in Substep 1, the first task statement
                  might be ‚ÄúREMOVE J-4 COMPASS SYSTEM FLUX VALVE.‚Äù The
                  verb and object describe, more specifically, how and what to
                  maintain.


Substep 4         Verify Task List


Purpose           The purpose of this substep is to verify that the task list is as
                  complete and accurate as possible. Verification requires close
                  coordination between applicable agencies. These agencies
                  might include other SMSs, test force counterparts, contractor
                  personnel, or user maintenance personnel.


Verification      Also, when asking for task list verification, give instructions to:
instructions
                     Identify additional tasks that need to be included.
                     Identify tasks not performed on this weapon system, but put
                     on the list in error.
                     Evaluate each statement‚Äôs action verb for accuracy.
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Modifications      Once the task list is verified, make the necessary modifications.
                   As with the duty list, reevaluate the task list.


Substep 5          Group Tasks


Purpose            The purpose of this substep is to group discrete/related tasks.
                   Keep in mind that the way tasks are grouped may influence the
                   design of the training equipment. Tasks that are grouped
                   together tend to be taught together.


Logical grouping   A logical procedure for grouping tasks is to separate them by
                   major subsystem. Tasks related to individual subsystems may be
                   further divided according to functions performed. Finally, tasks
                   may be subgrouped at the line/shop level within each function.
AFH 36-2235     VOLUME 3        1 NOVEMBER 2001                                         221


                           STEP 2
     IDENTIFY CHARACTERISTICS OF THE TARGET POPULATION

Introduction         Before training requirements can be determined, two pieces of
                     information are needed: (1) tasks identified in Step 1, and (2)
                     characteristics of the target population. These characteristics are
                     skills and knowledge that the target population already possess
                     and are combined to create a definition of the target population.


Definition           The target population is defined based on two factors: (1) skill
                     levels, and (2) previous training. Skill levels are simply the 3, 5,
                     or 7 level rating of the personnel who will perform the
                     maintenance. Previous training refers to technical training and
                     other formal education. These factors are important because
                     they influence the number of potential training requirements that
                     are analyzed and trained. Use sources such as
                     Course/Specialty Training Standards, Occupational Surveys,
                     personal experience, etc., to build on this information.


Document             Sometimes it‚Äôs necessary to make assumptions about the target
assumption           population. If this has to be done, base assumptions on the most
                     current information from applicable sources.


Target population    A summation of this analysis is called a Target Population
definition summary   Definition. This definition becomes the baseline for the remaining
                     analysis.
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                               STEP 3
             DETERMINE TASK-BASED TRAINING REQUIREMENTS

Introduction          In this step, an analyst decides what is and what isn‚Äôt included in
                      the training. This decision is made by comparing training
                      requirements (Step 1) with the characteristics of the target
                      population (Step 2).


Substep 1             Identify Activities


Purpose               The purpose of this substep is to document the sequence of
                      activities used to complete each task; each activity is then
                      analyzed to see if it can be classified as a potential training
                      requirement.


Definition            An activity can be considered a discrete step toward the
                      completion of a given task. Use any and all available data such
                      as Technical Orders (TOs), personal experience, interviews with
                      test personnel, etc., to build a list of activity statements. Each
                      activity must, as a minimum, describe (1) the action taken, (2) the
                      object of the action, (3) the equipment/tools used to perform the
                      action, (4) precautions and likely errors, (5) any alternative
                      activities, and (6) any existing contingencies.


Activities analysis   Each activity of every task is analyzed by answering five
                      questions. Only those activities identified as potential training
                      requirements need to be analyzed for related skill and knowledge
                      behaviors. To analyze activities, answer the following:
                         Is the activity new? That is, has this exact step in this system,
                         subsystem, or piece of equipment not been performed by the
                         target population before?
                         Are there any abnormal conditions associated with performing
                         the activity? Are there environmental limitations under which
                         the activity must be performed, such as extreme noise or

                                                                     Continued on next page
AFH 36-2235       VOLUME 3       1 NOVEMBER 2001                                          223


Activities analysis      limited access? Is the equipment or material being handled
(Continued)              hazardous, heavy, or bulky? Is more than one person
                         required to manipulate switches?
                         Are there new or unusual criteria related to the activity? Does
                         the activity require new or stricter standards of performance
                         for speed, timing, accuracy, or sequence?
                         Is there a chance for negative transfer to occur? If the target
                         population performs this activity in a manner previously
                         learned, will it cause incorrect or dangerous results?
                         Are any new or modified support tools or equipment required?


If no                 If the answer to all of these questions is "NO," the activity is
                      simply integrated into the training scenario for the benefit of
                      cohesion. That is, it doesn‚Äôt drive a training requirement but it‚Äôs
                      still needed to complete the task.


If yes                If the answer to any of these questions is "YES," the activity is
                      considered to be a potential training requirement.


Substep 2             Identify Skill and Knowledge Behaviors


Introduction          Skill and knowledge behaviors form the foundation of the
                      training. They influence the characteristics and type of training
                      equipment as well as the structure of the instructional content.


Definition            A skill behavior is any discrete physical action that requires
                      practice to master. Some examples are lifting or moving
                      equipment, positioning components, and tightening or loosening
                      hardware. A knowledge behavior is a fact, rule, or principle; it‚Äôs a
                      piece of information retained and results in observable behavior
                      during the performance of a task.


Identification        Each of these types of behavior needs to be identified before
                      applying any analysis parameters. Use the following taxonomy to
                      help identify activities. These points clarify skill and knowledge
                      behaviors related to an activity.

                                                                     Continued on next page
                                AFH 36-2235 VOLUME 3     1 NOVEMBER 2002




Identification               Skill and Knowledge Taxonomy
(Continued)
                 ASO   ASSOCIATING; naming; responding to a specific input.
                       The student associates the response with a specific
                       input only. The response may be verbal or written.

                       EXAMPLE: Naming objects or events; identifying parts
                       of equipment, such as an AC power cord.

                       ACTION VERBS (to join or combine things or thoughts;
                       to link or correlate): IDENTIFY; RELATE; NAME;
                       MATE; MATCH; INDICATE; LABEL; LOCATE.
                 RFP   RECALLING FACTS AND PRINCIPLES; remembering
                       and maintaining knowledge or nomenclature, functions
                       and physical lows. Restating basic knowledge through
                       mental rehearsal or verbal/written recall.

                       EXAMPLE: Recalling specific radio frequencies; listing
                       equipment parts; stating Ohm‚Äôs Law.

                       ACTION VERBS (to bring back to mind or summon
                       from memory): ENUMERATE; (RE)STATE; RECITE;
                       REITERATE; ITEMIZE, QUOTE, REPEAT.
                 RPR   RECALLING PROCEDURES, sequences, or required
                       behaviors in a specified order.

                       EXAMPLE: Recalling checkout procedures or
                       assembly/disassembly routines.

                       ACTION VERBS (Same as RFP).
                 DIS   DISCRIMINATING; being able to distinguish between
                       inputs. Making different responses to the different
                       items within a class.

                       EXAMPLE: Telling the difference between similar
                       gauges on an instrument panel; noticing frayed wiring.

                       ACTION VERBS (to mark the peculiar features of; to
                       recognize as being different from others): DETECT;
                       COMPARE; DISTINGUISH; DIFFERENTIATE;
                       SELECT.

                                                          Continued on next page
AFH 36-2235      VOLUME 3   1 NOVEMBER 2001                                          225




Identification                    Skill and Knowledge Taxonomy
(Continued)
                    CLS     CLASSIFYING; recognizing patterns; seeing the
                            similarity among a class of objects or events which call
                            for a common response; generalization.

                            EXAMPLE: Aircraft classification (friendly, enemy,
                            tactical, etc.).

                            ACTION VERBS (to arrange in groups according to
                            common characteristics; to assign systematically; to
                            show sameness or unity of): GROUP; SORT;
                            CATEGORIZE; RANK; RATE; ASSIGN; FILE.
                    RUS     RULE USING; applying a rule to a given situation by
                            responding to a class input with a set of actions.
                            Relating to two or more simpler concepts in the
                            manner of a rule. A rule states the relationships
                            among concepts. It‚Äôs helpful to think of principles as
                            "if/then" statements.

                            EXAMPLE: If the signal indicator flashes, then lower
                            the pressure in the pump.

                            ACTION VERBS (to put into practice): CONVERT;
                            CALCULATE; PREDICT; PRESCRIBE; TRANSLATE;
                            TRANSCRIBE; VALIDATE; VERIFY.
                    PBS     PROBLEM SOLVING; making a decision based on
                            limited information. Solving a novel problem by
                            combining previously learned rules or generating new
                            rules through trial and error.

                            EXAMPLE: Isolating the source of a malfunction.

                            ACTION VERBS (to find an answer or remedy for):
                            STUDY; ANALYZE; ADAPT; CREATE; DEVELOP;
                            DEVISE.

                                                                Continued on next page
                                AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Identification               Skill and Knowledge Taxonomy
(Continued)
                 PSM   POSITIONING & SERIAL MOVEMENT; positioning
                       switches, buttons, knobs, levers, etc., either
                       individually or in a chain of highly coordinated motor
                       tasks. Motor aspects of equipment set-up and
                       operating procedures.

                       EXAMPLE: Following equipment turn-on procedures;
                       typing; switch throwing.

                       ACTION VERBS (to fix in place; to be set in relation to
                       others): ALIGN; INSERT; TURN ON/OFF;
                       (DE)ACTIVATE; TUNE; (DIS)ENGAGE.
                 CMV   CONTINUOUS MOVEMENT; perceptual motor skills
                       involving continuous pursuit of a target or keeping
                       dials at a certain reading. Compensatory movements
                       based on feedback from displays. May involve
                       scanning of complex displays to determine current
                       status of the system and to predict the evolving state of
                       the system.

                       EXAMPLE: Steering on a constant course; tracking.

                       ACTION VERBS (to follow closely on a regular
                       course): TRACE; STEER; SLIDE; GUIDE.
                 RMV   REPETITIVE MOVEMENT/MANIPULATION or
                       standardized behaviors/mechanical skills. Emphasizes
                       dexterity, occasionally strength and endurance;
                       requires lower level of a larger task.

                       EXAMPLE: Use of hand tools such as a hammer,
                       wrench, or power tools.

                       ACTION VERBS (to bring together into a whole):
                       SPLICE; TIGHTEN; LOOSEN; HOLD; CUT;
                       DISASSEMBLE; LUBRICATE; GRASP; MEASURE;
                       (DIS) CONNECT; DRILL.
AFH 36-2235       VOLUME 3      1 NOVEMBER 2001                                        227




Analysis cautions    Be very careful when analyzing behaviors. For example, being
                     able to locate an object isn‚Äôt the same as being able to name an
                     object. That is, it isn‚Äôt necessary to associate a name to an object
                     in order to find it. The difference, although small, influences the
                     type of learning (taxonomy) and therefore the type of training
                     that‚Äôs developed.


Examples             The example of "APPLY AIRCRAFT POWER" contains several
                     behaviors. A technician would first need to find the power cable
                     before connecting it to the aircraft. Therefore, the first behavior
                     is knowledge-based and might be written as "IDENTIFY POWER
                     CABLE." Phrase knowledge behaviors carefully; list any
                     information that might help, form an objective such as
                     performance conditions or criteria.


How to identify      Now that skill and knowledge behaviors are identified, begin the
potential            analysis that determines which are potential training
requirements         requirements. Each skill/knowledge behavior of every activity
                     (identified as a potential training requirement) is analyzed by
                     answering several questions. Only those skill and knowledge
                     behaviors identified as potential training requirements need to be
                     analyzed for equipment requirements.

                        Is the skill/knowledge new to the target population?
                        Is there an unusual condition attached to the skill/knowledge?
                        For example, is there an awkward body movement necessary
                        in performance, limited or "blind" access/visibility, etc.?
                        Are there new or unusual criteria related to the activity? Does
                        the activity require new or stricter standards of performance
                        for speed, timing, accuracy, or sequence?
                        Is there a chance for negative transfer to occur? If the target
                        population performs this activity in a manner previously
                        learned, will it cause incorrect or dangerous results?
                        Are any new or modified support tools or equipment required?


If No                If the answer to all of these questions is "NO," the skill or
                     knowledge behavior is integrated into the training scenario for
                     the benefit of cohesion. It doesn‚Äôt drive a training requirement but
                     it‚Äôs still needed to complete the activity.
                             AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




If Yes   If the answer to any of these questions is "YES," the skill or
         knowledge behavior is considered to be a potential training
         requirement.
AFH 36-2235      VOLUME 3       1 NOVEMBER 2001                                        229


                          STEP 4
       DETERMINE CONCEPT-BASED TRAINING REQUIREMENTS


Introduction         In this step, skill and knowledge behaviors identified as potential
                     training requirements are analyzed further for potential concept
                     training requirements. That is, some of those pieces of
                     knowledge that indirectly support potential training requirements
                     (behaviors) may not be known to the target population.


Example              A jet engine mechanic has to perform engine changes from time
                     to time. Part of being able to do this is knowing (cognitive
                     domain) where the mount bolts are located and how to remove
                     them. Note: These are the types of behaviors identified and
                     analyzed in Step 3. But, there‚Äôs another knowledge component
                     here; mount bolts are required to be ‚Äúclose tolerance.‚Äù They fit
                     tighter and, therefore, create a more reliable structure. Does a
                     jet engine mechanic have to know this to change an engine? It
                     depends on the definition of the target population. This is ‚Äúnice to
                     know‚Äù information for someone who does nothing more than
                     engine changes, but consider the same information within a
                     different scenario. A jet engine mechanic is assigned to
                     troubleshoot an engine vibration problem. The problem isn‚Äôt
                     severe enough to warrant an engine change, but it‚Äôs noticeable.
                     The vibration of the engine is induced into the airframe; why?
                     ‚ÄúClose tolerance‚Äù mount bolts form a single structure of the
                     engine and the airframe. Vibrations propagate through a solid
                     mass more readily than through a looser structure. The word
                     ‚Äútroubleshoot‚Äù adds a new dimension to the task.


Cognitive training   The analysis performed in this step also supports potential
                     training requirements that fall exclusively into the cognitive
                     realm; for example, training scenarios that present a degree of
                     understanding for system theory or management practices.
                     These types of scenarios are on the same level with performance
                     (psychomotor)-based tasks and are assembled or disassembled
                     in the same way; they are referred to here as cognitive-based
                     training requirements.
                                         AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




What is a concept?   In both contexts, a concept is an idea or group of ideas with
                     common elements. From this perspective, a concept can be
                     thought of as a set of common elements, each element having its
                     own characteristics.


Substep 1            Identify Concepts


Purpose              The purpose of this substep is to identify concepts that are
                     relevant not only to those behaviors identified as potential
                     training requirements, but also to those tasks that fall exclusively
                     into the cognitive realm (potential cognitive-based training
                     requirements).


How to identify      First, identify concepts that are relevant to potential behavioral
concepts             training requirements. Next, identify those tasks (potential
                     cognitive-based training requirements) that are exclusively within
                     the cognitive realm. Apply the following six questions to each
                     potential cognitive-based training requirement. If the answer to
                     any of these questions is ‚ÄúYES,‚Äù then it‚Äôs considered a potential
                     concept-training requirement and is analyzed in the remaining
                     subsets.

                        Does the target population need to know about functional
                        operations of components?
                        Does the target population need to know about associated
                        system inputs (input signals or data from another system)?
                        Does the target population need to know uses, capabilities,
                        and limitations of test equipment?
                        Does the target population need to understand and be able to
                        apply knowledge of signal flow to find discrete components?
                        Does the target population need to use other data with test
                        equipment to find faulty components?
                        Does the target population need to know and apply specific
                        rules such as Ohm‚Äôs Law?


Substep 2            Identify Concept Elements
AFH 36-2235    VOLUME 3       1 NOVEMBER 2001                                          231




Definition         Concepts are made of elements, much the same way that tasks
                   are made of activities. Disassembling each concept into discrete
                   knowledge parts (elements) identifies these elements.


Example            The concept of troubleshooting contains many elements. Three
                   of these elements are: (1) identify the problem, (2) localize the
                   problem to a subassembly or function, and (3) isolate the
                   problem to the discrete component. The requirement to perform
                   troubleshooting may have been identified in Step 3, but the
                   concept or theory (collective concept elements) is identified here.


How to identify    Analyze each element and apply the following questions.
concept elements
                      Is the element relevant to the target population? An element
                      is relevant when it has a direct bearing on the subject matter,
                      so closely related to the concept as to provide reinforcement,
                      or is required for understanding and reinforcement.
                      Is the element incidental to the target population? These
                      elements are similar to those activities and behaviors that
                      weren‚Äôt identified as potential training requirements but were
                      included for the sake of cohesion.


If no              If the answer to both of these questions is ‚ÄúNO,‚Äù the element is
                   discarded.


If yes             If the answer to the first question is ‚ÄúYES,‚Äù the element is
                   identified as a potential concept element-training requirement.
                   These are the only elements considered for further analysis. If
                   the answer to the second question is ‚ÄúYES,‚Äù it isn‚Äôt considered
                   for further analysis, but is included in the training scenario for the
                   benefit of cohesion.


Substep 3          Identify Characteristics of Elements
                                      AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Introduction      Characteristics make an element unique. Referring to the
                  previous example, the concept of troubleshooting, each element
                  implies progress toward a goal. It‚Äôs because of the unique
                  characteristics of each element that a technician can reach that
                  goal.


Example           To identify a problem in an avionics system, technicians must
                  use several of the physical senses (that‚Äôs one characteristic) and
                  be able to understand the sequential signal flow from one
                  component to another (that‚Äôs another characteristic). Technicians
                  must also understand the functional operation of each
                  component (characteristic) as well as each component‚Äôs
                  interrelationship with others (yet another characteristic).


How to identify   Remember, characteristics are identified only for potential
characteristics   concept element training requirements that are ‚ÄúRELEVANT.‚Äù

                  Ask these questions:

                     Is the characteristic new? Has this target population been
                     exposed to this level of understanding on this system?
                     Is the characteristic complex? That is, is it detailed, does it
                     require a higher level of scientific theory, or understanding of
                     mathematical formulas?
                     Is the characteristic critical? Can the element be understood
                     without it?


If no             If the answer to all of these questions is ‚ÄúNO,‚Äù the characteristic
                  can be discarded.


If yes            If the answer to any of these questions is ‚ÄúYES,‚Äù it‚Äôs identified as
                  a potential element characteristic training requirement and
                  documented.
AFH 36-2235        VOLUME 3      1 NOVEMBER 2001                                        233


                                STEP 5
                   DETERMINE MEDIA AND METHODOLOGY


Introduction          If there‚Äôs a training requirement, something must be used to
                      teach it some form of media. The first image that comes to mind
                      is an instructor standing in front of a class lecturing from
                      transparencies. This is the most common way to get a point
                      across, but not always the most effective. Instructional scenarios
                      should stimulate the senses.

                      In this step, the analyst is prompted by a series of questions.
                      Answers given in response to these questions determine the best
                      delivery method, domain of learning, whether hardware or
                      alternate media is used, the appropriate media class, and the
                      best method of instruction for each media class.


Substep 1             Determine Method of Delivery


Purpose               The purpose of this substep is to determine how the instructional
                      message is delivered.


How to deliver        There are two ways to deliver the instructional message, (1) by
instruction           an instructor or instructor-based delivery, and (2) by specific
                      media or media-based delivery.


Instructor-based      Instructor-based delivery may use a variety of media such as
delivery              transparencies, exhibits, etc., but without the instructor the media
                      is lifeless. That is, the media can‚Äôt deliver the instructional
                      message by itself; media only supports delivery of the
                      instructional message.


Media-based           In media-based delivery, an instructor isn‚Äôt necessary for
delivery              learning to take place. The media is a self-contained instructional
                      unit; it provides (in most cases) stimulus and feedback, and an
                      instructor supports delivery of the instructional message.
                                         AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




How to determine     To determine the method of delivery for behaviors and concepts,
method of delivery   ask the following questions:

                           Is the instructional content dynamic? If the content is likely
                           to change, requiring updates more than once a year, it‚Äôs
                           considered to be dynamic. The cost of updating media-
                           based materials is very high compared to updating
                           alternate media.
                           Does the behavior deal with interpersonal skills, behavior
                           modification, or change in attitude? This focuses on the
                           affective domain of learning. Human interaction is
                           generally more effective in a classroom or laboratory
                           environment.
                           Is team effort or interaction with an instructor important?
                           Where team efforts are concerned, it‚Äôs usually desirable to
                           let an instructor control the learning environment.
                           Does the behavior require feedback on performance of
                           motor skills or procedures? When students work to master
                           a psychomotor skill they require feedback to correct
                           undesirable behavior.
                           Is the instructional message intended for wide
                           distribution? Consider the target population and where
                           they‚Äôll be located. If the target population is scattered
                           throughout a geographical location, then the instructional
                           message needs to be exported. The cost of sending
                           instructors may be prohibitive; therefore, hardware
                           (media-based instruction) may be appropriate.
                           Is it critical that the content be delivered the same way
                           every time? While it‚Äôs certainly desirable, it‚Äôs seldom
                           critical. This is intended to identify those behaviors where
                           inconsistent or faulty instruction could lead to unsafe acts
                           on the part of the target population.
                           Should the instructional content be adaptable to individual
                           differences and/or allow individuals to control the pace or
                           amount of practice? In situations where extremes exist in
                           individual capabilities, learning styles, or previous training/
                           experience, media-based material may be appropriate.
                           Is face-to-face instruction impractical? Consider the
                           availability of qualified instructors, instructor-to-student
                           ratios, and temporary duty (TDY) costs. If any of these
                           factors apply, media-based material is appropriate.

                                                                    Continued on next page
AFH 36-2235     VOLUME 3        1 NOVEMBER 2001                                        235




How to determine     Note: Each skill/knowledge behavior and concept training
method of delivery         requirement is analyzed using these same eight questions.
(Continued)                Notice, also, that the first four questions are considerations
                           for instructor-based delivery and the last four questions
                           are considerations for media-based delivery. Use Yes/No
                           answers to these questions to determine your media
                           recommendations


Substep 2            Determine Domain of Learning


Introduction         The domain of learning is another key factor that helps determine
                     whether to use hardware or alternate media. In this substep, the
                     domain of learning is determined for both delivery methods,
                     instructor-based and media-based.


How to determine     To determine the domain of learning for instructor- or media-
domain of learning   based delivery, each behavior and concept is analyzed using
                     these questions. If the answer to any one question is ‚ÄúYES,‚Äù an
                     analyst is directed to the appropriate domain of learning.
                     Document the results.

                        Is the behavior a motor skill? If so, it is a psychomotor domain
                        of learning. A motor skill is a matter of dexterity, that is, a
                        given body movement or motion that‚Äôs practiced so much that
                        it becomes automatic. For example, tightening a bolt with an
                        adjustable wrench is a motor skill that some 3-skill-level
                        technicians must practice. Practice is required for them to
                        learn how to use the wrench with one hand and adjust it
                        without dropping it.
                        Does the behavior call for knowledge or mental skills? If so, it
                        is a cognitive domain of learning. Task knowledge are facts
                        and principles that support performance. Mental skills allow a
                        person to make decisions, discriminate between similar
                        inputs, or to recognize the difference between
                        objects/concepts.

                                                                    Continued on next page
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




How to determine        Does the behavior require a change in attitude? If so, it is an
domain of learning      affective domain of learning. Attitudes refer to an internal
(Continued)             state that influences the choices a person makes. This
                        consists of emotional and intellectual aspects. The point of
                        focus here is those attitudes that can be changed with
                        exposure to role models, opportunities for successful
                        accomplishment, and positive reinforcement.


Conclusions          To this point, the delivery method and domain of learning are
                     known for each behavior and concept. The 374 TDS
                     recommends analyzing training requirements identified for
                     instructor-based delivery first.


Substep 3            Determine Instructor-Based Delivery Requirements


Are motor skills     If motor skills (psychomotor domain) are required in learning,
required?            analyze the behavior or concept by answering the following
                     questions:
                         Is the behavior difficult to execute? If the behavior is difficult
                         to execute, it‚Äôs probably difficult to master without some
                         hands-on practice. For example, adjusting the color bias on a
                         television or adjusting a carburetor on a running engine.
                         Are there unusual conditions? Consider the working
                         environment and stressful situations such as working in
                         cramped quarters or where pacing of events can‚Äôt be
                         controlled.
                         Are there special performance criteria? Criteria are standards
                         of performance. Are there specifications that can‚Äôt be met
                         without practice?
                         Are there hardware cues that affect performance? The actual
                         equipment may provide feedback that‚Äôs too difficult or
                         complex to explain. Simulation of equipment feedback may be
                         shown with audiovisual media or hardware. Which is most
                         cost-effective?
                         Are new support tools/equipment required? Note: ‚ÄúTools‚Äù are
                         not only wrenches and hammers but also mathematical
                         equations, computer programs, formulas, etc. However, the
                         concern here is hardware-type tools. Other tools should be
                         analyzed as alternate media.

                                                                     Continued on next page
AFH 36-2235        VOLUME 3      1 NOVEMBER 2001                                         237




Are motor skills         Are the consequences of error high? This focuses on
required?                chances of personal injury or damage to equipment if the
(Continued)              behavior isn‚Äôt performed correctly. An aircraft egress system,
                         for example, isn‚Äôt taught on actual equipment for 3-skill-level
                         training; one mistake is enough to kill a person and damage
                         equipment.
                         Is the frequency of performance very high or very low? Some
                         behaviors are performed so often that practice on a hardware
                         training device leads to greater efficiency. On the other hand,
                         behaviors (such as emergency procedures) may be
                         performed so rarely that they‚Äôre forgotten unless practiced
                         often.


If yes                If the answer to any of these questions is ‚ÄúYES,‚Äù the analyst is
                      directed to use a hardware (HW) training device.


If no                 For ‚ÄúNO‚Äù responses, hardware isn‚Äôt used, so alternate media is
                      the appropriate choice. Consider the relevant factors used as
                      stimuli. These factors influence student performance.


Alternate media       Media includes, but is not limited to:

                         Print
                         Audio
                         Projected still visual
                         Audio projected still visual
                         Motion visual
                         Audio motion visual
                         Color motion visual
                         Color audio motion visual


How to determine      To determine the appropriate alternate media for a behavior or
alternate media       concept (psychomotor domain), answer the following questions:

                         Is display of motion necessary? In some cases, students may
                         have to respond to feedback from system components; the
                         reverse may also be true.

                                                                    Continued on next page
                                       AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




How to determine      Is color necessary? Unless color flags an important aspect of
alternate media       system operation (warning lights, for example) or is present in
(Continued)           a real scenario, it isn‚Äôt necessary.
                      Is sound necessary? Again, consider whether or not this
                      factor flags an important aspect of system operation or is
                      present in the real scenario.

                   Note: All other behaviors and concepts are understood to
                   require hardware. These are analyzed at a later time.


Are knowledge      If knowledge skills are required, you must analyze the behavior
skills required?   or concept. Ask the following questions:

                      Does the behavior involve unfamiliar concepts or objects?
                      Consider whether or not the target population was ever
                      exposed to this specific behavior. This includes equipment
                      as well as environment.
                      Does the behavior involve interpersonal skills? Consider any
                      interaction/communication between people.
                      Is display of motion necessary? Again, students may have to
                      respond to feedback from system components.
                      Is sound necessary? If it flags important aspects of system
                      operation or is present in a real scenario, the answer is
                      ‚ÄúYES.‚Äù
                      Is it practical to demonstrate the real thing in class? For
                      example, it isn‚Äôt practical to bring a complete jet engine into
                      class to teach the knowledge portion of removing mount bolts.
                      Remember that this is the cognitive domain.

                   By answering these five questions, the analyst is led to the
                   appropriate alternate media.


Substep 4          Determine Media-Based Delivery Requirements


Synopsis           An analyst may have determined earlier that a given behavior is
                   suited to media-based delivery. That is, the training scenario is
                   controlled by some type of media that provides stimuli, monitors
                   student response, and provides feedback. Now you need to
                   determine delivery requirements for all three domains of learning
                   supported by media-based delivery.
AFH 36-2235    VOLUME 3           1 NOVEMBER 2001                                         239




Substep 5         Determine Method of Instruction


Introduction      The purpose of this substep is to determine the best method of
                  instruction for each media class (within alternate media)
                  identified earlier.


How to            The best way to determine method of instruction is to use a
determine         matrix of media/method (see Figure E-1). To use this matrix,
method of         match the method of instruction with the best media for the
instruction       training.

                  Figure E-1 Media/Method Matrix
                            METHOD______________________________


                          Student Performance


                          Demonstration


                          Guided Discussion


                          Traditional Lecture


                          Self-Paced/Prog Instruction



                            MEDIA________________________________


                          Print (PRT)                                 Y   Y   M   N   N


                          Audio (AUD)                                 M   Y   N   M   N


                          Projected Still Visual (PSV)                Y   Y   M   Y   M


                          Audio Projected Still Visual (APSV)         Y   Y   M   Y   M


                          Motion Visual (MV)                          Y   Y   M   Y   M

                          Color Motion Visual (CMV)                   Y   Y   M   Y   M


                          Color Audio Motion Visual (CAMV)            Y   Y   M   Y   M


                          Exhibit (EXH)                               Y   N   N   Y   M


                          Still Visual (SVS)                          M   Y   N   M   N


                          Interactive Courseware (ICW))               Y   N   N   M   M


                          Hardware (HDW)                              M   N   N   Y   Y



                           Y = Yes             N = No     M = Maybe
                                           AFH 36-2235 VOLUME 3       1 NOVEMBER 2002


                                  STEP 6
                     DEVELOP INSTRUCTIONAL STRATEGIES


Introduction           This step is designed to refocus efforts at the task level. The
                       end product is a preliminary overview of the training scenario.


Substep 1              Develop Criterion Objectives


Definition             When phrased properly, criterion objectives specify under what
                       circumstances a behavior is performed (condition), exactly what
                       must be done (behavior), and how well the behavior is performed
                       (standard).


Description            The condition describes the important aspects of the
                       performance environment. Examples of these aspects are the
                       tools the students are given to work with, access to technical
                       data, and special instructions.

                       The behavior of a criterion objective is observable and
                       measurable. That is, students must demonstrate a knowledge or
                       performance so that an instructor can determine/see a specified
                       degree of skill or change. The behavior also matches, as closely
                       as possible, actual task performance.

                       The standard of a criterion objective specifies the precise degree
                       of completeness and accuracy of a behavior. The degree of
                       completeness and accuracy may be stated as compliance with
                       Tos, minimum levels of acceptable performance, time
                       requirements, rate of production, etc.


Potential training     Criterion objectives are developed only from those tasks and
                       concepts identified as potential training requirements. Consider
                       each one carefully; some may not be observable or measurable.


Instructional goals    Training requirements that are neither observable nor
                       measurable are integrated into the training scenario, but are
                       written as instructional goals. Instructional goals have a similar
                       written format to behavioral statements.
AFH 36-2235        VOLUME 3     1 NOVEMBER 2001                                          241




Substep 2             Develop Tests


Introduction          As stated earlier, criterion objectives must be observable and
                      measurable. Therefore, all criterion objectives must be
                      measured with either a written or performance test. Of primary
                      concern for written tests is clear communication of questions or
                      problems to students.


Guidelines for test   Following are some guidelines for developing tests.
development
                         Write tests to the reading grade level of the target population.
                         Don‚Äôt write tests with the intent of tricking students.
                         Use correct grammar.
                         Use plausible distracters.
                         Use some type of illustration to clarify complex scenarios.
                         Don‚Äôt give away the correct answer to a question in its stem
                         or in a previous question.
                         Give clear instructions to the examiner and students.
                         Compare the test material to the criterion objective to ensure
                         that it measures the intended behavior and standard.


Performance test      Performance tests need to be structured correctly if students are
guidelines            to retain the instructional message. Pushing students into the
                      actual performance test before they‚Äôre ready causes frustration,
                      confusion, and lack of motivation to concentrate on the
                      remainder of the instruction. The following are some guidelines
                      to follow for developing performance tests.

                         Performance tests need to be demonstrated. This not only
                         clarifies aspects of the procedure, but serves as a summary
                         for the whole task.
                         Give students enough practice before actual testing. If time
                         permits, let them practice until they feel confident with the
                         scenario. Allow them to practice at least twice for complex
                         scenarios.
                         Identify appropriate safety precautions and procedures.
                         Include any special instructions to the examiner in the
                         Instructional Guidance. Special instructions might point out
                         considerations for preparation, timing of certain events, or
                         alternate plans.
                                        AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Substep 3            Develop Media Descriptions


Purpose              The purpose of this substep is to determine specific types of
                     media for each alternate media class from Step 5, then write a
                     brief narrative describing specific types.


How to develop       To develop media descriptions, answer the following questions.
media descriptions
                        Are materials readily available? For example, if print media is
                        needed, are there existing manuals and illustrations to
                        support the need?
                        What are the production costs for different media within a
                        given class?
                        How much time is available for development?
                        How many times will the training be offered? If it‚Äôs offered
                        one time, don‚Äôt spend the money for elaborate media.
                        Is the media subject to frequent change? This can also be
                        costly for certain types of media.


Identify resource    At this point, identify any resource requirements. Doing this now
requirements         allows plenty of time for changes, revisions, etc. Resource
                     requirements include any type of hardware support for specific
                     media such as projectors, cameras, dry marker boards, etc.


Substep 4            Develop Written Overview


Purpose              The purpose of this substep is to define how specific alternate
                     media and resources are used in the training scenario or how
                     they‚Äôre used to help deliver the instructional message.


Documentation        As you write your overview, estimate and document the number
                     of hours needed for each training requirement. Base this
                     estimate on task performance data and personal experience.


Substep 5            Sequence Training Requirements
AFH 36-2235    VOLUME 3       1 NOVEMBER 2001                                       243




Purpose            The purpose of this substep is to structure the training scenario.
                   Structuring allows students to comprehend relationships that
                   exist between concepts, skill and knowledge behaviors,
                   activities, and tasks.


Definition         There are two levels of structuring: (1) macro structuring, and (2)
                   micro structuring.

                      Macro structuring consists of the largest divisions by major
                      subject matter or topic. Using this structuring allows the
                      training scenario to be divided according to major work areas,
                      systems, or levels of progression. Once this is complete,
                      micro structuring can begin.
                      Micro structuring simply allows potential training requirements
                      to be structured into major divisions. This involves
                      sequencing activities into tasks and tasks into objectives.
                      There are three ways to sequence these activities and tasks
                      during micro structuring: (1) by job performance order, (2) by
                      psychological order, and (3) by logical order. These are
                      explained as follows.
                          Sequencing by job performance order produces an
                          instructional environment that replicates the real
                          environment. It‚Äôs very effective in scenarios where
                          students must produce immediately. This tends to have a
                          high level of psychological impact that translates into
                          quality job performance.
                          Sequencing by psychological order focuses on the
                          concepts of known to unknown, simple to complex, and
                          whole to part-whole. However, this type of sequencing is
                          subjective; the analyst determines what‚Äôs known and
                          unknown, simple and complex, etc.
                          Sequencing by logical order is a combination of the first
                          two types and leans heavily toward whole to part-whole.


Determine common Determine whether or not any objectives (for concept-based
elements         training requirements) have common elements. These objectives
                   are placed at the beginning of the training because they
                   represent concepts that are fundamental to several tasks. For
                   example, the concept of physical force is fundamental to learning
                   how simple machines work. Therefore, instruction on this
                   concept precedes instruction on specific machinery.
                                    AFH 36-2235 VOLUME 3      1 NOVEMBER 2002




Prerequisites   Next, determine if there are any skill and knowledge behaviors
                that are prerequisites for others. It may be beneficial to place
                these into objectives and sequence them before others. Also,
                while making this determination, look for chains of events that
                must be performed in specified order.
AFH 36-2235      VOLUME 3     1 NOVEMBER 2001                                       245


                               STEP 7
               IDENTIFY HARDWARE FIDELITY REQUIREMENTS


Introduction        By this time, an analyst has formed a general idea of what the
                    hardware requirements are for media classes identified in Step 5.
                    However, the degree of hardware fidelity needs to be
                    determined. That is, how realistic must the hardware be to
                    support the training scenario? Also, this analysis helps
                    determine the degree of fidelity needed to teach individual skill
                    behaviors; this is possible because tasks are analyzed to the skill
                    and knowledge behavior level (Steps 1 through 4). This prevents
                    development of hardware training devices that perform more
                    functions than are absolutely necessary to deliver the
                    instructional message.


Substep 1           Identify Hardware Fidelity Requirements


Introduction        There are two aspects to fidelity requirements for any hardware
                    training device: (1) functional, and (2) physical. Each of these
                    aspects is analyzed for the quality or degree of stimulus,
                    response, or feedback.


Purpose             The purpose of this substep is to identify and analyze hardware
                    for functional and physical fidelity requirements.
                                          AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




How to identify       Examine the media classes for both instructor-based and media-
hardware              based delivery. Specific hardware for these media classes is
functional fidelity   identified by considering which major assemblies,
requirements          subassemblies, or lower assembly units students come in contact
                      with (for a given training requirement). Answer the following
                      questions.

                         Does the hardware have a functional impact? The functional
                         impact of hardware consists of both stimuli and feedback.
                         Therefore, it‚Äôs considered from both perspectives. If students
                         must receive information from hardware to perform a task or
                         make a decision, the answer to this question is ‚ÄúYES.‚Äù
                         Is the stimulus/feedback difficult to understand? For example,
                         a digital pressure gauge is easier to comprehend than an
                         analog pressure gauge; analog scales are continuous with an
                         infinite number of values from point A to point B and,
                         therefore, it‚Äôs more difficult to determine an exact value.
                         Is the stimulus/feedback difficult to understand because of
                         motion, body position, or feel? This considers whether or not
                         the hardware is in motion during task performance, awkward
                         body positions interfere with performance, and/or
                         discrimination between some physical characteristics of the
                         hardware makes it difficult to make a decision.
                         Is there something unusual, abnormal, or dangerous about
                         the hardware? Use personal experience and good judgment
                         to answer this question. Careful consideration should be
                         given to scenarios with moving parts, explosives, and new or
                         modified tools.

                      Note: Depending on a given response, an analyst may be
                      directed to label the hardware as having various levels of fidelity.


Example               A behavior requires the target population to use a new piece of
                      test equipment. The test equipment is new only because a
                      modification included an external manual adjustment. Since the
                      test equipment has a functional impact, it requires ‚ÄúHigh‚Äù (H)
                      functional fidelity; specifically, the external manual adjustment.
AFH 36-2235       VOLUME 3      1 NOVEMBER 2001                                                     247




Functional fidelity   Use the flowchart in Figure E-2 to determine the degree to which
                      each piece of hardware must act like the real thing.

                      Figure E-2 Functional Fidelity

                                     Start
                                                          Functional Fidelity


                                    Functional    Yes
                                     Impact?


                                             No


                                                  Yes                   Yes   High Fidelity
                                   Difficult?           Motion?



                                             No                No


                                                        Possible High
                                                  Yes     Fidelity
                                   Unusual?



                                             No


                                  Low Fidelity




                                        See
                                     Figure E-3




How to determine      No matter what the response is during functional fidelity analysis,
physical fidelity     an analyst must determine the degree to which the hardware
requirements          must look like the real thing. Ask the following questions.

                         Is the hardware acted upon directly? If students must come in
                         physical contact with the hardware, the answer to this
                         question is ‚ÄúYES.‚Äù

                                                                                 Continued on next page
                                        AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




How to determine       Is the hardware next to or connected to high (H) fidelity
physical fidelity      hardware and does it enhance students‚Äô understanding? In
requirements           some cases, hardware may not provide stimuli, response, or
(Continued)            feedback; it‚Äôs merely next to or connected to hardware that
                       does.
                       Is unaided judgment used to respond to hardware outputs
                       (difficult response)?
                       Are there many possible hardware configurations? It‚Äôs simply
                       easier for students to comprehend the overall scenario using
                       a single high (H) fidelity piece of hardware as opposed to
                       numerous low (L) fidelity components.
                       Is there something unusual, abnormal, or dangerous about
                       the hardware?

                    Again, depending on the response, an analyst may be directed to
                    label hardware as ‚ÄúHigh‚Äù (H) fidelity, ‚ÄúPossible High‚Äù (PH) fidelity,
                    ‚ÄúLow‚Äù (L) fidelity, or ‚ÄúPossible Low‚Äù (PL) fidelity.
AFH 36-2235         VOLUME 3            1 NOVEMBER 2001                                                                            249




Physical fidelity      Use the flowchart in Figure E-3 to determine the degree to which
                       the hardware must look like the real thing.

                       Figure E-3 Physical Fidelity

                                      From
                                  Figure E-2                               Physical Fidelity



                                    Acted             No                                            Next to          No
                                    Upon?                                                       or connected
                                                                                                        to?


                                               Yes                                                             Yes


                                                                                                  Possible
                                                     Yes   High Fidelity
                                                                                               High Fidelity
                                 Difficult?



                                               No


                                                              Possible                                                 Continue
                                                     Yes   High Fidelity                                             Appropriate
                               Configurations?                                                                          Analysis



                                               No



                                                     Yes      Possible
                                   Unusual?                High Fidelity



                                               No


                                       Low
                                    fidelity




Substep 2              Perform Whole-Hardware Analysis


Purpose                This analysis is intended to make the hardware training device
                       appear as realistic as possible at minimal expense. Most
                       hardware training devices include components such as dials,
                       switches, lights, etc., from the actual equipment. But, depending
                       on the training scenario, many of the components may be
                       nonfunctional.


Example                Consider, for example, what should be done with the fifth
                       indicator in a group of five, if students receive
                       stimuli/response/feedback only from the first four. It‚Äôs suggested
                       that the fifth indicator be included as a ‚ÄúLow‚Äù (L) fidelity
                       component.
                                     AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Why include      There are two reasons to include these types of components.
various          First, it makes the hardware training device look realistic;
components?      second, including a modest number shouldn‚Äôt add significant
                 material costs. Two-dimensional drawings or photographs can
                 represent many of these low fidelity components.


Consider         In some cases it may be more expensive to fabricate the
operational      hardware training device with low fidelity components than to use
hardware         the actual hardware. Will including low fidelity components
                 contribute to learning? Or would it be best and most effective to
                 use actual hardware?


Substep 3        Compile Fidelity Recommendations Within Tasks


Introduction     Keep in mind that the hardware requirements analyzed in the
                 previous steps/substeps are developed from potential behavioral
                 training requirements. In turn, these behavioral training
                 requirements are developed from a given activity within a given
                 task.


Purpose          The purpose of this substep is to compile fidelity decisions within
                 that given task. The hardware requirements for a given activity
                 are probably used in other activities of the same task.
                 Therefore, these same hardware requirements are probably
                 labeled with varying fidelity levels. These fidelity labels, for
                 common hardware requirements, must be compiled first.


How to compile   To compile fidelity recommendations within tasks, look at the
                 hardware requirement for the first potential behavioral training
                 requirement of the first activity within the first task. Now, search
                 the rest of the activities in the first task for the same hardware
                 requirement. For this same hardware requirement, total the
                 number of ‚ÄúHigh‚Äù (H) fidelity labels; do the same for the ‚ÄúPossible
                 High‚Äù (PH) fidelity labels, ‚ÄúLow‚Äù (L) fidelity labels, and ‚ÄúPossible
                 Low‚Äù (L) fidelity labels.


Substep 4        Compile Fidelity Recommendations Within Blocks
AFH 36-2235       VOLUME 3      1 NOVEMBER 2001                                            251




Purpose              Just as some hardware is common to more than one activity
                     within a task, some hardware may be common to more than one
                     task within a block. The purpose of this substep is to compile
                     fidelity decisions within blocks.


How to compile       Search the hardware requirements analyzed in Substep 3, within
                     each task, to identify common hardware requirements within
                     blocks of instruction. The end result should be a hardware
                     training device that can be used in more than one unit and/or
                     objective.


Substep 5            Compile Final Fidelity Recommendations


Synopsis             The procedures for this substep are identical to procedures in
                     Substeps 3 and 4, only performed a step higher in the structure
                     of the training scenario. The purpose of this substep is to
                     compile fidelity decisions within the entire course of instruction.
                     That is, there may be hardware requirements that are common to
                     more than one block of instruction. This subset is simply a
                     compilation of the previous substeps in Step 7.

                                  STEP 8
                 IDENTIFY INTERACTIVE COURSEWARE (ICW)
                         FIDELITY REQUIREMENTS

Introduction         Research shows that ICW is a very powerful media in terms of
                     presenting information; it allows development of conceptual
                     skills, analytical skills, and psychomotor skills. It can take many
                     forms, including text-based and interactive video (IVD).


Purpose              The purpose of this step is to determine the type of ICW best
                     suited to training requirements.


Substep 1            Identify ICW Fidelity Requirements
                                           AFH 36-2235 VOLUME 3       1 NOVEMBER 2002




Background            Potential training requirements that need to be presented with a
                      media class of ICW are already identified. This is done in Step 5.
                      All potential skill-, knowledge-, and concept-based training
                      requirements within this media class are analyzed here for
                      fidelity requirements.


Why is ICW            The analysis for ICW fidelity differs from that of hardware fidelity
analysis different?   in the level of focus. In hardware fidelity analysis, the focus is on
                      the actual hardware required to support a behavior or concept.
                      Here, the focus is on course content.


Example               For example, a given task may require students to apply
                      electrical power to an aircraft‚Äôs avionics systems. An activity in
                      this task requires them to turn on the battery switch. The skill
                      behavior, here, is to physically move the toggle switch from
                      ‚ÄúOFF‚Äù to ‚ÄúON.‚Äù A knowledge behavior is the understanding why
                      the switch must be turned on; one concept used to help
                      understand this behavior is that of electron flow. If it‚Äôs already
                      determined (from Step 5) that the media class for these
                      behaviors is ICW, then the battery switch, associated system
                      knowledge, and concept of electron flow are analyzed for fidelity
                      requirements.


How to conduct        Conduct the analysis by answering the following questions.
ICW analysis
                         Is the skill, knowledge, or concept difficult to understand?
                         Consider whether or not it‚Äôs difficult to understand without
                         seeing an animated representation.
                         Do students come in contact with system components during
                         actual maintenance? If students are manipulating controls or
                         making judgments based on system performance/status, the
                         answer to this question is ‚ÄúYES.‚Äù
                         Is it difficult to make a response? Consider close coordination
                         between team members and quick cognitive or psychomotor
                         reaction.
                         Is the response based on specific system stimuli? Stimuli
                         may be in the form of feedback.
                         Is there a high consequence for incorrect student
                         performance? Consider what‚Äôs going to happen if students
                         perform the activity incorrectly or out of sequence.
AFH 36-2235      VOLUME 3     1 NOVEMBER 2001                                                                    253




ICW functional      Use the flowchart in Figure E-4 to determine ICW functional
fidelity            fidelity.

                    Figure E-4 ICW Functional Fidelity
                                      Start
                                                                           ICW Functional Fidelity


                                                            Yes                               High Fidelity
                                   Difficult?



                                                No


                                                                  Low Fidelity
                                                             No
                                  Contact?



                                                Yes



                                                            Yes
                                 Response?


                                                No



                                                       Yes
                                    Stimuli?



                                                No



                                                      Yes

                                Consequence?




                                                No



                                                                                                          To
                                Low Fidelity                                                            Figure
                                                                                                          E-5
                                           AFH 36-2235 VOLUME 3                     1 NOVEMBER 2002




ICW physical   Use the flowchart in Figure E-5 to determine ICW physical
fidelity       fidelity.

               Figure E-5 ICW Physical Fidelity

                               From
                              Figure                               ICW Physical Fidelity
                                E-3




                                                    Yes                              High Fidelity
                           Difficult?



                                        No


                                                          Low Fidelity
                                                    No
                           Contact?



                                        Yes



                                                Yes

                         Consequence?


                                        No



                                               Yes

                           Response?



                                        No



                                              Yes

                             Stimuli?




                                        No




                         Low Fidelity




Substep 2      Compile Final ICW Fidelity Recommendations


Purpose        The purpose of this substep is to compile all ICW fidelity analysis
               data into a cohesive package. These final recommendations are
               used to develop ICW.
AFH 36-2235   VOLUME 3     1 NOVEMBER 2001                                      255




Background       Notice that there‚Äôs no middle ground for the fidelity labels in
                 Substep 1. Each skill, knowledge, or concept either needs to be
                 presented in detail or isn‚Äôt critical to understanding.


Conclusions      Considering that both functional and physical fidelity
                 requirements can have either of these two labels, there are only
                 four possible scenarios that can exist for each type of training
                 requirement. These scenarios are explained next.

                    A low functional and low physical scenario indicates that the
                    training requirement can be met with a text-based tutorial;
                    there are no visual cues or system functions that students
                    must react to. Text-based tutorials may take the form of
                    discussions on specific concepts, which are used later. They
                    may also take the form of study aids that reinforce classroom
                    instruction.
                    A high functional and low physical scenario indicates that the
                    training requirement can be met with animated graphics with
                    text support. Animated graphics illustrate abstract concepts
                    while simulating some aspect of operation.
                    A low functional and high physical scenario indicates that the
                    training requirement can be met with static graphics. Static
                    graphics are generally used to clarify points.
                    A high functional and high physical scenario indicates that the
                    training requirement can be met with two-dimensional
                    part-task simulations. Creating a performance environment
                    requires animation, and this type of ICW is designed to give
                    the visual and auditory cues that go along with manipulation
                    of controls. A two-dimensional part-task simulation can be
                    developed using computer animation, interactive video, and
                    variations/combinations of the two.
                                   AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                             STEP 9
               IDENTIFY INSTRUCTIONAL FEATURES


Introduction   There are four aspects to any training scenario: (1) stimuli, (2)
               response monitoring, (3) feedback, and (4) next task or activity.

                  Stimuli prompt students into action and come from one of two
                  sources: (1) an instructor, or (2) hardware/alternate media.
                  Cues from these sources give students information that leads
                  them to proceed with a given performance.

                  What students do after seeing, hearing, or feeling stimuli is
                  called the response. In response to stimuli, students perform
                  a task or an activity. Student response must be monitored in
                  one of two ways: (1) by an instructor, or (2) by
                  hardware/alternate media. The data gathered during
                  response monitoring is used to provide a degree of feedback
                  to the students.

                  Feedback either reinforces student behavior or helps correct
                  it, depending on the data received during response
                  monitoring. Feedback may also prompt students to begin the
                  next task or activity; once this happens, feedback is then
                  considered stimuli.

                  The feedback/stimuli that directs students to perform the next
                  task or activity comes from (1) an instructor, or (2)
                  hardware/alternate media.


Commonality    Notice that there‚Äôs one thing in common for all four aspects.
               Either an instructor or hardware/alternate media controls the
               training scenario. Skill, knowledge, and concept training
               requirements, to this point, are already determined as delivered
               by the instructor or media.
AFH 36-2235       VOLUME 3      1 NOVEMBER 2001                                        257




Purpose              Even though media-based delivery is directed for some
                     behaviors and concepts, it may be necessary for an instructor to
                     control one of the aspects of stimuli, response monitoring,
                     feedback, or next activity. The purpose of this step is to identify
                     instructional features for those skill/knowledge behaviors and
                     concepts with a media class of HW, ICW, or EXH; specifically,
                     which (instructor or media) controls one of the given aspects and
                     what features are assigned to them. Each hardware training
                     device (or supporting hardware), under these media classes, is
                     analyzed in this step.


Substep 1            Identify Stimuli Controls and Features


How to identify      Look at the first behavior or concept with a hardware requirement
                     and define the stimuli. Once this is known, answer the following
                     questions to identify which (instructor or media) controls stimuli
                     and whether features are preprogrammed or variable:
                        Does the hardware control the stimuli? If dealing with a
                        simple task, such as remove and replace, instructors can
                        effectively control the stimuli and keep the training scenario
                        realistic.

                        Does the hardware control the intensity of stimuli? Intensity
                        is defined as pitch, tone, volume, or duration for audio stimuli
                        and brightness or duration for visual stimuli.

                        Is the intensity preprogrammed (PP) or variable (VAR)?
                        Preprogrammed means that each student sees the exact,
                        same presentation no matter when instruction is given.
                        Variable means that the presentation is adapted to individual
                        student needs. One or the other is included as a feature; it‚Äôs
                        simply a choice an analyst must make. This position is
                        maintained in the remaining analysis of Step 8.

                        Does the hardware control the rate of stimuli? Consider
                        whether or not the hardware training device must emulate
                        system operation.

                        Is the rate preprogrammed (PP) or variable (VAR)? Refer to
                        the clarification in the third question.

                                                                    Continued on next page
                                      AFH 36-2235 VOLUME 3        1 NOVEMBER 2002




How to identify      Does the hardware control the signal-to-noise ratio? A signal
(Continued)          is defined as the instructional message, whatever it may be.
                     Noise is defined as anything that interferes with or blocks the
                     signal; it distracts students‚Äô attention from the training
                     scenario. If the training scenario is very active/interactive, the
                     hardware is designed to control this feature.

                     Is the signal-to-noise ratio preprogrammed (PP) or variable
                     (VAR)? Refer to the clarification in the third question.


Substep 2         Identify Response Monitoring Controls and Features


How to identify   Consider the following:
                    Is hardware freeze capability required? Freeze capability
                    stops the training scenario temporarily, after a specified
                    number of student errors, a fatal error, or when time limits are
                    exceeded. Consider whether or not the nature of the training
                    scenario prompts frequent mistakes.
                    Are the freeze criteria preprogrammed (PP) or variable
                    (VAR)?
                    Do students remove the hardware freeze? If instructors are
                    available to do this, let them. This ensures they‚Äôre aware of
                    individual needs and puts the human factor back into the
                    training scenario. If students remove the freeze, remediation
                    must be included; this remediation is designed to keep
                    students from making the same mistake twice.
                    Is remediation preprogrammed (PP) or variable (VAR)?


Substep 3         Identify Feedback Controls and Features


How to identify   Specific feedback must be identified before continuing. Use the
                  following questions to identify controls and features for feedback:
                      Is feedback provided by the hardware? Consider how much
                      control is given to the hardware already and whether or not
                      the hardware is used in a self-paced training scenario.
                      Does the hardware provide the consequences of a false
                      response? If the hardware is to simulate the actual equipment
                      or provide remediation, the answer is "YES."
                      Is the feedback preprogrammed (PP) or variable (VAR)?
AFH 36-2235       VOLUME 3     1 NOVEMBER 2001                                          259




Substep 4            Identify Next Task/Activity Controls and Features


How to identify      The next task/activity must be identified before continuing. Use
                     the following questions to identify controls and features.

                        Does the hardware control whether or not students proceed
                        to the next activity? Consider the complexity of the training
                        scenario.

                        Is the next activity variable?

                        Is the variability derived from student response or student
                        score? Variability derived from student response
                        acknowledges the many possible hardware configurations;
                        this feature causes hardware to simulate actual equipment
                        and students are free to treat it as such (freeplay). Variability
                        derived from student score limits the possible action a student
                        may take. This feature is best suited for tasks or activities
                        that are critical in nature or to reinforce the instructional
                        message to lower skill levels.

                     Note: The last six steps of the 15-step process are brief and
                     summarized below.
                                 AFH 36-2235 VOLUME 3    1 NOVEMBER 2002


                           STEP 10
      PREPARE TRAINING EQUIPMENT ANALYSIS SUMMARY AND
                  FUNCTIONAL SPECIFICATION


Synopsis       This document is used to record training equipment design
               requirements. It‚Äôs used by the Program Office (PO) to prepare
               the Request For Proposal (RFP). The format will vary depending
               on SPO requirements.
AFH 36-2235   VOLUME 3     1 NOVEMBER 2001                                     261


                           STEP 11
              PREPARE COURSE CONTROL DOCUMENTS


Synopsis         A proposed Course Chart (CC) and Course Training
                 Standard/Specialty Training Standard (CTS/STS) are now
                 derived from the analysis. This is an AETC-specific requirement.


Purpose          The analysis from this ISD process provides the majority of the
                 information needed for development of the CC and CTS/STS.
                 Estimates of course lengths, instructional design, hardware
                 training devices, and AETC-furnished equipment can be derived.
                                    AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                            STEP 12
           PREPARE INSTRUCTIONAL MATERIALS AND TESTS


Synopsis         Preparing instructional materials and tests requires preparation
                 of criterion objectives (Steps 6 and 11). These criterion
                 objectives are listed in the appropriate CCDs and form the basis
                 for this step. Use the analysis data from Steps 3 and 4 when
                 developing test items for criterion objectives. AETC-specific
                 requirements and guidance are in ATCR 52-3 for testing
                 policy/procedures for resident training, and ATCR 50-21 for FTD
                 courses. Guidance and format requirements for training literature
                 (used in resident training) can be found in ATCR 52-2 and ATCR
                 50-21 (for FTD courses).
AFH 36-2235   VOLUME 3     1 NOVEMBER 2001                                      263


                                STEP 13
                         VALIDATE INSTRUCTION


Synopsis         Validation begins as soon as proposed CCDs are printed in their
                 final form. The initial step of the validation phase makes copies
                 of CCDs available to MAJCOMs, FTDs, and/or the Prime Center
                 responsible for weapon systems support (for their review and
                 recommendations). Review and recommendations by these
                 agencies includes providing instruction to a small group of
                 specialists. Time constraints placed on this test bed determine
                 the extent of the validation phase.
                                      AFH 36-2235 VOLUME 3      1 NOVEMBER 2002


                         STEPS 14 AND 15
                  CONDUCT AND EVALUATE TRAINING


Synopsis          These steps represent the final proof of the ISD process. They‚Äôre
                  accomplished in a formal training environment at the operational
                  site. At this point, conduct, management, and evaluation of the
                  course are the responsibility of the using agency. If training
                  analysis and development are performed properly, the result is a
                  training program that instructors are familiar with, and hardware
                  training devices that are justified.


Additional help   If you feel that you need additional information, you may write the
                  374 Training Development Squadron and request a copy of their
                  procedures manual, which is available on IBM-compatible floppy
                  disk. As mentioned earlier, they also have an ISD automation
                  software package (ISDA). Write them at:

                              374 Training Development Squadron/TST
                                        118 S. Wolfe Avenue
                                   Edwards AFB, CA 93524-6545

                  Note: Contractors MUST submit their requests through their
                  SPO.
